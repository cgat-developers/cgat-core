{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"CGAT-core Documentation","text":"<p>Welcome to the CGAT-core documentation! CGAT-core is a powerful Python framework for building and executing computational pipelines, with robust support for cluster environments and cloud integration.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Pipeline Management: Build and execute complex computational pipelines</li> <li>Cluster Support: Seamless integration with various cluster environments (SLURM, SGE, PBS)</li> <li>Cloud Integration: Native support for AWS S3 and other cloud services</li> <li>Resource Management: Intelligent handling of compute resources and job distribution</li> <li>Container Support: Execute pipeline tasks in containers for reproducibility</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#installation-guide","title":"Installation Guide","text":"<ul> <li>System Requirements</li> <li>Installation Methods</li> <li>Verification Steps</li> </ul>"},{"location":"#tutorial","title":"Tutorial","text":"<ul> <li>Basic Pipeline Concepts</li> <li>Running Your First Pipeline</li> <li>Troubleshooting Tips</li> </ul>"},{"location":"#examples","title":"Examples","text":"<ul> <li>Common Use Cases</li> <li>Pipeline Patterns</li> <li>Best Practices</li> </ul>"},{"location":"#core-components","title":"Core Components","text":""},{"location":"#pipeline-development","title":"Pipeline Development","text":""},{"location":"#writing-workflows","title":"Writing Workflows","text":"<ul> <li>Create Custom Pipeline Workflows</li> </ul>"},{"location":"#run-parameters","title":"Run Parameters","text":"<ul> <li>Configure Pipeline Execution</li> </ul>"},{"location":"#pipeline-modules","title":"Pipeline Modules","text":"<ul> <li>Core Pipeline Components</li> </ul>"},{"location":"#execution-environments","title":"Execution Environments","text":""},{"location":"#cluster-configuration","title":"Cluster Configuration","text":"<ul> <li>Set up Cluster Execution</li> </ul>"},{"location":"#container-support","title":"Container Support","text":"<ul> <li>Run Pipelines in Containers</li> </ul>"},{"location":"#cloud-integration","title":"Cloud Integration","text":"<ul> <li>Work with Cloud Storage</li> </ul>"},{"location":"#advanced-features","title":"Advanced Features","text":""},{"location":"#parameter-management","title":"Parameter Management","text":"<ul> <li>Handle Pipeline Parameters</li> </ul>"},{"location":"#execution-control","title":"Execution Control","text":"<ul> <li>Manage Task Execution</li> </ul>"},{"location":"#database-integration","title":"Database Integration","text":"<ul> <li>Work with Databases</li> </ul>"},{"location":"#project-information","title":"Project Information","text":""},{"location":"#how-to-contribute","title":"How to Contribute","text":"<ul> <li>Contributing Guidelines</li> </ul>"},{"location":"#citations","title":"Citations","text":"<ul> <li>Citation Information</li> </ul>"},{"location":"#license","title":"License","text":"<ul> <li>License Information</li> </ul>"},{"location":"#faq","title":"FAQ","text":"<ul> <li>Frequently Asked Questions</li> </ul>"},{"location":"#additional-resources","title":"Additional Resources","text":""},{"location":"#api-documentation","title":"API Documentation","text":"<ul> <li>API Reference</li> </ul>"},{"location":"#github-repository","title":"GitHub Repository","text":"<ul> <li>CGAT-core GitHub Repository</li> </ul>"},{"location":"#issue-tracker","title":"Issue Tracker","text":"<ul> <li>CGAT-core Issue Tracker</li> </ul>"},{"location":"#need-help","title":"Need Help?","text":"<p>If you need help or have questions:</p> <ol> <li>Check our FAQ</li> <li>Search existing GitHub Issues</li> <li>Create a new issue if your problem isn't already addressed</li> </ol>"},{"location":"#overview","title":"Overview","text":"<p>CGAT-core has been continuously developed over the past decade to serve as a Next Generation Sequencing (NGS) workflow management system. By combining CGAT-core with CGAT-apps, users can create diverse computational workflows. For a practical demonstration, refer to the cgat-showcase, which features a simple RNA-seq pipeline.</p> <p>For advanced usage examples, explore the cgat-flow repository, which contains production-ready pipelines for automating NGS data analysis. Note that it is under active development and may require additional software dependencies.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you use CGAT-core, please cite our publication in F1000 Research:</p> <p>Cribbs AP, Luna-Valero S, George C et al. CGAT-core: a python framework for building scalable, reproducible computational biology workflows [version 1; peer review: 1 approved, 1 approved with reservations]. F1000Research 2019, 8:377 https://doi.org/10.12688/f1000research.18674.1</p>"},{"location":"#support","title":"Support","text":"<ul> <li>For frequently asked questions, visit the FAQ.</li> <li>To report bugs or issues, raise an issue on our GitHub repository.</li> <li>To contribute, see the contributing guidelines and refer to the GitHub source code.</li> </ul>"},{"location":"#example-workflows","title":"Example Workflows","text":""},{"location":"#cgat-showcase","title":"cgat-showcase","text":"<p>A simple example of workflow development using CGAT-core. Visit the GitHub page or view the documentation.</p>"},{"location":"#cgat-flow","title":"cgat-flow","text":"<p>This repository demonstrates CGAT-core's flexibility through fully tested production pipelines. For details on usage and installation, see the GitHub page.</p>"},{"location":"#single-cell-rna-seq","title":"Single-Cell RNA-seq","text":"<ul> <li>Cribbs Lab: Uses CGAT-core for pseudoalignment pipelines in single-cell Drop-seq methods.</li> <li>Sansom Lab: Develops single-cell sequencing analysis workflows using the CGAT-core workflow engine (TenX workflows).</li> </ul>"},{"location":"#pipeline-modules-overview","title":"Pipeline Modules Overview","text":"<p>CGAT-core provides a comprehensive set of modules to facilitate the creation and management of data processing pipelines. These modules offer various functionalities, from pipeline control and execution to database management and file handling.</p>"},{"location":"#available-modules","title":"Available Modules","text":"<ol> <li>Control: Manages the overall pipeline execution flow.</li> <li>Database: Handles database operations and uploads.</li> <li>Files: Provides utilities for file management and temporary file handling.</li> <li>Cluster: Manages job submission and execution on compute clusters.</li> <li>Execution: Handles task execution and logging.</li> <li>Utils: Offers various utility functions for pipeline operations.</li> <li>Parameters: Manages pipeline parameters and configuration.</li> </ol>"},{"location":"#integration-with-ruffus","title":"Integration with Ruffus","text":"<p>CGAT-core builds upon the Ruffus pipeline library, extending its functionality and providing additional features. It includes the following Ruffus decorators:</p> <ul> <li><code>@transform</code></li> <li><code>@merge</code></li> <li><code>@split</code></li> <li><code>@originate</code></li> <li><code>@follows</code></li> <li><code>@suffix</code></li> </ul> <p>These decorators can be used to define pipeline tasks and their dependencies.</p>"},{"location":"#s3-integration","title":"S3 Integration","text":"<p>CGAT-core also provides S3-aware decorators and functions for seamless integration with AWS S3:</p> <ul> <li><code>@s3_transform</code></li> <li><code>@s3_merge</code></li> <li><code>@s3_split</code></li> <li><code>@s3_originate</code></li> <li><code>@s3_follows</code></li> </ul> <p>For more information on working with S3, see the S3 Integration section.</p> <p>By leveraging these modules and decorators, you can build powerful, scalable, and efficient data processing pipelines using CGAT-core.</p>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Getting Started</li> <li>Building a Workflow</li> <li>Pipeline Modules Overview</li> <li>S3 Integration</li> <li>Working with Remote Files</li> <li>Core Functions</li> <li>Kubernetes Functions</li> <li>Project Info</li> </ul>"},{"location":"container/tasks/","title":"Containerised Execution in <code>P.run()</code>","text":"<p>The <code>P.run()</code> method supports executing jobs within container environments using Docker or Singularity. This functionality enables seamless integration of containerisation for computational workflows.</p>"},{"location":"container/tasks/#features","title":"Features","text":"<ul> <li>Container Runtime Support: Execute jobs using either Docker or Singularity.</li> <li>Environment Variables: Pass custom environment variables to the container.</li> <li>Volume Mapping: Bind directories between the host system and the container.</li> <li>Container-Specific Command Construction: Automatically builds the appropriate command for Docker or Singularity.</li> </ul>"},{"location":"container/tasks/#api-documentation","title":"API Documentation","text":""},{"location":"container/tasks/#prun","title":"<code>P.run()</code>","text":"<p>The <code>P.run()</code> method executes a list of commands with optional support for containerisation via Docker or Singularity.</p>"},{"location":"container/tasks/#parameters","title":"Parameters","text":"Parameter Type Description Default <code>statement_list</code> <code>list</code> List of commands (statements) to execute. Required <code>job_memory</code> <code>str</code> Memory requirements for the job (e.g., <code>\"4G\"</code>). <code>None</code> <code>job_threads</code> <code>int</code> Number of threads to use. <code>None</code> <code>container_runtime</code> <code>str</code> Container runtime to use. Must be <code>\"docker\"</code> or <code>\"singularity\"</code>. <code>None</code> <code>image</code> <code>str</code> The container image to use (e.g., <code>\"ubuntu:20.04\"</code> for Docker or <code>/path/to/image.sif</code> for Singularity). <code>None</code> <code>volumes</code> <code>list</code> List of volume mappings (e.g., <code>\"/host/path:/container/path\"</code>). <code>None</code> <code>env_vars</code> <code>dict</code> Dictionary of environment variables to pass to the container (e.g., <code>{\"VAR\": \"value\"}</code>). <code>None</code> <code>**kwargs</code> <code>dict</code> Additional arguments passed to the executor. <code>None</code>"},{"location":"container/tasks/#returns","title":"Returns","text":"<ul> <li><code>list</code>: A list of benchmark data collected from executed jobs.</li> </ul>"},{"location":"container/tasks/#raises","title":"Raises","text":"<ul> <li><code>ValueError</code>: If invalid arguments are provided (e.g., container runtime is missing or invalid, or required arguments for container execution are not supplied).</li> <li><code>OSError</code>: If the job fails during execution.</li> </ul>"},{"location":"container/tasks/#examples","title":"Examples","text":""},{"location":"container/tasks/#running-a-job-with-docker","title":"Running a Job with Docker","text":"<p>To execute a job using Docker, specify the <code>container_runtime</code> as <code>\"docker\"</code> and provide an image. Optionally, bind host directories to container directories using <code>volumes</code>, and pass environment variables with <code>env_vars</code>.</p> <pre><code>P.run(\n    statement_list=[\"echo 'Hello from Docker'\"],\n    container_runtime=\"docker\",\n    image=\"ubuntu:20.04\",\n    volumes=[\"/data:/data\"],\n    env_vars={\"MY_VAR\": \"value\"}\n)\n</code></pre> <p>This will construct and execute the following Docker command:</p> <pre><code>docker run --rm -v /data:/data -e MY_VAR=value ubuntu:20.04 /bin/bash -c 'echo Hello from Docker'\n</code></pre>"},{"location":"container/tasks/#running-a-job-with-singularity","title":"Running a Job with Singularity","text":"<p>To execute a job using Singularity, specify the <code>container_runtime</code> as <code>\"singularity\"</code> and provide a Singularity Image File (SIF). Similarly, you can bind host directories and set environment variables.</p> <pre><code>P.run(\n    statement_list=[\"echo 'Hello from Singularity'\"],\n    container_runtime=\"singularity\",\n    image=\"/path/to/image.sif\",\n    volumes=[\"/data:/data\"],\n    env_vars={\"MY_VAR\": \"value\"}\n)\n</code></pre> <p>This will construct and execute the following Singularity command:</p> <pre><code>singularity exec --bind /data:/data --env MY_VAR=value /path/to/image.sif /bin/bash -c 'echo Hello from Singularity'\n</code></pre>"},{"location":"container/tasks/#usage-notes","title":"Usage Notes","text":"<ol> <li>Container Runtime Selection:</li> <li>Use <code>\"docker\"</code> for Docker-based container execution.</li> <li>Use <code>\"singularity\"</code> for Singularity-based container execution.</li> <li> <p>Ensure the appropriate runtime is installed and available on the system.</p> </li> <li> <p>Environment Variables:</p> </li> <li> <p>Use the <code>env_vars</code> argument to pass environment variables to the container.</p> </li> <li> <p>Volume Mapping:</p> </li> <li> <p>Use the <code>volumes</code> argument to bind directories between the host system and the container.</p> <ul> <li>Docker: Use <code>[\"/host/path:/container/path\"]</code>.</li> <li>Singularity: Use <code>[\"/host/path:/container/path\"]</code>.</li> </ul> </li> <li> <p>Validation:</p> </li> <li>If <code>container_runtime</code> is not specified, container-specific arguments such as <code>volumes</code>, <code>env_vars</code>, and <code>image</code> cannot be used.</li> <li>A valid container image must be provided if <code>container_runtime</code> is specified.</li> </ol>"},{"location":"container/tasks/#error-handling","title":"Error Handling","text":"<ul> <li>Invalid Configurations:</li> <li> <p>Raises <code>ValueError</code> for invalid configurations, such as:</p> <ul> <li>Missing container runtime.</li> <li>Missing or invalid container image.</li> <li>Incompatible arguments (e.g., volumes provided without a container runtime).</li> </ul> </li> <li> <p>Job Failures:</p> </li> <li>Automatically cleans up failed jobs, including temporary files and job outputs.</li> </ul>"},{"location":"container/tasks/#implementation-details","title":"Implementation Details","text":"<p>Internally, <code>P.run()</code> constructs the appropriate command based on the specified runtime and arguments:</p>"},{"location":"container/tasks/#docker","title":"Docker","text":"<p>For Docker, the command is constructed as follows: <pre><code>docker run --rm -v /host/path:/container/path -e VAR=value image /bin/bash -c 'statement'\n</code></pre></p>"},{"location":"container/tasks/#singularity","title":"Singularity","text":"<p>For Singularity, the command is constructed as follows: <pre><code>singularity exec --bind /host/path:/container/path --env VAR=value image /bin/bash -c 'statement'\n</code></pre></p> <p>Both commands ensure proper execution and clean-up after the job completes.</p>"},{"location":"container/tasks/#contributing","title":"Contributing","text":"<p>To add or enhance containerisation functionality, ensure: 1. New features or fixes support both Docker and Singularity. 2. Unit tests cover all edge cases for container runtime usage. 3. Updates to this documentation reflect changes in functionality.</p>"},{"location":"container/tasks/#adding-to-mkdocs","title":"Adding to MkDocs","text":"<p>Save this content in a markdown file (e.g., <code>docs/container_execution.md</code>) and add it to the <code>mkdocs.yml</code> navigation:</p> <pre><code>nav:\n  - Home: index.md\n  - P.run:\n      - Docker and Singularity: container_execution.md\n</code></pre> <p>This provides a clear, accessible reference for users leveraging containerisation with <code>P.run()</code>.</p>"},{"location":"container/whole_pipeline/","title":"Container Configuration for Entire Pipeline","text":"<p>This document describes how to use the <code>Pipeline</code> class from <code>cgatcore.pipeline</code> to configure container settings for the entire pipeline. Unlike configuring individual jobs with container support, this method allows you to set up a consistent execution environment for all tasks across the entire workflow. This is useful for ensuring reproducibility and simplifying pipeline management.</p>"},{"location":"container/whole_pipeline/#overview","title":"Overview","text":"<p>The <code>Pipeline</code> class from <code>cgatcore.pipeline</code> allows you to: - Configure container support for tasks. - Set up Docker or Singularity containers with environment variables and volume mappings. - Seamlessly execute multiple tasks inside containers. - Configure container settings for the entire pipeline, ensuring consistent execution environments across all tasks.</p> <p>By configuring the container support at the pipeline level, all commands that are run through <code>P.run()</code> will automatically use the specified container settings.</p>"},{"location":"container/whole_pipeline/#usage-examples","title":"Usage Examples","text":""},{"location":"container/whole_pipeline/#setting-docker-as-the-default-runtime-for-the-entire-pipeline","title":"Setting Docker as the Default Runtime for the Entire Pipeline","text":"<p>Below is an example of how to use the <code>Pipeline</code> class to configure and execute all tasks in the pipeline within a Docker container:</p> <pre><code>from cgatcore.pipeline import Pipeline\n\n# Create a pipeline instance\nP = Pipeline()\n\n# Configure container support for Docker for the entire pipeline\nP.set_container_config(\n    image=\"ubuntu:20.04\",\n    volumes=[\"/data:/data\", \"/reference:/reference\"],\n    env_vars={\"THREADS\": \"4\", \"PATH\": \"/usr/local/bin:$PATH\"},\n    runtime=\"docker\"\n)\n\n# Define and run tasks - these will all run in the specified Docker container\nP.run([\n    \"bwa mem /reference/genome.fa /data/sample1.fastq &gt; /data/sample1.bam\",\n    \"bwa mem /reference/genome.fa /data/sample2.fastq &gt; /data/sample2.bam\"\n])\n</code></pre>"},{"location":"container/whole_pipeline/#setting-singularity-as-the-default-runtime-for-the-entire-pipeline","title":"Setting Singularity as the Default Runtime for the Entire Pipeline","text":"<p>Similarly, the following example shows how to use Singularity for all tasks in the pipeline:</p> <pre><code>from cgatcore.pipeline import Pipeline\n\n# Create a pipeline instance\nP = Pipeline()\n\n# Configure container support for Singularity for the entire pipeline\nP.set_container_config(\n    image=\"/path/to/ubuntu.sif\",\n    volumes=[\"/data:/data\", \"/reference:/reference\"],\n    env_vars={\"THREADS\": \"4\", \"PATH\": \"/usr/local/bin:$PATH\"},\n    runtime=\"singularity\"\n)\n\n# Define and run tasks - these will all run in the specified Singularity container\nP.run([\n    \"bwa mem /reference/genome.fa /data/sample1.fastq &gt; /data/sample1.bam\",\n    \"bwa mem /reference/genome.fa /data/sample2.fastq &gt; /data/sample2.bam\"\n])\n</code></pre>"},{"location":"container/whole_pipeline/#when-to-use-this-approach","title":"When to Use This Approach","text":"<p>This configuration approach is ideal when: - You want all tasks in the pipeline to run in the same controlled container environment without having to configure container support repeatedly for each individual command. - Consistency and reproducibility are essential, as this ensures that all tasks use the same software versions, dependencies, and environment. - You are managing complex workflows where each step depends on a well-defined environment, avoiding any variations that may arise if each step had to be configured separately.</p>"},{"location":"container/whole_pipeline/#differences-from-per-command-containerisation","title":"Differences from Per-Command Containerisation","text":"<ul> <li>Pipeline-Level Configuration: Use <code>P.set_container_config()</code> to set the container settings for the entire pipeline. Every task executed through <code>P.run()</code> will use this configuration by default.</li> <li>Per-Command Containerisation: Use container-specific arguments in <code>P.run()</code> for each task individually, which allows different tasks to use different container settings if needed. This is covered in the separate documentation titled Containerised Execution in <code>P.run()</code>.</li> </ul>"},{"location":"container/whole_pipeline/#conclusion","title":"Conclusion","text":"<p>The <code>Pipeline</code> class provides an efficient way to standardise the execution environment across all pipeline tasks. By setting container configurations at the pipeline level: - All tasks will use the same Docker or Singularity environment. - Configuration is centralised, reducing redundancy and the risk of errors. - Portability and reproducibility are enhanced, making this approach particularly useful for workflows requiring a consistent environment across multiple stages.</p> <p>With these examples, users can set up a fully containerised workflow environment for all stages of their pipeline, ensuring robust and repeatable results.</p>"},{"location":"defining_workflow/tutorial/","title":"Writing a workflow - Tutorial","text":"<p>The explicit aim of cgat-core is to allow users to quickly and easily build their own computational pipelines that will speed up your analysis workflow.</p>"},{"location":"defining_workflow/tutorial/#installation-of-cgat-core","title":"Installation of cgat-core","text":"<p>In order to begin writing a pipeline, you will need to install the cgat-core code (see installation instructions in the \"Getting Started\" section).</p>"},{"location":"defining_workflow/tutorial/#tutorial-start","title":"Tutorial start","text":""},{"location":"defining_workflow/tutorial/#setting-up-the-pipeline","title":"Setting up the pipeline","text":"<p>1. First, navigate to a directory where you want to start building your code:</p> <pre><code>mkdir test &amp;&amp; cd test &amp;&amp; mkdir configuration &amp;&amp; touch configuration/pipeline.yml &amp;&amp; touch pipeline_test.py &amp;&amp; touch ModuleTest.py\n</code></pre> <p>This command will create a directory called <code>test</code> in the current directory with the following layout:</p> <pre><code>|-- configuration\n|   \\-- pipeline.yml\n|-- pipeline_test.py\n|-- ModuleTest.py\n</code></pre> <p>The layout has the following components:</p> <ul> <li>pipeline_test.py: This is the file that will contain all of the ruffus workflows. The file needs to be named in the format <code>pipeline_&lt;name&gt;.py</code>.</li> <li>test/: Directory containing the configuration <code>.yml</code> file. The directory needs to have the same name as the <code>pipeline_&lt;name&gt;.py</code> file. This folder will contain the <code>pipeline.yml</code> configuration file.</li> <li>ModuleTest.py: This file will contain functions that will be imported into the main ruffus workflow file (<code>pipeline_test.py</code>).</li> </ul> <p>2. View the source code within <code>pipeline_test.py</code></p> <p>This is where the ruffus tasks will be written. The code begins with a docstring detailing the pipeline functionality. You should use this section to document your pipeline:</p> <pre><code>'''This pipeline is a test and this is where the documentation goes '''\n</code></pre> <p>The pipeline then needs a few utility functions to help with executing the pipeline.</p> <ul> <li>Import statements: You will need to import ruffus and cgatcore utilities:</li> </ul> <pre><code>from ruffus import *\nimport cgatcore.experiment as E\nfrom cgatcore import pipeline as P\n</code></pre> <p>Importing <code>ruffus</code> allows ruffus decorators to be used within the pipeline. Importing <code>experiment</code> from <code>cgatcore</code> provides utility functions for argument parsing, logging, and record-keeping within scripts. Importing <code>pipeline</code> from <code>cgatcore</code> provides utility functions for interfacing CGAT ruffus pipelines with an HPC cluster, uploading data to a database, and parameterisation.</p> <p>You'll also need some Python modules:</p> <pre><code>import os\nimport sys\n</code></pre> <ul> <li>Config parser: This code helps with parsing the <code>pipeline.yml</code> file:</li> </ul> <pre><code># Load options from the config file\nPARAMS = P.get_parameters([\n    \"%s/pipeline.yml\" % os.path.splitext(__file__)[0],\n    \"../pipeline.yml\",\n    \"pipeline.yml\"])\n</code></pre> <ul> <li>Pipeline configuration: We will add configurable variables to our <code>pipeline.yml</code> file so that we can modify the output of our pipeline. Open <code>pipeline.yml</code> and add the following:</li> </ul> <pre><code>database:\n   name: \"csvdb\"\n</code></pre> <p>When you run the pipeline, the configuration variables (in this case <code>csvdb</code>) can be accessed in the pipeline by <code>PARAMS[\"database_name\"]</code>.</p> <ul> <li>Database connection: This code helps with connecting to an SQLite database:</li> </ul> <pre><code>def connect():\n    '''Utility function to connect to the database.\n\n    Use this method to connect to the pipeline database.\n    Additional databases can be attached here as well.\n\n    Returns an sqlite3 database handle.\n    '''\n    dbh = sqlite3.connect(PARAMS[\"database_name\"])\n    return dbh\n</code></pre> <ul> <li>Commandline parser: This code allows the pipeline to parse arguments:</li> </ul> <pre><code>def main(argv=None):\n    if argv is None:\n        argv = sys.argv\n    P.main(argv)\n\nif __name__ == \"__main__\":\n    sys.exit(P.main(sys.argv))\n</code></pre>"},{"location":"defining_workflow/tutorial/#running-the-test-pipeline","title":"Running the test pipeline","text":"<p>You now have the bare bones layout of the pipeline, and you need some code to execute. Below is example code that you can copy and paste into your <code>pipeline_test.py</code> file. The code includes two ruffus <code>@transform</code> tasks that parse <code>pipeline.yml</code>. The first function, called <code>countWords</code>, contains a statement that counts the number of words in the file. The statement is then executed using the <code>P.run()</code> function.</p> <p>The second ruffus <code>@transform</code> function called <code>loadWordCounts</code> takes as input the output of the function <code>countWords</code> and loads the number of words into an SQLite database using <code>P.load()</code>.</p> <p>The third function, <code>full()</code>, is a dummy task that runs the entire pipeline. It has an <code>@follows</code> decorator that takes the <code>loadWordCounts</code> function, completing the pipeline chain.</p> <p>The following code should be pasted just before the Commandline parser arguments and after the database connection code:</p> <pre><code># ---------------------------------------------------\n# Specific pipeline tasks\n@transform(\"pipeline.yml\",\n           regex(\"(.*)\\.(.*)\"),\n           r\"\\1.counts\")\ndef countWords(infile, outfile):\n    '''Count the number of words in the pipeline configuration files.'''\n\n    # The command line statement we want to execute\n    statement = '''awk 'BEGIN { printf(\"word\\tfreq\\n\"); }\n    {for (i = 1; i &lt;= NF; i++) freq[$i]++}\n    END { for (word in freq) printf \"%s\\t%d\\n\", word, freq[word] }'\n    &lt; %(infile)s &gt; %(outfile)s'''\n\n    # Execute the command in the variable statement.\n    P.run(statement)\n\n@transform(countWords,\n           suffix(\".counts\"),\n           \"_counts.load\")\ndef loadWordCounts(infile, outfile):\n    '''Load results of word counting into database.'''\n    P.load(infile, outfile, \"--add-index=word\")\n\n# ---------------------------------------------------\n# Generic pipeline tasks\n@follows(loadWordCounts)\ndef full():\n    pass\n</code></pre> <p>To run the pipeline, navigate to the working directory and then run the pipeline:</p> <pre><code>python /location/to/code/pipeline_test.py config\npython /location/to/code/pipeline_test.py show full -v 5\n</code></pre> <p>This will place the <code>pipeline.yml</code> in the folder. Then run:</p> <pre><code>python /location/to/code/pipeline_test.py make full -v5 --local\n</code></pre> <p>The pipeline will then execute and count the words in the <code>yml</code> file.</p>"},{"location":"defining_workflow/tutorial/#modifying-the-test-pipeline-to-build-your-own-workflows","title":"Modifying the test pipeline to build your own workflows","text":"<p>The next step is to modify the basic code in the pipeline to fit your particular NGS workflow needs. For example, suppose you want to convert a SAM file into a BAM file, then perform flag stats on that output BAM file. The code and layout that we just wrote can be easily modified to perform this.</p> <p>The pipeline will have two steps: 1. Identify all SAM files and convert them to BAM files. 2. Take the output of step 1 and perform flag stats on that BAM file.</p> <p>The first step would involve writing a function to identify all <code>sam</code> files in a <code>data.dir/</code> directory and convert them to BAM files using <code>samtools view</code>. The second function would then take the output of the first function, perform <code>samtools flagstat</code>, and output the results as a flat <code>.txt</code> file. This would be written as follows:</p> <pre><code>@transform(\"data.dir/*.sam\",\n           regex(\"data.dir/(\\S+).sam\"),\n           r\"\\1.bam\")\ndef bamConvert(infile, outfile):\n    '''Convert a SAM file into a BAM file using samtools view.'''\n\n    statement = '''samtools view -bT /ifs/mirror/genomes/plain/hg19.fasta \\\n                   %(infile)s &gt; %(outfile)s'''\n    P.run(statement)\n\n@transform(bamConvert,\n           suffix(\".bam\"),\n           \"_flagstats.txt\")\ndef bamFlagstats(infile, outfile):\n    '''Perform flagstats on a BAM file.'''\n\n    statement = '''samtools flagstat %(infile)s &gt; %(outfile)s'''\n    P.run(statement)\n</code></pre> <p>To run the pipeline:</p> <pre><code>python /path/to/file/pipeline_test.py make full -v5\n</code></pre> <p>The BAM files and flagstats outputs should be generated.</p>"},{"location":"defining_workflow/tutorial/#parameterising-the-code-using-the-yml-file","title":"Parameterising the code using the <code>.yml</code> file","text":"<p>As a philosophy, we try and avoid any hardcoded parameters, so that any variables can be easily modified by the user without changing the code.</p> <p>Looking at the code above, the hardcoded link to the <code>hg19.fasta</code> file can be added as a customisable parameter, allowing users to specify any FASTA file depending on the genome build used. In the <code>pipeline.yml</code>, add:</p> <pre><code>genome:\n    fasta: /ifs/mirror/genomes/plain/hg19.fasta\n</code></pre> <p>In the <code>pipeline_test.py</code> code, the value can be accessed via <code>PARAMS[\"genome_fasta\"]</code>. Therefore, the code for parsing BAM files can be modified as follows:</p> <pre><code>@transform(\"data.dir/*.sam\",\n           regex(\"data.dir/(\\S+).sam\"),\n           r\"\\1.bam\")\ndef bamConvert(infile, outfile):\n    '''Convert a SAM file into a BAM file using samtools view.'''\n\n    genome_fasta = PARAMS[\"genome_fasta\"]\n\n    statement = '''samtools view -bT %(genome_fasta)s \\\n                   %(infile)s &gt; %(outfile)s'''\n    P.run(statement)\n\n@transform(bamConvert,\n           suffix(\".bam\"),\n           \"_flagstats.txt\")\ndef bamFlagstats(infile, outfile):\n    '''Perform flagstats on a BAM file.'''\n\n    statement = '''samtools flagstat %(infile)s &gt; %(outfile)s'''\n    P.run(statement)\n</code></pre> <p>Running the code again will generate the same output. However, if you had BAM files that came from a different genome build, the parameter in the <code>yml</code> file can be easily modified, the output files deleted, and the pipeline run again with the new configuration values.</p>"},{"location":"defining_workflow/writing_workflows/","title":"Writing a workflow","text":""},{"location":"defining_workflow/writing_workflows/#our-workflow-philosophy","title":"Our workflow philosophy","text":"<p>The explicit aim of CGAT-core is to allow users to quickly and easily build their own computational pipelines, speeding up their analysis workflow.</p> <p>When building pipelines, it is often useful to keep in mind the following guiding principles:</p>"},{"location":"defining_workflow/writing_workflows/#flexibility","title":"Flexibility","text":"<p>There are always new tools and insights that could be incorporated into a pipeline. Ultimately, a pipeline should be flexible, and the code should not constrain you when implementing new features.</p>"},{"location":"defining_workflow/writing_workflows/#scriptability","title":"Scriptability","text":"<p>The pipeline should be scriptable, i.e., the entire pipeline can be run within another pipeline. Similarly, parts of a pipeline should be easily duplicated to process several data streams in parallel. This is crucial in genome studies, as a single analysis will not always permit making inferences by itself. When writing a pipeline, we typically create a command line script (included in the CGAT-apps repository) and then run this script as a command line statement in the pipeline.</p>"},{"location":"defining_workflow/writing_workflows/#reproducibility","title":"Reproducibility","text":"<p>The pipeline should be fully automated so that the same inputs and configuration produce the same outputs.</p>"},{"location":"defining_workflow/writing_workflows/#reusability","title":"Reusability","text":"<p>The pipeline should be reusable on similar data, preferably requiring only changes to a configuration file (such as <code>pipeline.yml</code>).</p>"},{"location":"defining_workflow/writing_workflows/#archivability","title":"Archivability","text":"<p>Once finished, the whole project should be archivable without relying heavily on external data. This process should be simple; all project data should be self-contained, without needing to go through various directories or databases to determine dependencies.</p>"},{"location":"defining_workflow/writing_workflows/#building-a-pipeline","title":"Building a pipeline","text":"<p>The best way to build a pipeline is to start from an example. The CGAT Showcase contains a toy example of an RNA-seq analysis pipeline, demonstrating how simple workflows can be generated with minimal code. For more complex workflows, you can refer to CGAT-Flow.</p> <p>For a step-by-step tutorial on running pipelines, refer to our Getting Started Tutorial.</p> <p>To construct a pipeline from scratch, continue reading below.</p> <p>In an empty directory, create a new directory and then a Python file with the same name. For example:</p> <pre><code>mkdir test &amp;&amp; touch pipeline_test.py\n</code></pre> <p>All pipelines require a <code>.yml</code> configuration file that allows you to modify the behaviour of your code. This file is placed in the <code>test/</code> directory and should have the same name as the pipeline Python file:</p> <pre><code>touch test/pipeline.yml\n</code></pre> <p>To facilitate debugging and reading, our pipelines are designed so that the pipeline task file contains Ruffus tasks, while the code to transform and analyse data is in an associated module file.</p> <p>If you wish to create a module file, it is conventionally named using the format <code>ModuleTest.py</code>. You can import it into the main pipeline task file (<code>pipeline_test.py</code>) as follows:</p> <pre><code>import ModuleTest\n</code></pre> <p>The pipeline module in CGAT-core provides many useful functions for pipeline construction.</p>"},{"location":"defining_workflow/writing_workflows/#pipeline-input","title":"Pipeline input","text":"<p>Pipelines are executed within a dedicated working directory, which usually contains:</p> <ul> <li>A pipeline configuration file: <code>pipeline.yml</code></li> <li>Input data files, typically specified in the pipeline documentation</li> </ul> <p>Other files that might be used include external data files, such as genomes, referred to by their full path.</p> <p>Pipelines work with input files in the working directory, usually identified by their suffix. For instance, a mapping pipeline might look for any <code>.fastq.gz</code> files in the directory, perform QC on them, and map the reads to a genome sequence.</p>"},{"location":"defining_workflow/writing_workflows/#pipeline-output","title":"Pipeline output","text":"<p>The pipeline will generate files and database tables in the working directory. You can structure your files/directories in any way that fits your needs\u2014some prefer a flat structure with many files, while others use deeper hierarchies.</p> <p>To save disk space, compressed files should be used wherever possible. Most data files compress well; for example, <code>fastq</code> files often compress by up to 80%. Working with compressed files is straightforward using Unix pipes (<code>gzip</code>, <code>gunzip</code>, <code>zcat</code>).</p> <p>If you need random access to a file, load it into a database and index it appropriately. Genomic interval files can be indexed with <code>tabix</code> to allow random access.</p>"},{"location":"defining_workflow/writing_workflows/#import-statements","title":"Import statements","text":"<p>To run our pipelines, you need to import the CGAT-core Python modules into your pipeline. We recommend importing the following modules for every CGAT pipeline:</p> <pre><code>from ruffus import *\nimport cgatcore.experiment as E\nfrom cgatcore import pipeline as P\nimport cgatcore.iotools as iotools\n</code></pre> <p>Additional modules can be imported as needed.</p>"},{"location":"defining_workflow/writing_workflows/#selecting-the-appropriate-ruffus-decorator","title":"Selecting the appropriate Ruffus decorator","text":"<p>Before starting a pipeline, it is helpful to map out the steps and flow of your potential pipeline on a whiteboard. This helps identify the inputs and outputs of each task. Once you have a clear picture, determine which Ruffus decorator to use for each task. For more information on each decorator, refer to the Ruffus documentation.</p>"},{"location":"defining_workflow/writing_workflows/#running-commands-within-tasks","title":"Running commands within tasks","text":"<p>To run a command line program within a pipeline task, build a statement and call the <code>P.run()</code> method:</p> <pre><code>@transform('*.unsorted', suffix('.unsorted'), '.sorted')\ndef sortFile(infile, outfile):\n    statement = '''sort %(infile)s &gt; %(outfile)s'''\n    P.run(statement)\n</code></pre> <p>In the <code>P.run()</code> method, the environment of the caller is examined for a variable called <code>statement</code>, which is then subjected to string substitution from other variables in the local namespace. In the example above, <code>%(infile)s</code> and <code>%(outfile)s</code> are replaced with the values of <code>infile</code> and <code>outfile</code>, respectively.</p> <p>The same mechanism also allows configuration parameters to be set, as shown here:</p> <pre><code>@transform('*.unsorted', suffix('.unsorted'), '.sorted')\ndef sortFile(infile, outfile):\n    statement = '''sort -t %(tmpdir)s %(infile)s &gt; %(outfile)s'''\n    P.run(statement)\n</code></pre> <p>In this case, the configuration parameter <code>tmpdir</code> is substituted into the command.</p>"},{"location":"defining_workflow/writing_workflows/#chaining-commands-with-error-checking","title":"Chaining commands with error checking","text":"<p>If you need to chain multiple commands, you can use <code>&amp;&amp;</code> to ensure that errors in upstream commands are detected:</p> <pre><code>@transform('*.unsorted.gz', suffix('.unsorted.gz'), '.sorted')\ndef sortFile(infile, outfile):\n    statement = '''gunzip %(infile)s %(infile)s.tmp &amp;&amp;\n                  sort -t %(tmpdir)s %(infile)s.tmp &gt; %(outfile)s &amp;&amp;\n                  rm -f %(infile)s.tmp'''\n    P.run(statement)\n</code></pre> <p>Alternatively, you can achieve this more efficiently using pipes:</p> <pre><code>@transform('*.unsorted.gz', suffix('.unsorted.gz'), '.sorted.gz')\ndef sortFile(infile, outfile):\n    statement = '''gunzip &lt; %(infile)s | sort -t %(tmpdir)s | gzip &gt; %(outfile)s'''\n    P.run(statement)\n</code></pre> <p>The pipeline automatically inserts code to check for error return codes when multiple commands are combined in a pipe.</p>"},{"location":"defining_workflow/writing_workflows/#running-commands-on-a-cluster","title":"Running commands on a cluster","text":"<p>To run commands on a cluster, set <code>to_cluster=True</code>:</p> <pre><code>@files('*.unsorted.gz', suffix('.unsorted.gz'), '.sorted.gz')\ndef sortFile(infile, outfile):\n    to_cluster = True\n    statement = '''gunzip &lt; %(infile)s | sort -t %(tmpdir)s | gzip &gt; %(outfile)s'''\n    P.run(statement)\n</code></pre> <p>Pipelines will use command line options such as <code>--cluster-queue</code> and <code>--cluster-priority</code> for global job control. For instance, to change the priority when starting the pipeline:</p> <pre><code>python &lt;pipeline_script.py&gt; --cluster-priority=-20\n</code></pre> <p>To set job-specific options, you can define additional variables:</p> <pre><code>@files('*.unsorted.gz', suffix('.unsorted.gz'), '.sorted.gz')\ndef sortFile(infile, outfile):\n    to_cluster = True\n    job_queue = 'longjobs.q'\n    job_priority = -10\n    job_options = \"-pe dedicated 4 -R y\"\n    statement = '''gunzip &lt; %(infile)s | sort -t %(tmpdir)s | gzip &gt; %(outfile)s'''\n    P.run(statement)\n</code></pre> <p>The statement above will run in the queue <code>longjobs.q</code> with a priority of <code>-10</code>. It will also be executed in the parallel environment <code>dedicated</code>, using at least four cores.</p>"},{"location":"defining_workflow/writing_workflows/#combining-commands","title":"Combining commands","text":"<p>To combine commands, use <code>&amp;&amp;</code> to ensure they execute in the intended order:</p> <pre><code>statement = \"\"\"\nmodule load cutadapt &amp;&amp;\ncutadapt ...\n\"\"\"\n\nP.run(statement)\n</code></pre> <p>Without <code>&amp;&amp;</code>, the command would fail because the <code>cutadapt</code> command would execute as part of the <code>module load</code> statement.</p>"},{"location":"defining_workflow/writing_workflows/#useful-information-regarding-decorators","title":"Useful information regarding decorators","text":"<p>For a full list of Ruffus decorators that control pipeline flow, see the Ruffus documentation.</p> <p>Here are some examples of modifying an input file name to transform it into the output filename:</p>"},{"location":"defining_workflow/writing_workflows/#using-suffix","title":"Using Suffix","text":"<pre><code>@transform(pairs, suffix('.fastq.gz'), ('_trimmed.fastq.gz', '_trimmed.fastq.gz'))\n</code></pre> <p>This will transform an input <code>&lt;name_of_file&gt;.fastq.gz</code> into an output <code>&lt;name_of_file&gt;_trimmed.fastq.gz</code>.</p>"},{"location":"defining_workflow/writing_workflows/#using-regex","title":"Using Regex","text":"<pre><code>@follows(mkdir(\"new_folder.dir\"))\n@transform(pairs, regex('(\\S+).fastq.gz'), ('new_folder.dir/\\1_trimmed.fastq.gz', 'new_folder.dir/\\1_trimmed.fastq.gz'))\n</code></pre>"},{"location":"defining_workflow/writing_workflows/#using-formatter","title":"Using Formatter","text":"<pre><code>@follows(mkdir(\"new_folder.dir\"))\n@transform(pairs, formatter('(\\S+).fastq.gz'), ('new_folder.dir/{SAMPLE[0]}_trimmed.fastq.gz', 'new_folder.dir/{SAMPLE[0]}_trimmed.fastq.gz'))\n</code></pre> <p>This documentation aims to provide a comprehensive guide to writing your own workflows and pipelines. For more advanced usage, please refer to the original CGAT-core and Ruffus documentation.</p>"},{"location":"function_doc/csv2db/","title":"CGATcore CSV2DB Module","text":""},{"location":"function_doc/csv2db/#cgatcore.csv2db--csv2dbpy-utilities-for-uploading-a-table-to-database","title":"CSV2DB.py - utilities for uploading a table to database","text":"<p>:Tags: Python</p>"},{"location":"function_doc/csv2db/#cgatcore.csv2db--purpose","title":"Purpose","text":"<p>create a table from a csv separated file and load data into it.</p> <p>This module supports backends for postgres and sqlite3. Column types are auto-detected.</p> <p>.. todo::</p> <p>Use file import where appropriate to speed up loading. Currently, this is    not always the case.</p>"},{"location":"function_doc/csv2db/#cgatcore.csv2db--usage","title":"Usage","text":""},{"location":"function_doc/csv2db/#cgatcore.csv2db--documentation","title":"Documentation","text":""},{"location":"function_doc/csv2db/#cgatcore.csv2db--code","title":"Code","text":""},{"location":"function_doc/csv2db/#cgatcore.csv2db.to_sql_pkey","title":"<code>to_sql_pkey(self, frame, name, if_exists='fail', index=True, index_label=None, schema=None, dtype=None, **kwargs)</code>","text":"<p>Function to load a table with the reqirement for a primary key.</p> Source code in <code>cgatcore/csv2db.py</code> <pre><code>def to_sql_pkey(self, frame, name, if_exists='fail', index=True,\n                index_label=None, schema=None,\n                dtype=None, **kwargs):\n    '''Function to load a table with the reqirement for a primary key.'''\n    if dtype is not None:\n        from sqlalchemy.types import to_instance, TypeEngine\n        for col, my_type in dtype.items():\n            if not isinstance(to_instance(my_type), TypeEngine):\n                raise ValueError('The type of %s is not a SQLAlchemy '\n                                 'type ' % col)\n\n    table = pandas.io.sql.SQLTable(name, self,\n                                   frame=frame,\n                                   index=index,\n                                   if_exists=if_exists,\n                                   index_label=index_label,\n                                   schema=schema,\n                                   dtype=dtype, **kwargs)\n    table.create()\n    table.insert()\n</code></pre>"},{"location":"function_doc/database/","title":"CGATcore Database Module","text":""},{"location":"function_doc/database/#cgatcore.database--databasepy-database-utility-functions","title":"database.py - database utility functions","text":"<p>This module contains convenience functions to work with a relational database.</p>"},{"location":"function_doc/database/#cgatcore.database--reference","title":"Reference","text":""},{"location":"function_doc/database/#cgatcore.database.apsw_connect","title":"<code>apsw_connect(dbname=None, modname='tsv')</code>","text":"<p>attempt to connect to apsw database.</p> <p>This method will attempt to establish a connection to a database .</p>"},{"location":"function_doc/database/#cgatcore.database.apsw_connect--arguments","title":"Arguments","text":"<p>modname: string     A module name to register with sqlite dbname: string     A database name to connect to</p>"},{"location":"function_doc/database/#cgatcore.database.apsw_connect--returns","title":"Returns","text":"<p>con : object     A connection to a database.</p> Source code in <code>cgatcore/database.py</code> <pre><code>def apsw_connect(dbname=None, modname=\"tsv\"):\n    '''\n    attempt to connect to apsw database.\n\n    This method will attempt to establish a\n    connection to a database .\n\n    Arguments\n    ---------\n    modname: string\n        A module name to register with sqlite\n    dbname: string\n        A database name to connect to\n\n    Returns\n    -------\n    con : object\n        A connection to a database.\n    '''\n\n    connection = apsw.Connection(dbname)\n\n    cursor = connection.cursor()\n\n    connection.createmodule(modname, _VirtualTable())\n\n    return cursor\n</code></pre>"},{"location":"function_doc/database/#cgatcore.database.connect","title":"<code>connect(dbhandle=None, attach=None, url=None)</code>","text":"<p>attempt to connect to database.</p> <p>If <code>dbhandle</code> is an existing connection to a database, it will be returned unchanged. Otherwise, this method will attempt to establish a connection.</p>"},{"location":"function_doc/database/#cgatcore.database.connect--arguments","title":"Arguments","text":"<p>url: string     A database url dbhandle : object or string     A database handle or a connection string.</p>"},{"location":"function_doc/database/#cgatcore.database.connect--returns","title":"Returns","text":"<p>dbhandle : object     A DB-API2 conforming database handle</p> Source code in <code>cgatcore/database.py</code> <pre><code>def connect(dbhandle=None, attach=None, url=None):\n    \"\"\"attempt to connect to database.\n\n    If `dbhandle` is an existing connection to a database,\n    it will be returned unchanged. Otherwise, this method\n    will attempt to establish a connection.\n\n    Arguments\n    ---------\n    url: string\n        A database url\n    dbhandle : object or string\n        A database handle or a connection string.\n\n    Returns\n    -------\n    dbhandle : object\n        A DB-API2 conforming database handle\n    \"\"\"\n\n    if url:\n        is_sqlite3 = url.startswith(\"sqlite\")\n\n        if is_sqlite3:\n            connect_args = {'check_same_thread': False}\n        else:\n            connect_args = {}\n\n        engine = sqlalchemy.create_engine(\n            url,\n            connect_args=connect_args)\n        return engine\n\n    if isinstance(dbhandle, str):\n        try:\n            import sqlite3\n        except ImportError:\n            raise ValueError(\n                \"If an sqlite database location is passed\"\n                \" directly the sqlite3 module must be installed\")\n\n        dbhandle = sqlite3.connect(dbhandle)\n\n    cc = dbhandle.cursor()\n\n    if attach is not None:\n        if isinstance(attach, str):\n            db_execute(cc, attach)\n        elif isinstance(attach, (tuple, list)):\n            for attach_statement in attach:\n                db_execute(cc, attach_statement)\n\n    return dbhandle\n</code></pre>"},{"location":"function_doc/database/#cgatcore.database.db_execute","title":"<code>db_execute(cc, statements)</code>","text":"<p>excute a statement or statements against a cursor</p> Source code in <code>cgatcore/database.py</code> <pre><code>def db_execute(cc, statements):\n    '''excute a statement or statements against a cursor'''\n\n    if type(statements) not in (list, tuple):\n        statements = [statements]\n\n    for statement in statements:\n        cc.execute(statement)\n</code></pre>"},{"location":"function_doc/database/#cgatcore.database.execute","title":"<code>execute(queries, dbhandle=None, attach=False)</code>","text":"<p>Execute a statement or a  list of statements (sequentially)</p> Source code in <code>cgatcore/database.py</code> <pre><code>def execute(queries, dbhandle=None, attach=False):\n    '''Execute a statement or a  list of statements (sequentially)'''\n\n    cc = dbhandle.cursor()\n\n    if attach:\n        db_execute(cc, attach)\n\n    db_execute(cc, queries)\n    cc.close()\n</code></pre>"},{"location":"function_doc/database/#cgatcore.database.executewait","title":"<code>executewait(dbhandle, statement, regex_error='locked', retries=-1, wait=5)</code>","text":"<p>repeatedly execute an SQL statement until it succeeds.</p>"},{"location":"function_doc/database/#cgatcore.database.executewait--arguments","title":"Arguments","text":"<p>dbhandle : object     A DB-API conform database handle. statement : string     SQL statement to execute. error : string     Exception to catch and examine for error messages. regex_error : string     Any error message matching <code>regex_error</code> will be ignored,     otherwise the procedure exists. retries : int     Number of retries. If set to negative number, retry indefinitely.     If set to 0, there will be only one attempt. wait : int     Number of seconds to way between retries.</p>"},{"location":"function_doc/database/#cgatcore.database.executewait--returns","title":"Returns","text":"<p>A cursor object</p> Source code in <code>cgatcore/database.py</code> <pre><code>def executewait(dbhandle, statement, regex_error=\"locked\",\n                retries=-1, wait=5):\n    '''repeatedly execute an SQL statement until it succeeds.\n\n    Arguments\n    ---------\n    dbhandle : object\n        A DB-API conform database handle.\n    statement : string\n        SQL statement to execute.\n    error : string\n        Exception to catch and examine for error messages.\n    regex_error : string\n        Any error message matching `regex_error` will be ignored,\n        otherwise the procedure exists.\n    retries : int\n        Number of retries. If set to negative number, retry indefinitely.\n        If set to 0, there will be only one attempt.\n    wait : int\n        Number of seconds to way between retries.\n\n    Returns\n    -------\n    A cursor object\n\n    '''\n    while 1:\n        try:\n            cc = dbhandle.execute(statement)\n        except AttributeError:\n            with dbhandle.begin() as conn:\n                cc = conn.execute(sqlalchemy.text(statement))\n        except Exception as msg:\n            if retries == 0:\n                raise\n            if not re.search(\"locked\", str(msg)):\n                raise\n            time.sleep(wait)\n            retries -= 1\n            continue\n        break\n    return cc\n</code></pre>"},{"location":"function_doc/database/#cgatcore.database.fetch","title":"<code>fetch(query, dbhandle=None, attach=False)</code>","text":"<p>Fetch all query results and return</p> Source code in <code>cgatcore/database.py</code> <pre><code>def fetch(query, dbhandle=None, attach=False):\n    '''Fetch all query results and return'''\n\n    cc = dbhandle.cursor()\n\n    if attach:\n        db_execute(cc, attach)\n\n    sqlresult = cc.execute(query).fetchall()\n    cc.close()\n    return sqlresult\n</code></pre>"},{"location":"function_doc/database/#cgatcore.database.fetch_DataFrame","title":"<code>fetch_DataFrame(query, dbhandle=None, attach=False)</code>","text":"<p>Fetch query results and returns them as a pandas dataframe</p> Source code in <code>cgatcore/database.py</code> <pre><code>def fetch_DataFrame(query,\n                    dbhandle=None,\n                    attach=False):\n    '''Fetch query results and returns them as a pandas dataframe'''\n\n    dbhandle = connect(dbhandle, attach=attach)\n\n    cc = dbhandle.cursor()\n    sqlresult = cc.execute(query).fetchall()\n    cc.close()\n\n    # see http://pandas.pydata.org/pandas-docs/dev/generated/\n    # pandas.DataFrame.from_records.html#pandas.DataFrame.from_records\n    # this method is design to handle sql_records with proper type\n    # conversion\n\n    field_names = [d[0] for d in cc.description]\n    pandas_DataFrame = DataFrame.from_records(\n        sqlresult,\n        columns=field_names)\n    return pandas_DataFrame\n</code></pre>"},{"location":"function_doc/database/#cgatcore.database.fetch_with_names","title":"<code>fetch_with_names(query, dbhandle=None, attach=False)</code>","text":"<p>Fetch query results and returns them as an array of row arrays, in which the first entry is an array of the field names</p> Source code in <code>cgatcore/database.py</code> <pre><code>def fetch_with_names(query,\n                     dbhandle=None,\n                     attach=False):\n    '''Fetch query results and returns them as an array of row arrays, in\n       which the first entry is an array of the field names\n\n    '''\n\n    dbhandle = connect(dbhandle, attach=attach)\n\n    cc = dbhandle.cursor()\n    sqlresult = cc.execute(query).fetchall()\n\n    data = []\n    # http://stackoverflow.com/questions/4147707/\n    # python-mysqldb-sqlite-result-as-dictionary\n    field_names = [d[0] for d in cc.description]\n    data.append([name for name in field_names])\n    for record in sqlresult:\n        line = [field for field in record]\n        data.append(line)\n\n    cc.close()\n    return data\n</code></pre>"},{"location":"function_doc/database/#cgatcore.database.getColumnNames","title":"<code>getColumnNames(dbhandle, table)</code>","text":"<p>return column names of a table from a database.</p> Source code in <code>cgatcore/database.py</code> <pre><code>def getColumnNames(dbhandle, table):\n    \"\"\"return column names of a table from a database.\n    \"\"\"\n\n    cc = executewait(dbhandle, \"SELECT * FROM %s LIMIT 1\" % table)\n    return tuple([x[0] for x in cc.description])\n</code></pre>"},{"location":"function_doc/database/#cgatcore.database.getTables","title":"<code>getTables(dbhandle)</code>","text":"<p>get list of tables in an sqlite database</p> Source code in <code>cgatcore/database.py</code> <pre><code>def getTables(dbhandle):\n    \"\"\"get list of tables in an sqlite database\"\"\"\n    cc = executewait(\n        dbhandle, \"\"\"select name from sqlite_master where type='table'\"\"\")\n    return tuple([x[0] for x in cc])\n</code></pre>"},{"location":"function_doc/database/#cgatcore.database.toTSV","title":"<code>toTSV(dbhandle, outfile, statement, remove_none=True)</code>","text":"<p>execute statement and save as tsv file to disk.</p> <p>If remove_none is true, empty/NULL values will be output as empty values.</p> Source code in <code>cgatcore/database.py</code> <pre><code>def toTSV(dbhandle, outfile, statement, remove_none=True):\n    '''execute statement and save as tsv file\n    to disk.\n\n    If *remove_none* is true, empty/NULL values will be output as\n    empty values.\n\n    '''\n    cc = dbhandle.cursor()\n    cc.execute(statement)\n    outfile.write(\"\\t\".join([x[0] for x in cc.description]) + \"\\n\")\n\n    def _str(x):\n        if x is None:\n            return \"\"\n        else:\n            return str(x)\n\n    if remove_none:\n        f = _str\n    else:\n        f = str\n\n    outfile.write(\"\\n\".join(\n        [\"\\t\".join(map(f, x)) for x in cc]))\n</code></pre>"},{"location":"function_doc/database/#cgatcore.database.write_DataFrame","title":"<code>write_DataFrame(dataframe, tablename, dbhandle=None, index=False, if_exists='replace')</code>","text":"<p>write a pandas dataframe to an sqlite db, index on given columns index columns given as a string or list eg. \"gene_id\" or [\"gene_id\", \"start\"]</p> Source code in <code>cgatcore/database.py</code> <pre><code>def write_DataFrame(dataframe,\n                    tablename,\n                    dbhandle=None,\n                    index=False,\n                    if_exists='replace'):\n    '''write a pandas dataframe to an sqlite db, index on given columns\n       index columns given as a string or list eg. \"gene_id\" or\n       [\"gene_id\", \"start\"]\n\n    '''\n\n    dbhandle = connect(dbhandle)\n\n    dataframe.to_sql(tablename,\n                     con=dbhandle,\n                     flavor='sqlite',\n                     if_exists=if_exists)\n\n    def indexStat(tablename, column):\n        istat = ('create index %(tablename)s_%(column)s '\n                 'on %(tablename)s(%(column)s)') % locals()\n        return istat\n\n    if index:\n\n        cc = dbhandle.cursor()\n\n        if isinstance(index, str):\n            istat = indexStat(tablename, index)\n            print(istat)\n            db_execute(cc, istat)\n        elif isinstance(index, (tuple, list)):\n            for column in index:\n                istat = indexStat(tablename, column)\n                db_execute(cc, istat)\n\n        cc.close()\n</code></pre>"},{"location":"function_doc/experiment/","title":"CGATcore Experiment Module","text":""},{"location":"function_doc/experiment/#cgatcore.experiment--experimentpy-writing-reproducible-scripts","title":"experiment.py - writing reproducible scripts","text":"<p>The :mod:<code>experiment</code> modules contains utility functions for argument parsing, logging and record keeping within scripts.</p> <p>This module is imported by most CGAT scripts. It provides convenient and consistent methods for</p> <ul> <li><code>Record keeping</code>_</li> <li><code>Argument parsing</code>_</li> <li><code>Input/Output redirection</code>_</li> <li><code>Logging</code>_</li> <li><code>Running external commands</code>_</li> <li><code>Benchmarking</code>_</li> </ul> <p>See :doc:<code>../scripts/cgat_script_template</code> on how to use this module.</p> <p>This module can handle both optparse and argparse. The default is to return an argparse object.</p> <p>For default argparse: The basic usage of this module within a script is::</p> <pre><code>\"\"\"script_name.py - my script\n\nMode Documentation\n\"\"\"\nimport sys\nimport optparse\nimport CGAT.experiment as E\n\ndef main(argv=None):\n    \"\"\"script main.\n\n    parses command line options in sys.argv, unless *argv* is given.\n    \"\"\"\n\n    if not argv: argv = sys.argv\n\n    # setup command line parser\n    parser = E.OptionParser(description=__doc__)\n\n    parser.add_arguments(\"-t\", \"--test\", dest=\"test\", type=\"string\",\n                      help=\"supply help\")\n\n    # add common options (-h/--help, ...) and parse\n    # command line\n    args = E.Start(parser)\n\n    # do something\n    # ...\n    E.info(\"an information message\")\n    E.warn(\"a warning message)\n\n    ## write footer and output benchmark information.\n    E.Stop()\n\nif __name__ == \"__main__\":\n    sys.exit(main(sys.argv))\n</code></pre> <p>To use optparse: The basic usage of this module within a script is::</p> <pre><code>def main(argv=None):\n    \"\"\"script main.\n\n    parses command line options in sys.argv, unless *argv* is given.\n    \"\"\"\n\n    if not argv: argv = sys.argv\n\n    # setup command line parser\n    parser = E.OptionParser(version=\"%prog version: $Id$\",\n                            usage=globals()[\"__doc__\"], optparse=True)\n\n    parser.add_option(\"-t\", \"--test\", dest=\"test\", type=\"string\",\n                      help=\"supply help\")\n\n    # add common options (-h/--help, ...) and parse\n    # command line\n    (options, args) = E.Start(parser, optparse=True)\n\n    # do something\n    # ...\n    E.info(\"an information message\")\n    E.warn(\"a warning message)\n\n    ## write footer and output benchmark information.\n    E.Stop()\n\nif __name__ == \"__main__\":\n    sys.exit(main(sys.argv))\n</code></pre>"},{"location":"function_doc/experiment/#cgatcore.experiment--record-keeping","title":"Record keeping","text":"<p>The central functions in this module are the :py:func:<code>Start</code> and :py:func:<code>Stop</code> methods which are called before or after any work is done within a script.</p> <p>The :py:func:<code>Start</code> is called with an E.OptionParser object. :py:func:<code>Start</code> will add additional command line arguments, such as <code>--help</code> for command line help or <code>--verbose</code> to control the :term:<code>loglevel</code>.  It can also add optional arguments for scripts needing database access, writing to multiple output files, etc.</p> <p>:py:func:<code>Start</code> will write record keeping information to a logfile. Typically, logging information is output on stdout, prefixed by a <code>#</code>, but it can be re-directed to a separate file. Below is a typical output::</p> <pre><code># output generated by /ifs/devel/andreas/cgat/beds2beds.py --force-output --exclusive-overlap --method=unmerged-combinations --output-filename-pattern=030m.intersection.tsv.dir/030m.intersection.tsv-%s.bed.gz --log=030m.intersection.tsv.log Irf5-030m-R1.bed.gz Rela-030m-R1.bed.gz  # nopep8\n# job started at Thu Mar 29 13:06:33 2012 on cgat150.anat.ox.ac.uk -- e1c16e80-03a1-4023-9417-f3e44e33bdcd\n# pid: 16649, system: Linux 2.6.32-220.7.1.el6.x86_64 #1 SMP Fri Feb 10 15:22:22 EST 2012 x86_64\n# exclusive                               : True\n# filename_update                         : None\n# ignore_strand                           : False\n# loglevel                                : 1\n# method                                  : unmerged-combinations\n# output_filename_pattern                 : 030m.intersection.tsv.dir/030m.intersection.tsv-%s.bed.gz\n# output_force                            : True\n# pattern_id                              : (.*).bed.gz\n# stderr                                  : &lt;open file '&lt;stderr&gt;', mode 'w' at 0x2ba70e0c2270&gt;\n# stdin                                   : &lt;open file '&lt;stdin&gt;', mode 'r' at 0x2ba70e0c2150&gt;\n# stdlog                                  : &lt;open file '030m.intersection.tsv.log', mode 'a' at 0x1f1a810&gt;\n# stdout                                  : &lt;open file '&lt;stdout&gt;', mode 'w' at 0x2ba70e0c21e0&gt;\n# timeit_file                             : None\n# timeit_header                           : None\n# timeit_name                             : all\n# tracks                                  : None\n</code></pre> <p>The header contains information about:</p> <pre><code>* the script name (``beds2beds.py``)\n\n* the command line options (``--force-output --exclusive-overlap\n  --method=unmerged-combinations\n  --output-filename-pattern=030m.intersection.tsv.dir/030m.intersection.tsv-%s.bed.gz\n  --log=030m.intersection.tsv.log Irf5-030m-R1.bed.gz\n  Rela-030m-R1.bed.gz``)\n\n* the time when the job was started (``Thu Mar 29 13:06:33 2012``)\n* the location it was executed (``cgat150.anat.ox.ac.uk``)\n* a unique job id (``e1c16e80-03a1-4023-9417-f3e44e33bdcd``)\n* the pid of the job (``16649``)\n\n* the system specification (``Linux 2.6.32-220.7.1.el6.x86_64 #1\n  SMP Fri Feb 10 15:22:22 EST 2012 x86_64``)\n</code></pre> <p>It is followed by a list of all options that have been set in the script.</p> <p>Once completed, a script will call the :py:func:<code>Stop</code> function to signify the end of the experiment.</p> <p>:py:func:<code>Stop</code> will output to the log file that the script has concluded successfully. Below is typical output::</p> <pre><code># job finished in 11 seconds at Thu Mar 29 13:06:44 2012 -- 11.36 0.45 0.00 0.01       -- e1c16e80-03a1-4023-9417-f3e44e33bdcd\n</code></pre> <p>The footer contains information about:</p> <ul> <li>the job has finished (<code>job finished</code>)</li> <li>the time it took to execute (<code>11 seconds</code>)</li> <li>when it completed (<code>Thu Mar 29 13:06:44 2012</code>)</li> <li>some benchmarking information (<code>11.36  0.45  0.00  0.01</code>)      which is <code>user time</code>, <code>system time</code>,      <code>child user time</code>, <code>child system time</code>.</li> <li>the unique job id (<code>e1c16e80-03a1-4023-9417-f3e44e33bdcd</code>)</li> </ul> <p>The unique job id can be used to easily retrieve matching information from a concatenation of log files.</p>"},{"location":"function_doc/experiment/#cgatcore.experiment--argument-parsing","title":"Argument parsing","text":"<p>The module provides :class:<code>OptionParser</code> to facilitate option parsing.  :class:<code>OptionParser</code> is derived from the :py:class:<code>optparse.OptionParser</code> class, but has improvements to provide better formatted output on the command line. It also allows to provide a comma-separated list to options that accept multiple arguments. Thus, <code>--method=sort --method=crop</code> and <code>--method=sort,crop</code> are equivalent.</p> <p>Additionally, there are set of commonly used option groups that are used in many scripts. The :func:<code>Start</code> method has options to automatically add these. For example::</p> <p>(options, args) = E.Start(parser, add_output_options=True)</p> <p>will add the option <code>--output-filename-pattern</code>. Similarly::</p> <p>(options, args) = E.Start(parser, add_database_options=True)</p> <p>will add multiple options for scripts accessing databases, such as <code>--database-host</code> and <code>--database-username</code>.</p>"},{"location":"function_doc/experiment/#cgatcore.experiment--inputoutput-redirection","title":"Input/Output redirection","text":"<p>:func:<code>Start</code> adds the options <code>--stdin</code>, <code>--stderr` and</code>--stdout`` which allow using files as input/output streams.</p> <p>To make this work, scripts should not read from sys.stdin or write to sys.stdout directly, but instead use <code>options.stdin</code> and <code>options.stdout</code>. For example to simply read all lines from stdin and write to stdout, use::</p> <p>(options, args) = E.Start(parser)</p> <p>input_data = options.stdin.readlines()    options.stdout.write(\"\".join(input_data))</p> <p>The script can then be used in many different contexts::</p> <p>cat in.data | python script.py &gt; out.data    python script.py --stdin=in.data &gt; out.data    python script.py --stdin=in.data --stdout=out.data</p> <p>The method handles gzip compressed files transparently. The following are equivalent::</p> <p>zcat in.data.gz | python script.py | gzip &gt; out.data.gz    python script.py --stdin=in.data.gz --stdout=out.data.gz</p> <p>For scripts producing multiple output files, use the argument <code>add_output_options=True</code> to :func:<code>Start</code>. This provides the option <code>--output-filename-pattern</code> on the command line. The user can then supply a pattern for output files. Any <code>%s</code> appearing in the pattern will be substituted by a <code>section</code>. Inside the script, When opening an output file, use the method :func:<code>open_output_file</code> to provide a file object::</p> <p>output_histogram = E.open_output_file(section=\"histogram\")    output_stats = E.open_output_file(section=\"stats\")</p> <p>If the user calls the script with::</p> <p>python script.py --output-filename-pattern=sample1_%s.tsv.gz</p> <p>the script will create the files <code>sample1_histogram.tsv.gz</code> and <code>sample1_stats.tsv.gz</code>.</p> <p>This method will also add the option <code>--force-output</code> to permit overwriting existing files.</p>"},{"location":"function_doc/experiment/#cgatcore.experiment--logging","title":"Logging","text":"<p>:py:mod:<code>experiment</code> provides the well known logging methods from the :py:mod:<code>logging</code> module such as :py:func:<code>info</code>, :py:func:<code>warn</code>, etc. These are provided so that no additional import of the :py:mod:<code>logging</code> module is required, but either functions can be used.</p>"},{"location":"function_doc/experiment/#cgatcore.experiment--running-external-commands","title":"Running external commands","text":"<p>The :func:<code>run</code> method is a shortcut :py:func:<code>subprocess.call</code> and similar methods with some additional sanity checking.</p>"},{"location":"function_doc/experiment/#cgatcore.experiment--benchmarking","title":"Benchmarking","text":"<p>The :func:<code>Start</code> method records basic benchmarking information when a script starts and :func:<code>Stop</code> outputs it as part of its final log message::</p> <pre><code># job finished in 11 seconds at Thu Mar 29 13:06:44 2012 -- 11.36 0.45 0.00 0.01       -- e1c16e80-03a1-4023-9417-f3e44e33bdcd\n</code></pre> <p>See <code>Record keeping</code>_ for an explanations of the fields.</p> <p>To facilitate collecting benchmark information from running multiple scripts, these data can be tagged and saved in a separate file. See the command line options <code>--timeit</code>, <code>--timeit-name</code>, <code>--timeit-header</code> in :func:<code>Start</code>.</p> <p>The module contains some decorator functions for benchmarking (:func:<code>benchmark</code>) and caching function (:func:<code>cached_function</code>) or class method (:func:<code>cached_method</code>) calls.</p>"},{"location":"function_doc/experiment/#cgatcore.experiment--api","title":"API","text":""},{"location":"function_doc/experiment/#cgatcore.experiment.AppendCommaOption","title":"<code>AppendCommaOption</code>","text":"<p>               Bases: <code>Option</code></p> <p>Option with additional parsing capabilities.</p> <ul> <li> <p>\",\" in arguments to options that have the action 'append'   are treated as a list of options. This is what galaxy does,   but generally convenient.</p> </li> <li> <p>Option values of \"None\" and \"\" are treated as default values.</p> </li> </ul> Source code in <code>cgatcore/experiment.py</code> <pre><code>class AppendCommaOption(optparse.Option):\n\n    '''Option with additional parsing capabilities.\n\n    * \",\" in arguments to options that have the action 'append'\n      are treated as a list of options. This is what galaxy does,\n      but generally convenient.\n\n    * Option values of \"None\" and \"\" are treated as default values.\n    '''\n#    def check_value( self, opt, value ):\n# do not check type for ',' separated lists\n#        if \",\" in value:\n#            return value\n#        else:\n#            return optparse.Option.check_value( self, opt, value )\n#\n#    def take_action(self, action, dest, opt, value, values, parser):\n#        if action == \"append\" and \",\" in value:\n#            lvalue = value.split(\",\")\n#            values.ensure_value(dest, []).extend(lvalue)\n#        else:\n#            optparse.Option.take_action(\n#                self, action, dest, opt, value, values, parser)\n#\n\n    def convert_value(self, opt, value):\n        if value is not None:\n            if self.nargs == 1:\n                if self.action == \"append\":\n                    if \",\" in value:\n                        return [self.check_value(opt, v) for v in\n                                value.split(\",\") if v != \"\"]\n                    else:\n                        if value != \"\":\n                            return self.check_value(opt, value)\n                        else:\n                            return value\n                else:\n                    return self.check_value(opt, value)\n            else:\n                return tuple([self.check_value(opt, v) for v in value])\n\n    # why is it necessary to pass action and dest to this function when\n    # they could be accessed as self.action and self.dest?\n    def take_action(self, action, dest, opt, value, values, parser):\n\n        if action == \"append\" and isinstance(value, list):\n            values.ensure_value(dest, []).extend(value)\n        else:\n            optparse.Option.take_action(\n                self, action, dest, opt, value, values, parser)\n</code></pre>"},{"location":"function_doc/experiment/#cgatcore.experiment.ArgumentParser","title":"<code>ArgumentParser</code>","text":"<p>               Bases: <code>ArgumentParser</code></p> <p>'CGAT derivative of ArgumentParser. OptionParser is still implimented for backwards compatibility</p> Source code in <code>cgatcore/experiment.py</code> <pre><code>class ArgumentParser(argparse.ArgumentParser):\n    ''''CGAT derivative of ArgumentParser. OptionParser is still\n    implimented for backwards compatibility\n    '''\n\n    def __init__(self, *args, **kwargs):\n\n        # if \"--short\" is a command line option\n        # remove usage from kwargs\n        if \"--no-usage\" in sys.argv:\n            kwargs[\"usage\"] = None\n\n        argparse.ArgumentParser.__init__(self, *args,\n                                         formatter_class=CustomFormatter,\n                                         **kwargs)\n\n        if \"--no-usage\" in sys.argv:\n            self.add_argument(\"--no-usage\", dest=\"help_no_usage\",\n                              action=\"store_true\",\n                              help=\"output help without usage information\")\n</code></pre>"},{"location":"function_doc/experiment/#cgatcore.experiment.BetterFormatter","title":"<code>BetterFormatter</code>","text":"<p>               Bases: <code>IndentedHelpFormatter</code></p> <p>A formatter for :class:<code>OptionParser</code> outputting indented help text.</p> Source code in <code>cgatcore/experiment.py</code> <pre><code>class BetterFormatter(optparse.IndentedHelpFormatter):\n    \"\"\"A formatter for :class:`OptionParser` outputting indented\n    help text.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n\n        optparse.IndentedHelpFormatter.__init__(self, *args, **kwargs)\n        self.wrapper = textwrap.TextWrapper(width=self.width)\n\n    def _formatter(self, text):\n\n        return '\\n'.join(['\\n'.join(p) for p in\n                          map(self.wrapper.wrap,\n                              self.parser.expand_prog_name(text).split('\\n'))])\n\n    def format_description(self, description):\n\n        if description:\n            return self._formatter(description) + '\\n'\n        else:\n            return ''\n\n    def format_epilog(self, epilog):\n\n        if epilog:\n            return '\\n' + self._formatter(epilog) + '\\n'\n        else:\n            return ''\n\n    def format_usage(self, usage):\n\n        return self._formatter(optparse._(\"Usage: %s\\n\") % usage)\n\n    def format_option(self, option):\n        # Ripped and modified from Python 2.6's optparse's HelpFormatter\n        result = []\n        opts = self.option_strings[option]\n        opt_width = self.help_position - self.current_indent - 2\n        if len(opts) &gt; opt_width:\n            opts = \"%*s%s\\n\" % (self.current_indent, \"\", opts)\n            indent_first = self.help_position\n        else:                       # start help on same line as opts\n            opts = \"%*s%-*s  \" % (self.current_indent, \"\", opt_width, opts)\n            indent_first = 0\n        result.append(opts)\n        if option.help:\n            help_text = self.expand_default(option)\n            # Added expand program name\n            help_text = self.parser.expand_prog_name(help_text)\n            # Modified the generation of help_line\n            help_lines = []\n            wrapper = textwrap.TextWrapper(width=self.help_width)\n            for p in map(wrapper.wrap, help_text.split('\\n')):\n                if p:\n                    help_lines.extend(p)\n                else:\n                    help_lines.append('')\n            # End of modification\n            result.append(\"%*s%s\\n\" % (indent_first, \"\", help_lines[0]))\n            result.extend([\"%*s%s\\n\" % (self.help_position, \"\", line)\n                           for line in help_lines[1:]])\n        elif opts[-1] != \"\\n\":\n            result.append(\"\\n\")\n        return \"\".join(result)\n</code></pre>"},{"location":"function_doc/experiment/#cgatcore.experiment.Counter","title":"<code>Counter</code>","text":"<p>               Bases: <code>object</code></p> <p>a counter class.</p> <p>The counter acts both as a dictionary and a object permitting attribute access.</p> <p>Counts are automatically initialized to 0.</p> <p>Instantiate and use like this::</p> <p>c = Counter()    c.input += 1    c.output += 2    c[\"skipped\"] += 1</p> <p>print str(c)</p> Source code in <code>cgatcore/experiment.py</code> <pre><code>class Counter(object):\n    '''a counter class.\n\n    The counter acts both as a dictionary and a object permitting\n    attribute access.\n\n    Counts are automatically initialized to 0.\n\n    Instantiate and use like this::\n\n       c = Counter()\n       c.input += 1\n       c.output += 2\n       c[\"skipped\"] += 1\n\n       print str(c)\n    '''\n\n    __slots__ = [\"_counts\"]\n\n    def __init__(self):\n        \"\"\"Store data returned by function.\"\"\"\n        object.__setattr__(self, \"_counts\", collections.defaultdict(int))\n\n    def __setitem__(self, key, value):\n        self._counts[key] = value\n\n    def __getitem__(self, key):\n        return self._counts[key]\n\n    def __str__(self):\n        return \", \".join(\"%s=%i\" % x for x in self._counts.items())\n\n    def items(self):\n        return self._counts.items()\n\n    def __iadd__(self, other):\n        for key, val in other.items():\n            self._counts[key] += val\n        return self\n\n    def iteritems(self):\n        return iter(self._counts.items())\n\n    def values(self):\n        return self._counts.values()\n\n    def asTable(self, as_rows=True):\n        '''return values as tab-separated table (without header).\n\n        Key, value pairs are sorted lexicographically.\n        '''\n        if as_rows:\n            return '\\n'.join(\"%s\\t%i\" % x\n                             for x in sorted(self._counts.items()))\n        else:\n            columns, values = list(zip(*sorted(self._counts.items())))\n            return '{}\\n{}'.format(\"\\t\".join(columns),\n                                   \"\\t\".join(map(str, values)))\n\n    def __getattr__(self, name):\n        return self._counts[name]\n\n    def __setattr__(self, name, value):\n        self._counts[name] = value\n</code></pre>"},{"location":"function_doc/experiment/#cgatcore.experiment.Counter.__init__","title":"<code>__init__()</code>","text":"<p>Store data returned by function.</p> Source code in <code>cgatcore/experiment.py</code> <pre><code>def __init__(self):\n    \"\"\"Store data returned by function.\"\"\"\n    object.__setattr__(self, \"_counts\", collections.defaultdict(int))\n</code></pre>"},{"location":"function_doc/experiment/#cgatcore.experiment.Counter.asTable","title":"<code>asTable(as_rows=True)</code>","text":"<p>return values as tab-separated table (without header).</p> <p>Key, value pairs are sorted lexicographically.</p> Source code in <code>cgatcore/experiment.py</code> <pre><code>def asTable(self, as_rows=True):\n    '''return values as tab-separated table (without header).\n\n    Key, value pairs are sorted lexicographically.\n    '''\n    if as_rows:\n        return '\\n'.join(\"%s\\t%i\" % x\n                         for x in sorted(self._counts.items()))\n    else:\n        columns, values = list(zip(*sorted(self._counts.items())))\n        return '{}\\n{}'.format(\"\\t\".join(columns),\n                               \"\\t\".join(map(str, values)))\n</code></pre>"},{"location":"function_doc/experiment/#cgatcore.experiment.MultiLineFormatter","title":"<code>MultiLineFormatter</code>","text":"<p>               Bases: <code>Formatter</code></p> <p>logfile formatter: add identation for multi-line entries.</p> Source code in <code>cgatcore/experiment.py</code> <pre><code>class MultiLineFormatter(logging.Formatter):\n    '''logfile formatter: add identation for multi-line entries.'''\n\n    def format(self, record):\n\n        s = logging.Formatter.format(self, record)\n        if s.startswith(\"#\"):\n            prefix = \"#\"\n        else:\n            prefix = \"\"\n        if record.message:\n            header, footer = s.split(record.message)\n            s = s.replace(\"\\n\", \" \\\\\\n%s\" % prefix + \" \" * (len(header) - 1))\n        return s\n</code></pre>"},{"location":"function_doc/experiment/#cgatcore.experiment.OptionParser","title":"<code>OptionParser</code>","text":"<p>               Bases: <code>OptionParser</code></p> <p>CGAT derivative of ArgumentParser. OptionParser is still implemented for backwards compatibility</p> Source code in <code>cgatcore/experiment.py</code> <pre><code>class OptionParser(optparse.OptionParser):\n    '''CGAT derivative of ArgumentParser. OptionParser is still\n    implemented for backwards compatibility\n\n    '''\n\n    def __init__(self, *args, **kwargs):\n\n        # if \"--short\" is a command line option\n        # remove usage from kwargs\n        if \"--no-usage\" in sys.argv:\n            kwargs[\"usage\"] = None\n\n        optparse.OptionParser.__init__(self, *args,\n                                       option_class=AppendCommaOption,\n                                       formatter=BetterFormatter(),\n                                       **kwargs)\n\n        # set new option parser\n        # parser.formatter = BetterFormatter()\n        # parser.formatter.set_parser(parser)\n        if \"--no-usage\" in sys.argv:\n            self.add_option(\"--no-usage\", dest=\"help_no_usage\",\n                            action=\"store_true\",\n                            help=\"output help without usage information\")\n</code></pre>"},{"location":"function_doc/experiment/#cgatcore.experiment.cached_function","title":"<code>cached_function</code>","text":"<p>               Bases: <code>object</code></p> <p>Decorator that caches a function's return value each time it is called. If called later with the same arguments, the cached value is returned, and not re-evaluated.</p> <p>Taken from http://wiki.python.org/moin/PythonDecoratorLibrary#Memoize</p> Source code in <code>cgatcore/experiment.py</code> <pre><code>class cached_function(object):\n    \"\"\"Decorator that caches a function's return value each time it is called.\n    If called later with the same arguments, the cached value is returned, and\n    not re-evaluated.\n\n    Taken from http://wiki.python.org/moin/PythonDecoratorLibrary#Memoize\n    \"\"\"\n\n    def __init__(self, func):\n        self.func = func\n        self.cache = {}\n\n    def __call__(self, *args, **kwargs):\n        key = (args, frozenset(list(kwargs.items())))\n        try:\n            return self.cache[key]\n        except KeyError:\n            value = self.func(*args, **kwargs)\n            self.cache[key] = value\n            return value\n        except TypeError:\n            # uncachable -- for instance, passing a list as an argument.\n            # Better to not cache than to blow up entirely.\n            return self.func(*args, **kwargs)\n\n    def __repr__(self):\n        \"\"\"Return the function's docstring.\"\"\"\n        return self.func.__doc__\n\n    def __get__(self, obj, objtype):\n        \"\"\"Support instance methods.\"\"\"\n        return functools.partial(self.__call__, obj)\n\n    def delete(self, *args, **kwargs):\n        \"\"\"remove a cache entry\"\"\"\n        key = (args, frozenset(list(kwargs.items())))\n        del self.cache[key]\n</code></pre>"},{"location":"function_doc/experiment/#cgatcore.experiment.cached_function.__get__","title":"<code>__get__(obj, objtype)</code>","text":"<p>Support instance methods.</p> Source code in <code>cgatcore/experiment.py</code> <pre><code>def __get__(self, obj, objtype):\n    \"\"\"Support instance methods.\"\"\"\n    return functools.partial(self.__call__, obj)\n</code></pre>"},{"location":"function_doc/experiment/#cgatcore.experiment.cached_function.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the function's docstring.</p> Source code in <code>cgatcore/experiment.py</code> <pre><code>def __repr__(self):\n    \"\"\"Return the function's docstring.\"\"\"\n    return self.func.__doc__\n</code></pre>"},{"location":"function_doc/experiment/#cgatcore.experiment.cached_function.delete","title":"<code>delete(*args, **kwargs)</code>","text":"<p>remove a cache entry</p> Source code in <code>cgatcore/experiment.py</code> <pre><code>def delete(self, *args, **kwargs):\n    \"\"\"remove a cache entry\"\"\"\n    key = (args, frozenset(list(kwargs.items())))\n    del self.cache[key]\n</code></pre>"},{"location":"function_doc/experiment/#cgatcore.experiment.cached_property","title":"<code>cached_property</code>","text":"<p>               Bases: <code>object</code></p> <p>Decorator for read-only properties.</p> <p>Modified from https://wiki.python.org/moin/PythonDecoratorLibrary#Memoize</p> Source code in <code>cgatcore/experiment.py</code> <pre><code>class cached_property(object):\n    '''Decorator for read-only properties.\n\n    Modified from https://wiki.python.org/moin/PythonDecoratorLibrary#Memoize\n    '''\n\n    def __init__(self, hello):\n        print(hello)\n\n    def __call__(self, fget, doc=None):\n        self.fget = fget\n        self.__doc__ = doc or fget.__doc__\n        self.__name__ = fget.__name__\n        self.__module__ = fget.__module__\n        return self\n\n    def __get__(self, inst, owner):\n        try:\n            value = inst._cache[self.__name__]\n        except KeyError:\n            value = self.fget(inst)\n            try:\n                cache = inst._cache\n            except AttributeError:\n                cache = inst._cache = {}\n            cache[self.__name__] = value\n        return value\n</code></pre>"},{"location":"function_doc/experiment/#cgatcore.experiment.benchmark","title":"<code>benchmark(func)</code>","text":"<p>decorator collecting wall clock time spent in decorated method.</p> Source code in <code>cgatcore/experiment.py</code> <pre><code>def benchmark(func):\n    \"\"\"decorator collecting wall clock time spent in decorated method.\"\"\"\n\n    def wrapper(*arg):\n        t1 = time.time()\n        res = func(*arg)\n        t2 = time.time()\n        key = \"%s:%i\" % (func.__name__, func.__code__.co_firstlineno)\n        global_benchmark[key] += t2 - t1\n        global_options.stdlog.write(\n            '## benchmark: %s completed in %6.4f s\\n' % (key, (t2 - t1)))\n        global_options.stdlog.flush()\n        return res\n    return wrapper\n</code></pre>"},{"location":"function_doc/experiment/#cgatcore.experiment.callbackShortHelp","title":"<code>callbackShortHelp(option, opt, value, parser)</code>","text":"<p>output short help (only command line options).</p> Source code in <code>cgatcore/experiment.py</code> <pre><code>def callbackShortHelp(option, opt, value, parser):\n    '''output short help (only command line options).'''\n    # clear usage and description\n    parser.set_description(None)\n    parser.set_usage(None)\n    # output help\n    parser.print_help()\n    # exit\n    parser.exit()\n</code></pre>"},{"location":"function_doc/experiment/#cgatcore.experiment.get_args","title":"<code>get_args()</code>","text":"<p>return global options depending on parser chosen.</p> Source code in <code>cgatcore/experiment.py</code> <pre><code>def get_args():\n    \"\"\"return global options depending on parser chosen.\n    \"\"\"\n    if isinstance(global_args, list):\n        return global_options\n    return global_args\n</code></pre>"},{"location":"function_doc/experiment/#cgatcore.experiment.get_footer","title":"<code>get_footer()</code>","text":"<p>return a header string with command line options and timestamp.</p> Source code in <code>cgatcore/experiment.py</code> <pre><code>def get_footer():\n    \"\"\"return a header string with command line options and\n    timestamp.\n    \"\"\"\n    return \"job finished in %i seconds at %s -- %s -- %s\" %\\\n           (time.time() - global_starting_time,\n            time.asctime(time.localtime(time.time())),\n            \" \".join([\"%5.2f\" % x for x in os.times()[:4]]),\n            global_id)\n</code></pre>"},{"location":"function_doc/experiment/#cgatcore.experiment.get_header","title":"<code>get_header()</code>","text":"<p>return a header string with command line options and timestamp</p> Source code in <code>cgatcore/experiment.py</code> <pre><code>def get_header():\n    \"\"\"return a header string with command line options and timestamp\n\n    \"\"\"\n    system, host, release, version, machine = os.uname()\n\n    return \"output generated by %s\\njob started at %s on %s -- %s\\npid: %i, system: %s %s %s %s\" %\\\n           (\" \".join(sys.argv),\n            time.asctime(time.localtime(time.time())),\n            host,\n            global_id,\n            os.getpid(),\n            system, release, version, machine)\n</code></pre>"},{"location":"function_doc/experiment/#cgatcore.experiment.get_output_file","title":"<code>get_output_file(section, suffix=None)</code>","text":"<p>return filename to write to, replacing any <code>%s</code> with section in the output pattern for files (<code>--output-filename-pattern</code>).</p>"},{"location":"function_doc/experiment/#cgatcore.experiment.get_output_file--arguments","title":"Arguments","text":"<p>section : string     section will replace any %s in the pattern for output files suffix : string     optional suffix to append to the filename. If provided, it will be     added before any existing extension, or at the end if no extension exists.</p> Source code in <code>cgatcore/experiment.py</code> <pre><code>def get_output_file(section, suffix=None):\n    '''return filename to write to, replacing any ``%s`` with section in\n    the output pattern for files (``--output-filename-pattern``).\n\n    Arguments\n    ---------\n    section : string\n        section will replace any %s in the pattern for output files\n    suffix : string\n        optional suffix to append to the filename. If provided, it will be\n        added before any existing extension, or at the end if no extension exists.\n    '''\n    filename = re.sub(\"%s\", section, get_args().output_filename_pattern)\n\n    if suffix:\n        # Split filename into base and extension (if any)\n        root, ext = os.path.splitext(filename)\n        # Add suffix before extension (or at end if no extension)\n        filename = root + suffix + ext\n\n    return filename\n</code></pre>"},{"location":"function_doc/experiment/#cgatcore.experiment.get_params","title":"<code>get_params(options=None)</code>","text":"<p>return a string containing script parameters.</p> <p>Parameters are all variables that start with <code>param_</code>.</p> Source code in <code>cgatcore/experiment.py</code> <pre><code>def get_params(options=None):\n    \"\"\"return a string containing script parameters.\n\n    Parameters are all variables that start with ``param_``.\n    \"\"\"\n    result = []\n    if options:\n        members = options.__dict__\n        for k, v in sorted(members.items()):\n            result.append(\"%-40s: %s\" % (k, str(v)))\n    else:\n        vars = inspect.currentframe().f_back.f_locals\n        for var in [x for x in list(vars.keys()) if re.match(\"param_\", x)]:\n            result.append(\"%-40s: %s\" %\n                          (var, str(vars[var])))\n\n    if result:\n        return \"\\n\".join(result)\n    else:\n        return \"# no parameters.\"\n</code></pre>"},{"location":"function_doc/experiment/#cgatcore.experiment.open_file","title":"<code>open_file(filename, mode='r', create_dir=False, encoding='utf-8')</code>","text":"<p>open file in filename with mode mode.</p> <p>If create is set, the directory containing filename will be created if it does not exist.</p> <p>gzip - compressed files are recognized by the suffix <code>.gz</code> and opened transparently.</p> <p>Note that there are differences in the file like objects returned, for example in the ability to seek.</p> <p>returns a file or file-like object.</p> Source code in <code>cgatcore/experiment.py</code> <pre><code>def open_file(filename, mode=\"r\", create_dir=False, encoding=\"utf-8\"):\n    '''open file in *filename* with mode *mode*.\n\n    If *create* is set, the directory containing filename\n    will be created if it does not exist.\n\n    gzip - compressed files are recognized by the\n    suffix ``.gz`` and opened transparently.\n\n    Note that there are differences in the file\n    like objects returned, for example in the\n    ability to seek.\n\n    returns a file or file-like object.\n    '''\n\n    _, ext = os.path.splitext(filename)\n\n    if create_dir:\n        dirname = os.path.abspath(os.path.dirname(filename))\n        if dirname and not os.path.exists(dirname):\n            os.makedirs(dirname)\n\n    if ext.lower() in (\".gz\", \".z\"):\n        # in gzip, r defaults to \"rt\", so make it compatible with open\n        if mode == \"r\":\n            mode = \"rt\"\n        elif mode == \"w\":\n            mode = \"wt\"\n        return gzip.open(filename, mode, encoding=encoding)\n    else:\n        return open(filename, mode, encoding=encoding)\n</code></pre>"},{"location":"function_doc/experiment/#cgatcore.experiment.open_output_file","title":"<code>open_output_file(section, mode='w', encoding='utf-8')</code>","text":"<p>open file for writing substituting section in the output_pattern (if defined).</p> <p>This method will automatically create any parent directories that are missing.</p> <p>If the filename ends with \".gz\", the output is opened as a gzip'ed file.</p>"},{"location":"function_doc/experiment/#cgatcore.experiment.open_output_file--arguments","title":"Arguments","text":"<p>section : string     section will replace any %s in the pattern for output files</p> char <p>file opening mode</p>"},{"location":"function_doc/experiment/#cgatcore.experiment.open_output_file--returns","title":"Returns","text":"<p>File     an opened file</p> Source code in <code>cgatcore/experiment.py</code> <pre><code>def open_output_file(section, mode=\"w\", encoding=\"utf-8\"):\n    \"\"\"open file for writing substituting section in the\n    output_pattern (if defined).\n\n    This method will automatically create any parent directories that\n    are missing.\n\n    If the filename ends with \".gz\", the output is opened as a gzip'ed\n    file.\n\n    Arguments\n    ---------\n    section : string\n        section will replace any %s in the pattern for output files\n\n    mode : char\n        file opening mode\n\n    Returns\n    -------\n    File\n        an opened file\n\n    \"\"\"\n\n    fn = get_output_file(section)\n    args = get_args()\n\n    if fn == \"-\":\n        return args.stdout\n\n    if not args.output_force and os.path.exists(fn):\n        raise OSError(\n            \"file %s already exists, use --force-output to \"\n            \"overwrite existing files.\".format(fn))\n\n    return open_file(fn, mode=mode, create_dir=True, encoding=encoding)\n</code></pre>"},{"location":"function_doc/experiment/#cgatcore.experiment.run","title":"<code>run(statement, return_stdout=False, return_stderr=False, return_popen=False, on_error='raise', encoding='utf-8', **kwargs)</code>","text":"<p>execute a command line statement.</p> <p>By default this method returns the code returned by the executed command. If return_stdout is True, the contents of stdout are returned as a string, likewise for return_stderr.</p> <p>If return_popen, the Popen object is returned.</p> <p><code>kwargs</code> are passed on to subprocess.call, subprocess.check_output or subprocess.Popen.</p>"},{"location":"function_doc/experiment/#cgatcore.experiment.run--arguments","title":"Arguments","text":"string <p>Action to perform on error. Valid actions are \"ignore\" and \"raise\".</p>"},{"location":"function_doc/experiment/#cgatcore.experiment.run--raises","title":"Raises","text":"<p>OSError    If process failed or was terminated.</p> Source code in <code>cgatcore/experiment.py</code> <pre><code>def run(statement,\n        return_stdout=False,\n        return_stderr=False,\n        return_popen=False,\n        on_error=\"raise\",\n        encoding=\"utf-8\",\n        **kwargs):\n    '''execute a command line statement.\n\n    By default this method returns the code returned by the executed\n    command. If *return_stdout* is True, the contents of stdout are\n    returned as a string, likewise for *return_stderr*.\n\n    If *return_popen*, the Popen object is returned.\n\n    ``kwargs`` are passed on to subprocess.call,\n    subprocess.check_output or subprocess.Popen.\n\n    Arguments\n    ---------\n\n    on_error: string\n       Action to perform on error. Valid actions are \"ignore\" and \"raise\".\n\n    Raises\n    ------\n\n    OSError\n       If process failed or was terminated.\n\n    '''\n\n    # remove new lines\n    statement = \" \".join(re.sub(\"\\t+\", \" \", statement).split(\"\\n\")).strip()\n\n    if \"&lt;(\" in statement:\n        shell = os.environ.get('SHELL', \"/bin/bash\")\n        if \"bash\" not in shell:\n            raise ValueError(\n                \"require bash for advanced shell syntax: &lt;()\")\n        # Note: pipes.quote is deprecated. In Py3, use shlex.quote\n        # (not present in Py2.7)\n        statement = \"%s -c %s\" % (shell, pipes.quote(statement))\n\n    if return_stdout:\n        try:\n            output = subprocess.check_output(statement, shell=True, **kwargs)\n        except subprocess.CalledProcessError as e:\n            if on_error == \"raise\":\n                raise\n            output = e.output\n        return output.decode(encoding)\n\n    elif return_stderr:\n        # expect that process fails\n        p = subprocess.Popen(statement,\n                             shell=True,\n                             stdout=subprocess.PIPE,\n                             stderr=subprocess.PIPE,\n                             **kwargs)\n        stdout, stderr = p.communicate()\n        return stderr.decode(encoding)\n\n    elif return_popen:\n        return subprocess.Popen(statement, shell=True, **kwargs)\n\n    else:\n        retcode = subprocess.call(statement, shell=True, **kwargs)\n        if retcode &lt; 0:\n            raise OSError(\n                \"process was terminated by signal: {}\".format(-retcode))\n        elif retcode &gt; 0:\n            raise OSError(\"process exited with code: {}\".format(retcode))\n        return retcode\n</code></pre>"},{"location":"function_doc/experiment/#cgatcore.experiment.start","title":"<code>start(parser=None, argv=None, quiet=False, no_parsing=False, add_csv_options=False, add_database_options=False, add_pipe_options=True, add_cluster_options=False, add_output_options=False, logger_callback=None, return_parser=False, unknowns=False)</code>","text":"<p>set up an experiment.</p> <p>The :py:func:<code>Start</code> method will set up a file logger and add some default and some optional options to the command line parser.  It will then parse the command line and set up input/output redirection and start a timer for benchmarking purposes.</p> <p>The default options added by this method are:</p> <p><code>-v/--verbose</code>     the :term:<code>loglevel</code></p> <p><code>timeit</code>     turn on benchmarking information and save to file</p> <p><code>timeit-name</code>      name to use for timing information,</p> <p><code>timeit-header</code>      output header for timing information.</p> <p><code>seed</code>      the random seed. If given, the python random      number generator will be initialized with this      seed.</p> <p>Optional options added are:</p> <p>add_csv_options</p> <p><code>dialect</code>         csv_dialect. the default is <code>excel-tab</code>, defaulting to         :term:<code>tsv</code> formatted files.</p> <p>add_database_options    <code>-C/--connection</code>        psql connection string    <code>-U/--user</code>        psql user name</p> <p>add_cluster_options    <code>--use-cluster</code>        use cluster    <code>--cluster-priority</code>        cluster priority to request    <code>--cluster-queue</code>        cluster queue to use    <code>--cluster-num-jobs</code>        number of jobs to submit to the cluster at the same time    <code>--cluster-options</code>        additional options to the cluster for each job.</p> <p>add_output_options    <code>-P/--output-filename-pattern</code>         Pattern to use for output filenames.</p>"},{"location":"function_doc/experiment/#cgatcore.experiment.start--arguments","title":"Arguments","text":":py:class:<code>E.OptionParser</code> <p>instance with command line options.</p> list <p>command line options to parse. Defaults to :py:data:<code>sys.argv</code></p> bool <p>set :term:<code>loglevel</code> to 0 - no logging</p> bool <p>do not parse command line options</p> bool <p>return the parser object, no parsing. Useful for inspecting the command line options of a script without running it.</p> bool <p>add common options for parsing :term:<code>tsv</code> separated files.</p> bool <p>add common options for connecting to various databases.</p> bool <p>add common options for redirecting input/output</p> bool <p>add common options for scripts submitting jobs to the cluster</p> bool <p>add commond options for working with multiple output files</p> object <p>callback function to further configure logging system. The callback should accept the options as first parameter and return a logger.</p> bool <p>if a set of unknown args are to be returned</p> bool <p>specify if parser type is either optparse or argparse</p>"},{"location":"function_doc/experiment/#cgatcore.experiment.start--returns","title":"Returns","text":"<p>tuple    (:py:class:<code>E.OptionParser</code> object, list of positional    arguments)</p> Source code in <code>cgatcore/experiment.py</code> <pre><code>def start(parser=None,\n          argv=None,\n          quiet=False,\n          no_parsing=False,\n          add_csv_options=False,\n          add_database_options=False,\n          add_pipe_options=True,\n          add_cluster_options=False,\n          add_output_options=False,\n          logger_callback=None,\n          return_parser=False,\n          unknowns=False):\n    \"\"\"set up an experiment.\n\n    The :py:func:`Start` method will set up a file logger and add some\n    default and some optional options to the command line parser.  It\n    will then parse the command line and set up input/output\n    redirection and start a timer for benchmarking purposes.\n\n    The default options added by this method are:\n\n    ``-v/--verbose``\n        the :term:`loglevel`\n\n    ``timeit``\n        turn on benchmarking information and save to file\n\n    ``timeit-name``\n         name to use for timing information,\n\n    ``timeit-header``\n         output header for timing information.\n\n    ``seed``\n         the random seed. If given, the python random\n         number generator will be initialized with this\n         seed.\n\n    Optional options added are:\n\n    add_csv_options\n\n       ``dialect``\n            csv_dialect. the default is ``excel-tab``, defaulting to\n            :term:`tsv` formatted files.\n\n    add_database_options\n       ``-C/--connection``\n           psql connection string\n       ``-U/--user``\n           psql user name\n\n    add_cluster_options\n       ``--use-cluster``\n           use cluster\n       ``--cluster-priority``\n           cluster priority to request\n       ``--cluster-queue``\n           cluster queue to use\n       ``--cluster-num-jobs``\n           number of jobs to submit to the cluster at the same time\n       ``--cluster-options``\n           additional options to the cluster for each job.\n\n    add_output_options\n       ``-P/--output-filename-pattern``\n            Pattern to use for output filenames.\n\n    Arguments\n    ---------\n\n    param parser : :py:class:`E.OptionParser`\n       instance with command line options.\n\n    argv : list\n        command line options to parse. Defaults to\n        :py:data:`sys.argv`\n\n    quiet : bool\n        set :term:`loglevel` to 0 - no logging\n\n    no_parsing : bool\n        do not parse command line options\n\n    return_parser : bool\n        return the parser object, no parsing. Useful for inspecting\n        the command line options of a script without running it.\n\n    add_csv_options : bool\n        add common options for parsing :term:`tsv` separated files.\n\n    add_database_options : bool\n        add common options for connecting to various databases.\n\n    add_pipe_options : bool\n        add common options for redirecting input/output\n\n    add_cluster_options : bool\n        add common options for scripts submitting jobs to the cluster\n\n    add_output_options : bool\n        add commond options for working with multiple output files\n\n    logger_callback : object\n        callback function to further configure logging system. The\n        callback should accept the options as first parameter and\n        return a logger.\n\n    unknowns : bool\n        if a set of unknown args are to be returned\n\n    optparse : bool\n        specify if parser type is either optparse or argparse\n\n    Returns\n    -------\n    tuple\n       (:py:class:`E.OptionParser` object, list of positional\n       arguments)\n\n    \"\"\"\n\n    if argv is None:\n        argv = sys.argv\n\n    global global_args, global_starting_time\n\n    global_starting_time = time.time()\n\n    # Argparse options\n    if \"OptionParser\" in str(parser.__class__):\n        if not parser:\n            parser = OptionParser(version=\"%prog version: $Id$\")\n\n        global global_options\n\n        # save default values given by user\n        user_defaults = copy.copy(parser.defaults)\n\n        group = OptionGroup(parser, \"Script timing options\")\n\n        group.add_option(\"--timeit\", dest='timeit_file', type=\"string\",\n                         help=\"store timeing information in file [%default].\")\n        group.add_option(\"--timeit-name\", dest='timeit_name', type=\"string\",\n                         help=\"name in timing file for this class of jobs \"\n                         \"[%default].\")\n        group.add_option(\"--timeit-header\", dest='timeit_header',\n                         action=\"store_true\",\n                         help=\"add header for timing information [%default].\")\n        parser.add_option_group(group)\n\n        group = OptionGroup(parser, \"Common options\")\n\n        group.add_option(\"--random-seed\", dest='random_seed', type=\"int\",\n                         help=\"random seed to initialize number generator \"\n                         \"with [%default].\")\n\n        group.add_option(\"-v\", \"--verbose\", dest=\"loglevel\", type=\"int\",\n                         help=\"loglevel [%default]. The higher, the more output.\")\n\n        group.add_option(\"--log-config-filename\",\n                         dest=\"log_config_filename\",\n                         type=\"string\",\n                         default=\"logging.yml\",\n                         help=\"Configuration file for logger [%default].\")\n\n        group.add_option(\"--tracing\", dest=\"tracing\", type=\"choice\",\n                         choices=(\"function\",),\n                         default=None,\n                         help=\"enable function tracing [%default].\")\n\n        group.add_option(\"-?\", dest=\"short_help\", action=\"callback\",\n                         callback=callbackShortHelp,\n                         help=\"output short help (command line options only.\")\n\n        parser.add_option_group(group)\n\n        if quiet:\n            parser.set_defaults(loglevel=0)\n        else:\n            parser.set_defaults(loglevel=1)\n\n        parser.set_defaults(\n            timeit_file=None,\n            timeit_name='all',\n            timeit_header=None,\n            random_seed=None,\n            tracing=None,\n        )\n\n        if add_csv_options:\n            parser.add_option(\"--csv-dialect\", dest=\"csv_dialect\", type=\"string\",\n                              help=\"csv dialect to use [%default].\")\n\n            parser.set_defaults(\n                csv_dialect=\"excel-tab\",\n                csv_lineterminator=\"\\n\",\n            )\n\n        if add_cluster_options:\n            group = OptionGroup(parser, \"cluster options\")\n            group.add_option(\"--no-cluster\", \"--local\", dest=\"without_cluster\",\n                             action=\"store_true\",\n                             help=\"do no use cluster - run locally [%default].\")\n            group.add_option(\"--cluster-priority\", dest=\"cluster_priority\",\n                             type=\"int\",\n                             help=\"set job priority on cluster [%default].\")\n            group.add_option(\"--cluster-queue\", dest=\"cluster_queue\",\n                             type=\"string\",\n                             help=\"set cluster queue [%default].\")\n            group.add_option(\"--cluster-num-jobs\", dest=\"cluster_num_jobs\",\n                             type=\"int\",\n                             help=\"number of jobs to submit to the queue execute \"\n                             \"in parallel [%default].\")\n            group.add_option(\"--cluster-parallel-environment\",\n                             dest=\"cluster_parallel_environment\",\n                             type=\"string\",\n                             help=\"name of the parallel environment to use \"\n                             \"[%default].\")\n            group.add_option(\"--cluster-options\", dest=\"cluster_options\",\n                             type=\"string\",\n                             help=\"additional options for cluster jobs, passed \"\n                             \"on to queuing system [%default].\")\n            group.add_option(\"--cluster-queue-manager\",\n                             dest=\"cluster_queue_manager\",\n                             type=\"choice\",\n                             choices=(\"sge\", \"slurm\", \"torque\", \"pbspro\"),\n                             help=\"cluster queuing system \"\n                             \"[%default].\")\n            group.add_option(\"--cluster-memory-resource\",\n                             dest=\"cluster_memory_resource\",\n                             type=\"string\",\n                             help=\"resource name to allocate memory with \"\n                             \"[%default].\")\n            group.add_option(\"--cluster-memory-default\",\n                             dest=\"cluster_memory_default\",\n                             type=\"string\",\n                             help=\"default amount of memory to allocate \"\n                             \"[%default].\")\n\n            parser.set_defaults(without_cluster=False,\n                                cluster_queue=None,\n                                cluster_priority=None,\n                                cluster_num_jobs=None,\n                                cluster_parallel_environment=None,\n                                cluster_options=None,\n                                cluster_memory_resource=None,\n                                cluster_memory_default=\"unlimited\",\n                                cluster_queue_manager=\"sge\")\n            parser.add_option_group(group)\n\n        if add_output_options or add_pipe_options:\n            group = OptionGroup(parser, \"Input/output options\")\n\n            if add_output_options:\n                group.add_option(\n                    \"-P\", \"--output-filename-pattern\",\n                    dest=\"output_filename_pattern\", type=\"string\",\n                    help=\"OUTPUT filename pattern for various methods \"\n                    \"[%default].\")\n\n                group.add_option(\"-F\", \"--force-output\", dest=\"output_force\",\n                                 action=\"store_true\",\n                                 help=\"force over-writing of existing files.\")\n\n                parser.set_defaults(output_filename_pattern=\"%s\",\n                                    output_force=False)\n\n            if add_pipe_options:\n\n                group.add_option(\"-I\", \"--stdin\", dest=\"stdin\", type=\"string\",\n                                 help=\"file to read stdin from [default = stdin].\",\n                                 metavar=\"FILE\")\n                group.add_option(\"-L\", \"--log\", dest=\"stdlog\", type=\"string\",\n                                 help=\"file with logging information \"\n                                 \"[default = stdout].\",\n                                 metavar=\"FILE\")\n                group.add_option(\"-E\", \"--error\", dest=\"stderr\", type=\"string\",\n                                 help=\"file with error information \"\n                                 \"[default = stderr].\",\n                                 metavar=\"FILE\")\n                group.add_option(\"-S\", \"--stdout\", dest=\"stdout\", type=\"string\",\n                                 help=\"file where output is to go \"\n                                 \"[default = stdout].\",\n                                 metavar=\"FILE\")\n\n                parser.set_defaults(stderr=sys.stderr)\n                parser.set_defaults(stdout=sys.stdout)\n                parser.set_defaults(stdlog=sys.stdout)\n                parser.set_defaults(stdin=sys.stdin)\n\n            parser.add_option_group(group)\n\n        if add_database_options:\n            group = OptionGroup(parser, \"database connection options\")\n            group.add_option(\n                \"--database-url\", dest=\"database_url\", type=\"string\",\n                help=\"database connection url, for example sqlite:///./csvdb [%default].\")\n\n            group.add_option(\n                \"--database-schema\", dest=\"database_schema\", type=\"string\",\n                help=\"database schema [%default]\")\n\n            parser.set_defaults(database_url=\"sqlite:///./csvdb\")\n            parser.set_defaults(database_schema=None)\n            parser.add_option_group(group)\n\n        # restore user defaults\n        parser.defaults.update(user_defaults)\n\n        if return_parser:\n            return parser\n\n        if not no_parsing:\n            (global_options, global_args) = parser.parse_args(argv[1:])\n\n        opt1, opt2 = global_options, global_args\n        retval = (opt1, opt2)\n\n    # Argparse options\n    else:\n        group = parser.add_argument_group(\"Script timing options\")\n\n        group.add_argument(\"--timeit\", dest='timeit_file', type=str,\n                           help=\"store timeing information in file.\")\n        group.add_argument(\"--timeit-name\", dest='timeit_name', type=str,\n                           help=\"name in timing file for this class of jobs \")\n        group.add_argument(\"--timeit-header\", dest='timeit_header',\n                           action=\"store_true\",\n                           help=\"add header for timing information.\")\n\n        group = parser.add_argument_group(\"Common options\")\n\n        group.add_argument(\"--random-seed\", dest='random_seed', type=int,\n                           help=\"random seed to initialize number generator \"\n                           \"with\")\n\n        group.add_argument(\"-v\", \"--verbose\", dest=\"loglevel\", type=int,\n                           help=\"loglevel. The higher, the more output.\")\n\n        group.add_argument(\"--log-config-filename\",\n                           dest=\"log_config_filename\",\n                           type=str,\n                           default=\"logging.yml\",\n                           help=\"Configuration file for logger.\")\n\n        group.add_argument(\"--tracing\", dest=\"tracing\", type=str,\n                           choices=[\"function\"],\n                           default=None,\n                           help=\"enable function tracing.\")\n\n        group.add_argument(\"-?\", type=callbackShortHelp,\n                           help=\"output short help (command line options only.\")\n\n        if quiet:\n            parser.set_defaults(loglevel=0)\n        else:\n            parser.set_defaults(loglevel=1)\n\n        parser.set_defaults(\n            timeit_file=None,\n            timeit_name='all',\n            timeit_header=None,\n            random_seed=None,\n            tracing=None,\n        )\n\n        if add_csv_options:\n            parser.add_argument(\"--csv-dialect\", dest=\"csv_dialect\", type=str,\n                                help=\"csv dialect to use.\")\n\n            parser.set_defaults(\n                csv_dialect=\"excel-tab\",\n                csv_lineterminator=\"\\n\",\n            )\n\n        if add_cluster_options:\n            group = parser.add_argument_group(\"cluster options\")\n            group.add_argument(\"--no-cluster\", \"--local\", dest=\"without_cluster\",\n                               action=\"store_true\",\n                               help=\"do no use cluster - run locally.\")\n            group.add_argument(\"--cluster-priority\", dest=\"cluster_priority\",\n                               type=int,\n                               help=\"set job priority on cluster.\")\n            group.add_argument(\"--cluster-queue\", dest=\"cluster_queue\",\n                               type=str,\n                               help=\"set cluster queue.\")\n            group.add_argument(\"--cluster-num-jobs\", dest=\"cluster_num_jobs\",\n                               type=int,\n                               help=\"number of jobs to submit to the queue execute \"\n                               \"in parallel.\")\n            group.add_argument(\"--cluster-parallel\",\n                               dest=\"cluster_parallel_environment\",\n                               type=str,\n                               help=\"name of the parallel environment to use \")\n            group.add_argument(\"--cluster-options\", dest=\"cluster_options\",\n                               type=str,\n                               help=\"additional options for cluster jobs, passed \"\n                               \"on to queuing system.\")\n            group.add_argument(\"--cluster-queue-manager\",\n                               dest=\"cluster_queue_manager\",\n                               type=str,\n                               choices=[\"sge\", \"slurm\", \"torque\", \"pbspro\"],\n                               help=\"cluster queuing system \")\n            group.add_argument(\"--cluster-memory-resource\",\n                               dest=\"cluster_memory_resource\",\n                               type=str,\n                               help=\"resource name to allocate memory with \")\n            group.add_argument(\"--cluster-memory-default\",\n                               dest=\"cluster_memory_default\",\n                               type=str,\n                               help=\"default amount of memory to allocate \")\n\n            parser.set_defaults(without_cluster=False,\n                                cluster_queue=None,\n                                cluster_priority=None,\n                                cluster_num_jobs=None,\n                                cluster_parallel_environment=None,\n                                cluster_options=None,\n                                cluster_memory_resource=None,\n                                cluster_memory_default=\"unlimited\",\n                                cluster_queue_manager=\"sge\")\n\n        if add_output_options or add_pipe_options:\n            group = parser.add_argument_group(\"Input/output options\")\n\n            if add_output_options:\n                group.add_argument(\n                    \"-P\", \"--output-filename-pattern\",\n                    dest=\"output_filename_pattern\", type=str,\n                    help=\"OUTPUT filename pattern for various methods \")\n\n                group.add_argument(\"-F\", \"--force-output\", dest=\"output_force\",\n                                   action=\"store_true\",\n                                   help=\"force over-writing of existing files.\")\n\n                parser.set_defaults(output_filename_pattern=\"%s\",\n                                    output_force=False)\n\n            if add_pipe_options:\n\n                group.add_argument(\"-I\", \"--stdin\", dest=\"stdin\", type=str,\n                                   help=\"file to read stdin from.\")\n                group.add_argument(\"-L\", \"--log\", dest=\"stdlog\", type=str,\n                                   help=\"file with logging information.\")\n                group.add_argument(\"-E\", \"--error\", dest=\"stderr\", type=str,\n                                   help=\"file with error information.\")\n                group.add_argument(\"-S\", \"--stdout\", dest=\"stdout\", type=str,\n                                   help=\"file where output is to go. \")\n\n                parser.set_defaults(stderr=sys.stderr)\n                parser.set_defaults(stdout=sys.stdout)\n                parser.set_defaults(stdlog=sys.stdout)\n                parser.set_defaults(stdin=sys.stdin)\n\n        if add_database_options:\n            group = parser.add_argument_group(\"database connection options\")\n            group.add_argument(\n                \"--database-url\", dest=\"database_url\", type=str,\n                help=\"database connection url, for example sqlite:///./csvdb.\")\n\n            group.add_argument(\n                \"--database-schema\", dest=\"database_schema\", type=str,\n                help=\"database schem\")\n\n            parser.set_defaults(database_url=\"sqlite:///./csvdb\")\n            parser.set_defaults(database_schema=None)\n\n        if return_parser:\n            return parser\n\n        if not no_parsing:\n            global_args, unknown = parser.parse_known_args(argv[1:])\n\n        if unknowns:\n            opt1, opt2 = global_args, unknown\n            retval = (opt1, opt2)\n        else:\n            opt1, opt2 = global_args, None\n            retval = opt1\n\n    _setup_seed(opt1)\n    _setup_pipes(opt1, add_pipe_options)\n    _setup_tracing(opt1)\n    logger = _setup_logging(opt1, logger_callback)\n\n    logger.info(get_header())\n    logger.info(get_params(opt1))\n\n    return retval\n</code></pre>"},{"location":"function_doc/experiment/#cgatcore.experiment.stop","title":"<code>stop(logger=None)</code>","text":"<p>stop the experiment.</p> <p>This method performs final book-keeping, closes the output streams and writes the final log messages indicating script completion.</p> Source code in <code>cgatcore/experiment.py</code> <pre><code>def stop(logger=None):\n    \"\"\"stop the experiment.\n\n    This method performs final book-keeping, closes the output streams\n    and writes the final log messages indicating script completion.\n    \"\"\"\n\n    args = get_args()\n\n    if args.loglevel &gt;= 1 and global_benchmark:\n        t = time.time() - global_starting_time\n        args.stdlog.write(\n            \"######### Time spent in benchmarked functions #########\\n\")\n        args.stdlog.write(\"# function\\tseconds\\tpercent\\n\")\n        for key, value in list(global_benchmark.items()):\n            args.stdlog.write(\n                \"# %s\\t%6i\\t%5.2f%%\\n\" % (key, value,\n                                          (100.0 * float(value) / t)))\n        args.stdlog.write(\n            \"#######################################################\\n\")\n\n    if logger is None:\n        logger = logging.getLogger(\"cgatcore\")\n    logger.info(get_footer())\n\n    # close files\n    if args.stdout != sys.stdout:\n        args.stdout.close()\n    # do not close log, otherwise error occurs in atext.py\n    # if global_options.stdlog != sys.stdout:\n    #   global_options.stdlog.close()\n\n    if args.stderr != sys.stderr:\n        args.stderr.close()\n\n    if args.timeit_file:\n\n        outfile = open(args.timeit_file, \"a\")\n\n        if args.timeit_header:\n            outfile.write(\"\\t\".join(\n                (\"name\", \"wall\", \"user\", \"sys\", \"cuser\", \"csys\",\n                 \"host\", \"system\", \"release\", \"machine\",\n                 \"start\", \"end\", \"path\", \"cmd\")) + \"\\n\")\n\n        csystem, host, release, version, machine = list(map(str, os.uname()))\n        uusr, usys, c_usr, c_sys = [\"%5.2f\" % x for x in os.times()[:4]]\n        t_end = time.time()\n        c_wall = \"%5.2f\" % (t_end - global_starting_time)\n\n        if sys.argv[0] == \"run.py\":\n            cmd = args[0]\n            if len(args) &gt; 1:\n                cmd += \" '\" + \"' '\".join(args[1:]) + \"'\"\n        else:\n            cmd = sys.argv[0]\n\n        result = \"\\t\".join((global_options.timeit_name,\n                            c_wall, uusr, usys, c_usr, c_sys,\n                            host, csystem, release, machine,\n                            time.asctime(time.localtime(global_starting_time)),\n                            time.asctime(time.localtime(t_end)),\n                            os.path.abspath(os.getcwd()),\n                            cmd)) + \"\\n\"\n\n        outfile.write(result)\n        outfile.close()\n</code></pre>"},{"location":"function_doc/experiment/#cgatcore.experiment.trace_calls","title":"<code>trace_calls(frame, event, arg)</code>","text":"<p>trace functions calls for debugging purposes.</p> <p>See https://pymotw.com/2/sys/tracing.html</p> Source code in <code>cgatcore/experiment.py</code> <pre><code>def trace_calls(frame, event, arg):\n    \"\"\"trace functions calls for debugging purposes.\n\n    See https://pymotw.com/2/sys/tracing.html\n    \"\"\"\n    if event != 'call':\n        return\n    co = frame.f_code\n    func_name = co.co_name\n    if func_name == 'write':\n        # Ignore write() calls from print statements\n        return\n    func_line_no = frame.f_lineno\n    func_filename = co.co_filename\n    caller = frame.f_back\n    if caller is None:\n        print('# called: %s:%s (%s) (from None' %\n              (func_name, func_line_no, func_filename))\n    else:\n        caller_line_no = caller.f_lineno\n        caller_filename = caller.f_code.co_filename\n        print('# called: %s:%s (%s) (from %s:%s)' %\n              (func_name, func_line_no, func_filename,\n               caller_filename, caller_line_no))\n    return\n</code></pre>"},{"location":"function_doc/iotools/","title":"CGATcore IOTools Module","text":""},{"location":"function_doc/iotools/#cgatcore.iotools--iotoolspy-tools-for-io-operations","title":"iotools.py - Tools for I/O operations","text":"<p>This module contains utility functions for reading/writing from files. These include methods for</p> <ul> <li> <p>inspecting files, such as :func:<code>get_first_line</code>, :func:<code>get_last_line</code>   and :func:<code>is_empty</code>,</p> </li> <li> <p>working with filenames, such as :func:<code>which</code> and :func:<code>snip</code>,   :func:<code>check_presence_of_files</code></p> </li> <li> <p>manipulating file, such as :func:<code>open_file</code>, :func:<code>zap_file</code>,   :func:<code>clone_file</code>, :func:<code>touch_file</code>.</p> </li> <li> <p>converting values for input/output, such as :func:<code>val2str</code>,   :func:<code>str2val</code>, :func:<code>pretty_percent</code>, :func:<code>human2bytes</code>,   :func:<code>convert_dictionary_values</code>.</p> </li> <li> <p>iterating over file contents, such as :func:<code>iterate</code>,   :func:<code>iterator_split</code>,</p> </li> <li> <p>creating lists/dictionaries from files, such as :func:<code>readMap</code> and   :func:<code>read_list</code>, and</p> </li> <li> <p>working with file collections (see :class:<code>FilePool</code>).</p> </li> </ul>"},{"location":"function_doc/iotools/#cgatcore.iotools--api","title":"API","text":""},{"location":"function_doc/iotools/#cgatcore.iotools.FilePool","title":"<code>FilePool</code>","text":"<p>manage a pool of output files.</p> <p>This class will keep a large number of files open. To see if you can handle this, check the limit within the shell::</p> <p>ulimit -n</p> <p>The number of currently open and maximum open files in the system:</p> <p>cat /proc/sys/fs/file-nr</p> <p>Changing these limits might not be easy without root privileges.</p> <p>The maximum number of files opened is given by :attr:<code>maxopen</code>. This class is inefficient if the number of files is larger than :attr:<code>maxopen</code> and calls to <code>write</code> do not group keys together.</p> <p>To use this class, create a FilePool and write to it as if it was a single file, specifying a section for each write::</p> <pre><code>pool = FilePool(\"%s.tsv\")\nfor value in range(100):\n    for section in (\"file1\", \"file2\", \"file3\"):\n         pool.write(section, str(value) + \",\")\n</code></pre> <p>This will create three files called <code>file1.tsv</code>, <code>file2.tsv</code>, <code>file3.tsv</code>, each containing the numbers from 0 to 99.</p> <p>The FilePool acts otherwise as a dictionary providing access to the number of times an item has been written to each file::</p> <pre><code>print pool[\"file1]\nprint pool.items()\n</code></pre>"},{"location":"function_doc/iotools/#cgatcore.iotools.FilePool--parameters","title":"Parameters","text":"string <p>output pattern to use. Should contain a \"%s\". If set to None, the pattern \"%s\" will be used.</p> <p>header : string    optional header to write when writing to a file the first time. force : bool    overwrite existing files. All files matching the pattern will be    deleted.</p> Source code in <code>cgatcore/iotools.py</code> <pre><code>class FilePool:\n    \"\"\"manage a pool of output files.\n\n    This class will keep a large number of files open. To\n    see if you can handle this, check the limit within the shell::\n\n       ulimit -n\n\n    The number of currently open and maximum open files in the system:\n\n      cat /proc/sys/fs/file-nr\n\n    Changing these limits might not be easy without root privileges.\n\n    The maximum number of files opened is given by :attr:`maxopen`.\n    This class is inefficient if the number of files is larger than\n    :attr:`maxopen` and calls to `write` do not group keys together.\n\n    To use this class, create a FilePool and write to it as if it was\n    a single file, specifying a section for each write::\n\n        pool = FilePool(\"%s.tsv\")\n        for value in range(100):\n            for section in (\"file1\", \"file2\", \"file3\"):\n                 pool.write(section, str(value) + \",\")\n\n    This will create three files called ``file1.tsv``, ``file2.tsv``,\n    ``file3.tsv``, each containing the numbers from 0 to 99.\n\n    The FilePool acts otherwise as a dictionary providing access to\n    the number of times an item has been written to each file::\n\n        print pool[\"file1]\n        print pool.items()\n\n    Parameters\n    ----------\n\n    output_pattern : string\n       output pattern to use. Should contain a \"%s\". If set to None, the\n       pattern \"%s\" will be used.\n    header : string\n       optional header to write when writing to a file the first time.\n    force : bool\n       overwrite existing files. All files matching the pattern will be\n       deleted.\n\n    \"\"\"\n\n    maxopen = 5000\n\n    def __init__(self,\n                 output_pattern=None,\n                 header=None,\n                 force=True):\n\n        self.mFiles = {}\n        self.mOutputPattern = output_pattern\n\n        self.open = open_file\n\n        self.mCounts = collections.defaultdict(int)\n        self.mHeader = header\n        if force and output_pattern:\n            for f in glob.glob(re.sub(\"%s\", \"*\", output_pattern)):\n                os.remove(f)\n\n    def __del__(self):\n        \"\"\"close all open files.\"\"\"\n        for file in list(self.mFiles.values()):\n            file.close()\n\n    def __len__(self):\n        return len(self.mCounts)\n\n    def close(self):\n        \"\"\"close all open files.\"\"\"\n        for file in list(self.mFiles.values()):\n            file.close()\n\n    def values(self):\n        return list(self.mCounts.values())\n\n    def keys(self):\n        return list(self.mCounts.keys())\n\n    def iteritems(self):\n        return iter(self.mCounts.items())\n\n    def items(self):\n        return list(self.mCounts.items())\n\n    def __iter__(self):\n        return self.mCounts.__iter__()\n\n    def getFile(self, identifier):\n        return identifier\n\n    def getFilename(self, identifier):\n        \"\"\"get filename for an identifier.\"\"\"\n\n        if self.mOutputPattern:\n            return re.sub(\"%s\", str(identifier), self.mOutputPattern)\n        else:\n            return identifier\n\n    def setHeader(self, header):\n        \"\"\"set the header to be written to each file when opening\n        for the first time.\"\"\"\n\n        self.mHeader = header\n\n    def open_file(self, filename, mode=\"w\"):\n        \"\"\"open file.\n\n        If file is in a new directory, create directories.\n        \"\"\"\n        if mode in (\"w\", \"a\"):\n            dirname = os.path.dirname(filename)\n            if dirname and not os.path.exists(dirname):\n                os.makedirs(dirname)\n\n        return self.open(filename, mode)\n\n    def write(self, identifier, line):\n        \"\"\"write `line` to file specified by `identifier`\"\"\"\n        filename = self.getFilename(identifier)\n\n        if filename not in self.mFiles:\n\n            if self.maxopen and len(self.mFiles) &gt; self.maxopen:\n                for f in list(self.mFiles.values()):\n                    f.close()\n                self.mFiles = {}\n\n            self.mFiles[filename] = self.open_file(filename, \"a\")\n            if self.mHeader:\n                self.mFiles[filename].write(self.mHeader)\n\n        try:\n            self.mFiles[filename].write(line)\n        except ValueError as msg:\n            raise ValueError(\n                \"error while writing to %s: msg=%s\" % (filename, msg))\n        self.mCounts[filename] += 1\n\n    def deleteFiles(self, min_size=0):\n        \"\"\"delete all files below a minimum size `min_size` bytes.\"\"\"\n\n        ndeleted = 0\n        for filename, counts in list(self.mCounts.items()):\n            if counts &lt; min_size:\n                os.remove(filename)\n                ndeleted += 1\n\n        return ndeleted\n</code></pre>"},{"location":"function_doc/iotools/#cgatcore.iotools.FilePool.__del__","title":"<code>__del__()</code>","text":"<p>close all open files.</p> Source code in <code>cgatcore/iotools.py</code> <pre><code>def __del__(self):\n    \"\"\"close all open files.\"\"\"\n    for file in list(self.mFiles.values()):\n        file.close()\n</code></pre>"},{"location":"function_doc/iotools/#cgatcore.iotools.FilePool.close","title":"<code>close()</code>","text":"<p>close all open files.</p> Source code in <code>cgatcore/iotools.py</code> <pre><code>def close(self):\n    \"\"\"close all open files.\"\"\"\n    for file in list(self.mFiles.values()):\n        file.close()\n</code></pre>"},{"location":"function_doc/iotools/#cgatcore.iotools.FilePool.deleteFiles","title":"<code>deleteFiles(min_size=0)</code>","text":"<p>delete all files below a minimum size <code>min_size</code> bytes.</p> Source code in <code>cgatcore/iotools.py</code> <pre><code>def deleteFiles(self, min_size=0):\n    \"\"\"delete all files below a minimum size `min_size` bytes.\"\"\"\n\n    ndeleted = 0\n    for filename, counts in list(self.mCounts.items()):\n        if counts &lt; min_size:\n            os.remove(filename)\n            ndeleted += 1\n\n    return ndeleted\n</code></pre>"},{"location":"function_doc/iotools/#cgatcore.iotools.FilePool.getFilename","title":"<code>getFilename(identifier)</code>","text":"<p>get filename for an identifier.</p> Source code in <code>cgatcore/iotools.py</code> <pre><code>def getFilename(self, identifier):\n    \"\"\"get filename for an identifier.\"\"\"\n\n    if self.mOutputPattern:\n        return re.sub(\"%s\", str(identifier), self.mOutputPattern)\n    else:\n        return identifier\n</code></pre>"},{"location":"function_doc/iotools/#cgatcore.iotools.FilePool.open_file","title":"<code>open_file(filename, mode='w')</code>","text":"<p>open file.</p> <p>If file is in a new directory, create directories.</p> Source code in <code>cgatcore/iotools.py</code> <pre><code>def open_file(self, filename, mode=\"w\"):\n    \"\"\"open file.\n\n    If file is in a new directory, create directories.\n    \"\"\"\n    if mode in (\"w\", \"a\"):\n        dirname = os.path.dirname(filename)\n        if dirname and not os.path.exists(dirname):\n            os.makedirs(dirname)\n\n    return self.open(filename, mode)\n</code></pre>"},{"location":"function_doc/iotools/#cgatcore.iotools.FilePool.setHeader","title":"<code>setHeader(header)</code>","text":"<p>set the header to be written to each file when opening for the first time.</p> Source code in <code>cgatcore/iotools.py</code> <pre><code>def setHeader(self, header):\n    \"\"\"set the header to be written to each file when opening\n    for the first time.\"\"\"\n\n    self.mHeader = header\n</code></pre>"},{"location":"function_doc/iotools/#cgatcore.iotools.FilePool.write","title":"<code>write(identifier, line)</code>","text":"<p>write <code>line</code> to file specified by <code>identifier</code></p> Source code in <code>cgatcore/iotools.py</code> <pre><code>def write(self, identifier, line):\n    \"\"\"write `line` to file specified by `identifier`\"\"\"\n    filename = self.getFilename(identifier)\n\n    if filename not in self.mFiles:\n\n        if self.maxopen and len(self.mFiles) &gt; self.maxopen:\n            for f in list(self.mFiles.values()):\n                f.close()\n            self.mFiles = {}\n\n        self.mFiles[filename] = self.open_file(filename, \"a\")\n        if self.mHeader:\n            self.mFiles[filename].write(self.mHeader)\n\n    try:\n        self.mFiles[filename].write(line)\n    except ValueError as msg:\n        raise ValueError(\n            \"error while writing to %s: msg=%s\" % (filename, msg))\n    self.mCounts[filename] += 1\n</code></pre>"},{"location":"function_doc/iotools/#cgatcore.iotools.FilePoolMemory","title":"<code>FilePoolMemory</code>","text":"<p>               Bases: <code>FilePool</code></p> <p>manage a pool of output files in memory.</p> <p>The usage is the same as :class:<code>FilePool</code> but the data is cached in memory before writing to disk.</p> Source code in <code>cgatcore/iotools.py</code> <pre><code>class FilePoolMemory(FilePool):\n    \"\"\"manage a pool of output files in memory.\n\n    The usage is the same as :class:`FilePool` but the data is cached\n    in memory before writing to disk.\n\n    \"\"\"\n\n    maxopen = 5000\n\n    def __init__(self, *args, **kwargs):\n        FilePool.__init__(self, *args, **kwargs)\n\n        self.data = collections.defaultdict(list)\n        self.isClosed = False\n\n    def __del__(self):\n        \"\"\"close all open files.\n        \"\"\"\n        if not self.isClosed:\n            self.close()\n\n    def close(self):\n        \"\"\"close all open files.\n        writes the data to disk.\n        \"\"\"\n        if self.isClosed:\n            raise IOError(\"write on closed FilePool in close()\")\n\n        for filename, data in self.data.items():\n            f = self.open_file(filename, \"a\")\n            if self.mHeader:\n                f.write(self.mHeader)\n            f.write(\"\".join(data))\n            f.close()\n\n        self.isClosed = True\n\n    def write(self, identifier, line):\n\n        filename = self.getFilename(identifier)\n        self.data[filename].append(line)\n        self.mCounts[filename] += 1\n</code></pre>"},{"location":"function_doc/iotools/#cgatcore.iotools.FilePoolMemory.__del__","title":"<code>__del__()</code>","text":"<p>close all open files.</p> Source code in <code>cgatcore/iotools.py</code> <pre><code>def __del__(self):\n    \"\"\"close all open files.\n    \"\"\"\n    if not self.isClosed:\n        self.close()\n</code></pre>"},{"location":"function_doc/iotools/#cgatcore.iotools.FilePoolMemory.close","title":"<code>close()</code>","text":"<p>close all open files. writes the data to disk.</p> Source code in <code>cgatcore/iotools.py</code> <pre><code>def close(self):\n    \"\"\"close all open files.\n    writes the data to disk.\n    \"\"\"\n    if self.isClosed:\n        raise IOError(\"write on closed FilePool in close()\")\n\n    for filename, data in self.data.items():\n        f = self.open_file(filename, \"a\")\n        if self.mHeader:\n            f.write(self.mHeader)\n        f.write(\"\".join(data))\n        f.close()\n\n    self.isClosed = True\n</code></pre>"},{"location":"function_doc/iotools/#cgatcore.iotools.nested_dict","title":"<code>nested_dict</code>","text":"<p>               Bases: <code>defaultdict</code></p> <p>Auto-vivifying nested dictionaries.</p> <p>For example::</p> <p>nd= nested_dict()   nd\"mouse\"[\"+\"] = 311</p> Source code in <code>cgatcore/iotools.py</code> <pre><code>class nested_dict(collections.defaultdict):\n    \"\"\"Auto-vivifying nested dictionaries.\n\n    For example::\n\n      nd= nested_dict()\n      nd[\"mouse\"][\"chr1\"][\"+\"] = 311\n\n   \"\"\"\n\n    def __init__(self):\n        collections.defaultdict.__init__(self, nested_dict)\n\n    def iterflattened(self):\n        \"\"\"\n        iterate through values with nested keys flattened into a tuple\n        \"\"\"\n\n        for key, value in self.items():\n            if isinstance(value, nested_dict):\n                for keykey, value in value.iterflattened():\n                    yield (key,) + keykey, value\n            else:\n                yield (key,), value\n</code></pre>"},{"location":"function_doc/iotools/#cgatcore.iotools.nested_dict.iterflattened","title":"<code>iterflattened()</code>","text":"<p>iterate through values with nested keys flattened into a tuple</p> Source code in <code>cgatcore/iotools.py</code> <pre><code>def iterflattened(self):\n    \"\"\"\n    iterate through values with nested keys flattened into a tuple\n    \"\"\"\n\n    for key, value in self.items():\n        if isinstance(value, nested_dict):\n            for keykey, value in value.iterflattened():\n                yield (key,) + keykey, value\n        else:\n            yield (key,), value\n</code></pre>"},{"location":"function_doc/iotools/#cgatcore.iotools.bytes2human","title":"<code>bytes2human(n, format='%(value).1f%(symbol)s', symbols='customary')</code>","text":"<p>Convert n bytes into a human readable string based on format. symbols can be either \"customary\", \"customary_ext\", \"iec\" or \"iec_ext\", see: http://goo.gl/kTQMs</p> <p>bytes2human(0)   '0.0B' bytes2human(0.9)   '0.0B' bytes2human(1)   '1.0B' bytes2human(1.9)   '1.0B' bytes2human(1024)   '1.0K' bytes2human(1048576)   '1.0M' bytes2human(1099511627776127398123789121)   '909.5Y'</p> <p>bytes2human(9856, symbols=\"customary\")   '9.6K' bytes2human(9856, symbols=\"customary_ext\")   '9.6kilo' bytes2human(9856, symbols=\"iec\")   '9.6Ki' bytes2human(9856, symbols=\"iec_ext\")   '9.6kibi'</p> <p>bytes2human(10000, \"%(value).1f %(symbol)s/sec\")   '9.8 K/sec'</p> <p>Author: Giampaolo Rodola'  License: MIT https://gist.github.com/leepro/9694638 Source code in <code>cgatcore/iotools.py</code> <pre><code>def bytes2human(n, format='%(value).1f%(symbol)s', symbols='customary'):\n    \"\"\"\n    Convert n bytes into a human readable string based on format.\n    symbols can be either \"customary\", \"customary_ext\", \"iec\" or \"iec_ext\",\n    see: http://goo.gl/kTQMs\n\n      &gt;&gt;&gt; bytes2human(0)\n      '0.0B'\n      &gt;&gt;&gt; bytes2human(0.9)\n      '0.0B'\n      &gt;&gt;&gt; bytes2human(1)\n      '1.0B'\n      &gt;&gt;&gt; bytes2human(1.9)\n      '1.0B'\n      &gt;&gt;&gt; bytes2human(1024)\n      '1.0K'\n      &gt;&gt;&gt; bytes2human(1048576)\n      '1.0M'\n      &gt;&gt;&gt; bytes2human(1099511627776127398123789121)\n      '909.5Y'\n\n      &gt;&gt;&gt; bytes2human(9856, symbols=\"customary\")\n      '9.6K'\n      &gt;&gt;&gt; bytes2human(9856, symbols=\"customary_ext\")\n      '9.6kilo'\n      &gt;&gt;&gt; bytes2human(9856, symbols=\"iec\")\n      '9.6Ki'\n      &gt;&gt;&gt; bytes2human(9856, symbols=\"iec_ext\")\n      '9.6kibi'\n\n      &gt;&gt;&gt; bytes2human(10000, \"%(value).1f %(symbol)s/sec\")\n      '9.8 K/sec'\n\n      &gt;&gt;&gt; # precision can be adjusted by playing with %f operator\n      &gt;&gt;&gt; bytes2human(10000, format=\"%(value).5f %(symbol)s\")\n      '9.76562 K'\n\n    Author: Giampaolo Rodola' &lt;g.rodola [AT] gmail [DOT] com&gt;\n    License: MIT\n    https://gist.github.com/leepro/9694638\n    \"\"\"\n    n = int(n)\n    if n &lt; 0:\n        raise ValueError(\"n &lt; 0\")\n    symbols = SYMBOLS[symbols]\n    prefix = {}\n    for i, s in enumerate(symbols[1:]):\n        prefix[s] = 1 &lt;&lt; (i + 1) * 10\n    for symbol in reversed(symbols[1:]):\n        if n &gt;= prefix[symbol]:\n            value = float(n) / prefix[symbol]\n            return format % locals()\n    return format % dict(symbol=symbols[0], value=n)\n</code></pre>"},{"location":"function_doc/iotools/#cgatcore.iotools.bytes2human--precision-can-be-adjusted-by-playing-with-f-operator","title":"precision can be adjusted by playing with %f operator","text":"<p>bytes2human(10000, format=\"%(value).5f %(symbol)s\")   '9.76562 K'</p>"},{"location":"function_doc/iotools/#cgatcore.iotools.check_presence_of_files","title":"<code>check_presence_of_files(filenames)</code>","text":"<p>check for the presence/absence of files</p>"},{"location":"function_doc/iotools/#cgatcore.iotools.check_presence_of_files--parameters","title":"Parameters","text":"<p>filenames : list     Filenames to check for presence.</p>"},{"location":"function_doc/iotools/#cgatcore.iotools.check_presence_of_files--returns","title":"Returns","text":"<p>missing : list     List of missing filenames</p> Source code in <code>cgatcore/iotools.py</code> <pre><code>def check_presence_of_files(filenames):\n    \"\"\"check for the presence/absence of files\n\n    Parameters\n    ----------\n    filenames : list\n        Filenames to check for presence.\n\n    Returns\n    -------\n    missing : list\n        List of missing filenames\n    \"\"\"\n\n    missing = []\n    for filename in filenames:\n        if not os.path.exists(filename):\n            missing.append(filename)\n    return missing\n</code></pre>"},{"location":"function_doc/iotools/#cgatcore.iotools.clone_file","title":"<code>clone_file(infile, outfile)</code>","text":"<p>create a clone of <code>infile</code> named <code>outfile</code> by creating a soft-link.</p> Source code in <code>cgatcore/iotools.py</code> <pre><code>def clone_file(infile, outfile):\n    '''create a clone of ``infile`` named ``outfile``\n    by creating a soft-link.\n    '''\n    # link via relative paths, otherwise it\n    # fails if infile and outfile are in different\n    # directories or in a subdirectory\n    if os.path.dirname(infile) != os.path.dirname(outfile):\n        relpath = os.path.relpath(\n            os.path.dirname(infile), os.path.dirname(outfile))\n    else:\n        relpath = \".\"\n    target = os.path.join(relpath, os.path.basename(infile))\n\n    try:\n        os.symlink(target, outfile)\n    except OSError:\n        pass\n</code></pre>"},{"location":"function_doc/iotools/#cgatcore.iotools.convert_dictionary_values","title":"<code>convert_dictionary_values(d, map={})</code>","text":"<p>convert string values in a dictionary to numeric types.</p> <p>Arguments d : dict    The dictionary to convert map : dict    If map contains 'default', a default conversion is enforced.    For example, to force int for every column but column <code>id</code>,    supply map = {'default' : \"int\", \"id\" : \"str\" }</p> Source code in <code>cgatcore/iotools.py</code> <pre><code>def convert_dictionary_values(d, map={}):\n    \"\"\"convert string values in a dictionary to numeric types.\n\n    Arguments\n    d : dict\n       The dictionary to convert\n    map : dict\n       If map contains 'default', a default conversion is enforced.\n       For example, to force int for every column but column ``id``,\n       supply map = {'default' : \"int\", \"id\" : \"str\" }\n    \"\"\"\n\n    rx_int = re.compile(r\"^\\s*[+-]*[0-9]+\\s*$\")\n    rx_float = re.compile(r\"^[-+]?[0-9]*\\.?[0-9]+([eE][-+]?[0-9]+)?$\")\n\n    # pre-process with 'default'\n    if \"default\" in map:\n        k = \"default\"\n        if map[k] == \"int\":\n            default = int\n        elif map[k] == \"float\":\n            default = float\n        elif map[k] == \"string\":\n            default = str\n    else:\n        default = False\n\n    for k, vv in list(d.items()):\n\n        if vv is None:\n            continue\n        v = vv.strip()\n        try:\n            if k in map:\n                if map[k] == \"int\":\n                    d[k] = int(v)\n                elif map[k] == \"float\":\n                    d[k] = float(v)\n                elif map[k] == \"string\":\n                    pass\n                continue\n            elif default:\n                if v != \"\":\n                    d[k] = default(v)\n                else:\n                    d[k] = v\n                continue\n        except TypeError as msg:\n            raise TypeError(\"conversion in field: %s, %s\" % (k, msg))\n\n        try:\n            if rx_int.match(v):\n                d[k] = int(v)\n            elif rx_float.match(v):\n                d[k] = float(v)\n        except TypeError as msg:\n            raise TypeError(\n                \"expected string or buffer: offending value = '%s' \" % str(v))\n        except ValueError as msg:\n            raise ValueError(\"conversion error: %s, %s\" % (msg, str(d)))\n    return d\n</code></pre>"},{"location":"function_doc/iotools/#cgatcore.iotools.flatten","title":"<code>flatten(nested_list, ltypes=(list, tuple))</code>","text":"<p>flatten a nested list.</p> <p>This method works with any list-like container such as tuples.</p>"},{"location":"function_doc/iotools/#cgatcore.iotools.flatten--arguments","title":"Arguments","text":"<p>nested_list : list     A nested list. ltypes : list     A list of valid container types.</p>"},{"location":"function_doc/iotools/#cgatcore.iotools.flatten--returns","title":"Returns","text":"<p>list : list     A flattened list.</p> Source code in <code>cgatcore/iotools.py</code> <pre><code>def flatten(nested_list, ltypes=(list, tuple)):\n    '''flatten a nested list.\n\n    This method works with any list-like container\n    such as tuples.\n\n    Arguments\n    ---------\n    nested_list : list\n        A nested list.\n    ltypes : list\n        A list of valid container types.\n\n    Returns\n    -------\n    list : list\n        A flattened list.\n    '''\n    ltype = type(nested_list)\n    nested_list = list(nested_list)\n    i = 0\n    while i &lt; len(nested_list):\n        while isinstance(nested_list[i], ltypes):\n            if not nested_list[i]:\n                nested_list.pop(i)\n                i -= 1\n                break\n            else:\n                nested_list[i:i + 1] = nested_list[i]\n        i += 1\n    return ltype(nested_list)\n</code></pre>"},{"location":"function_doc/iotools/#cgatcore.iotools.force_str","title":"<code>force_str(iterator, encoding='ascii')</code>","text":"<p>iterate over lines in iterator and force to string</p> Source code in <code>cgatcore/iotools.py</code> <pre><code>def force_str(iterator, encoding=\"ascii\"):\n    \"\"\"iterate over lines in iterator and force to string\"\"\"\n    for line in iterator:\n        yield line.decode(encoding)\n</code></pre>"},{"location":"function_doc/iotools/#cgatcore.iotools.get_first_line","title":"<code>get_first_line(filename, nlines=1)</code>","text":"<p>return the first line of a file.</p>"},{"location":"function_doc/iotools/#cgatcore.iotools.get_first_line--arguments","title":"Arguments","text":"<p>filename : string    The name of the file to be opened. nlines : int    Number of lines to return.</p>"},{"location":"function_doc/iotools/#cgatcore.iotools.get_first_line--returns","title":"Returns","text":"<p>string    The first line(s) of the file.</p> Source code in <code>cgatcore/iotools.py</code> <pre><code>def get_first_line(filename, nlines=1):\n    \"\"\"return the first line of a file.\n\n    Arguments\n    ---------\n    filename : string\n       The name of the file to be opened.\n    nlines : int\n       Number of lines to return.\n\n    Returns\n    -------\n    string\n       The first line(s) of the file.\n\n    \"\"\"\n    # U is to open it with Universal newline support\n    with open(filename, 'rU') as f:\n        line = \"\".join([f.readline() for x in range(nlines)])\n    return line\n</code></pre>"},{"location":"function_doc/iotools/#cgatcore.iotools.get_last_line","title":"<code>get_last_line(filename, nlines=1, read_size=1024, encoding='utf-8')</code>","text":"<p>return the last line of a file.</p> <p>This method works by working back in blocks of <code>read_size</code> until the beginning of the last line is reached.</p>"},{"location":"function_doc/iotools/#cgatcore.iotools.get_last_line--arguments","title":"Arguments","text":"<p>filename : string    Name of the file to be opened. nlines : int    Number of lines to return. read_size : int    Number of bytes to read.</p>"},{"location":"function_doc/iotools/#cgatcore.iotools.get_last_line--returns","title":"Returns","text":"<p>string    The last line(s) of the file.</p> Source code in <code>cgatcore/iotools.py</code> <pre><code>def get_last_line(filename, nlines=1, read_size=1024, encoding=\"utf-8\"):\n    \"\"\"return the last line of a file.\n\n    This method works by working back in blocks of `read_size` until\n    the beginning of the last line is reached.\n\n    Arguments\n    ---------\n    filename : string\n       Name of the file to be opened.\n    nlines : int\n       Number of lines to return.\n    read_size : int\n       Number of bytes to read.\n\n    Returns\n    -------\n    string\n       The last line(s) of the file.\n\n    \"\"\"\n\n    # py3 requires binary mode for negative seeks\n    f = open(filename, 'rb')\n    offset = read_size\n    f.seek(0, 2)\n    file_size = f.tell()\n    if file_size == 0:\n        return \"\"\n    while 1:\n        if file_size &lt; offset:\n            offset = file_size\n        f.seek(-1 * offset, 2)\n        read_str = f.read(offset)\n        read_str = read_str.decode(encoding)\n        lines = read_str.strip().splitlines()\n        if len(lines) &gt;= nlines + 1:\n            return \"\\n\".join(lines[-nlines:])\n        if offset == file_size:   # reached the beginning\n            return read_str\n        offset += read_size\n    f.close()\n</code></pre>"},{"location":"function_doc/iotools/#cgatcore.iotools.get_num_lines","title":"<code>get_num_lines(filename, ignore_comments=True)</code>","text":"<p>count number of lines in filename.</p>"},{"location":"function_doc/iotools/#cgatcore.iotools.get_num_lines--arguments","title":"Arguments","text":"<p>filename : string    Name of the file to be opened. ignore_comments : bool    If true, ignore lines starting with <code>#</code>.</p>"},{"location":"function_doc/iotools/#cgatcore.iotools.get_num_lines--returns","title":"Returns","text":"<p>int    The number of line(s) in the file.</p> Source code in <code>cgatcore/iotools.py</code> <pre><code>def get_num_lines(filename, ignore_comments=True):\n    \"\"\"count number of lines in filename.\n\n    Arguments\n    ---------\n    filename : string\n       Name of the file to be opened.\n    ignore_comments : bool\n       If true, ignore lines starting with ``#``.\n\n    Returns\n    -------\n    int\n       The number of line(s) in the file.\n\n    \"\"\"\n\n    if ignore_comments:\n        filter_cmd = '| grep -v \"#\" '\n    else:\n        filter_cmd = \"\"\n\n    # the implementation below seems to fastest\n    # see https://gist.github.com/0ac760859e614cd03652\n    # and\n    # http://stackoverflow.com/questions/845058/how-to-get-line-count-cheaply-in-python\n    if filename.endswith(\".gz\"):\n        cmd = \"zcat %(filename)s %(filter_cmd)s | wc -l\" % locals()\n    else:\n        cmd = \"cat %(filename)s %(filter_cmd)s | wc -l\" % locals()\n\n    out = subprocess.Popen(cmd,\n                           shell=True,\n                           stdout=subprocess.PIPE,\n                           stderr=subprocess.STDOUT\n                           ).communicate()[0]\n    return int(out.partition(b' ')[0])\n</code></pre>"},{"location":"function_doc/iotools/#cgatcore.iotools.human2bytes","title":"<code>human2bytes(s)</code>","text":"<p>Attempts to guess the string format based on default symbols set and return the corresponding bytes as an integer. When unable to recognize the format ValueError is raised.</p> <p>human2bytes('0 B')   0 human2bytes('1 K')   1024 human2bytes('1 M')   1048576 human2bytes('1 Gi')   1073741824 human2bytes('1 tera')   1099511627776</p> <p>human2bytes('0.5kilo')   512 human2bytes('0.1  byte')   0 human2bytes('1 k')  # k is an alias for K   1024 human2bytes('12 foo')   Traceback (most recent call last):       ...   ValueError: can't interpret '12 foo'</p> <p>Author: Giampaolo Rodola'  License: MIT https://gist.github.com/leepro/9694638 Source code in <code>cgatcore/iotools.py</code> <pre><code>def human2bytes(s):\n    \"\"\"\n    Attempts to guess the string format based on default symbols\n    set and return the corresponding bytes as an integer.\n    When unable to recognize the format ValueError is raised.\n\n      &gt;&gt;&gt; human2bytes('0 B')\n      0\n      &gt;&gt;&gt; human2bytes('1 K')\n      1024\n      &gt;&gt;&gt; human2bytes('1 M')\n      1048576\n      &gt;&gt;&gt; human2bytes('1 Gi')\n      1073741824\n      &gt;&gt;&gt; human2bytes('1 tera')\n      1099511627776\n\n      &gt;&gt;&gt; human2bytes('0.5kilo')\n      512\n      &gt;&gt;&gt; human2bytes('0.1  byte')\n      0\n      &gt;&gt;&gt; human2bytes('1 k')  # k is an alias for K\n      1024\n      &gt;&gt;&gt; human2bytes('12 foo')\n      Traceback (most recent call last):\n          ...\n      ValueError: can't interpret '12 foo'\n\n    Author: Giampaolo Rodola' &lt;g.rodola [AT] gmail [DOT] com&gt;\n    License: MIT\n    https://gist.github.com/leepro/9694638\n    \"\"\"\n    init = s\n    num = \"\"\n    while s and s[0:1].isdigit() or s[0:1] == '.':\n        num += s[0]\n        s = s[1:]\n    num = float(num)\n    letter = s.strip()\n    for name, sset in list(SYMBOLS.items()):\n        if letter in sset:\n            break\n    else:\n        if letter == 'k':\n            # treat 'k' as an alias for 'K' as per: http://goo.gl/kTQMs\n            sset = SYMBOLS['customary']\n            letter = letter.upper()\n        else:\n            raise ValueError(\"can't interpret %r\" % init)\n    prefix = {sset[0]: 1}\n    for i, s in enumerate(sset[1:]):\n        prefix[s] = 1 &lt;&lt; (i + 1) * 10\n\n    return int(num * prefix[letter])\n</code></pre>"},{"location":"function_doc/iotools/#cgatcore.iotools.invert_dictionary","title":"<code>invert_dictionary(dict, make_unique=False)</code>","text":"<p>returns an inverted dictionary with keys and values swapped.</p> Source code in <code>cgatcore/iotools.py</code> <pre><code>def invert_dictionary(dict, make_unique=False):\n    \"\"\"returns an inverted dictionary with keys and values swapped.\n    \"\"\"\n    inv = {}\n    if make_unique:\n        for k, v in dict.items():\n            inv[v] = k\n    else:\n        for k, v in dict.items():\n            inv.setdefault(v, []).append(k)\n    return inv\n</code></pre>"},{"location":"function_doc/iotools/#cgatcore.iotools.is_complete","title":"<code>is_complete(filename)</code>","text":"<p>return True if file exists and is complete.</p> <p>A file is complete if its last line contains <code>job finished</code>.</p> Source code in <code>cgatcore/iotools.py</code> <pre><code>def is_complete(filename):\n    '''return True if file exists and is complete.\n\n    A file is complete if its last line contains\n    ``job finished``.\n    '''\n    if filename.endswith(\".gz\"):\n        raise NotImplementedError(\n            'is_complete not implemented for compressed files')\n    if is_empty(filename):\n        return False\n    lastline = get_last_line(filename)\n    return \"job finished\" in lastline\n</code></pre>"},{"location":"function_doc/iotools/#cgatcore.iotools.is_empty","title":"<code>is_empty(filename)</code>","text":"<p>return True if file exists and is empty.</p>"},{"location":"function_doc/iotools/#cgatcore.iotools.is_empty--raises","title":"Raises","text":"<p>OSError    If file does not exist</p> Source code in <code>cgatcore/iotools.py</code> <pre><code>def is_empty(filename):\n    \"\"\"return True if file exists and is empty.\n\n    Raises\n    ------\n    OSError\n       If file does not exist\n    \"\"\"\n    # don't now about stdin\n    if filename == \"-\":\n        return False\n    return os.stat(filename)[stat.ST_SIZE] == 0\n</code></pre>"},{"location":"function_doc/iotools/#cgatcore.iotools.is_nested","title":"<code>is_nested(container)</code>","text":"<p>return true if container is a nested data structure.</p> <p>A nested data structure is a dict of dicts or a list of list, but not a dict of list or a list of dicts.</p> Source code in <code>cgatcore/iotools.py</code> <pre><code>def is_nested(container):\n    \"\"\"return true if container is a nested data structure.\n\n    A nested data structure is a dict of dicts or a list of list,\n    but not a dict of list or a list of dicts.\n    \"\"\"\n    for t in [collections.abc.Mapping, list, tuple]:\n        if isinstance(container, t):\n            return any([isinstance(v, t) for v in container.values()])\n    return False\n</code></pre>"},{"location":"function_doc/iotools/#cgatcore.iotools.iterate","title":"<code>iterate(infile)</code>","text":"<p>iterate over infile and return a :py:class:<code>collections.namedtuple</code> according to a header in the first row.</p> <p>Lines starting with <code>#</code> are skipped.</p> Source code in <code>cgatcore/iotools.py</code> <pre><code>def iterate(infile):\n    '''iterate over infile and return a :py:class:`collections.namedtuple`\n    according to a header in the first row.\n\n    Lines starting with ``#`` are skipped.\n\n    '''\n\n    n = 0\n    for line in infile:\n        if line.startswith(\"#\"):\n            continue\n        n += 1\n        if n == 1:\n            # replace non-alphanumeric characters with _\n            header = re.sub(r\"[^a-zA-Z0-9_\\s]\", \"_\", line[:-1]).split()\n            DATA = collections.namedtuple(\"DATA\", header)\n            continue\n\n        result = DATA(*line[:-1].split())\n\n        yield result\n</code></pre>"},{"location":"function_doc/iotools/#cgatcore.iotools.iterate_tabular","title":"<code>iterate_tabular(infile, sep='\\t')</code>","text":"<p>iterate over file <code>infile</code> skipping lines starting with <code>#</code>.</p> <p>Within a line, records are separated by <code>sep</code>.</p>"},{"location":"function_doc/iotools/#cgatcore.iotools.iterate_tabular--yields","title":"Yields","text":"<p>tuple     Records within a line</p> Source code in <code>cgatcore/iotools.py</code> <pre><code>def iterate_tabular(infile, sep=\"\\t\"):\n    '''iterate over file `infile` skipping lines starting with\n    ``#``.\n\n    Within a line, records are separated by `sep`.\n\n    Yields\n    ------\n    tuple\n        Records within a line\n\n    '''\n    for line in infile:\n        if line.startswith(\"#\"):\n            continue\n        yield line[:-1].split(sep)\n</code></pre>"},{"location":"function_doc/iotools/#cgatcore.iotools.iterator_split","title":"<code>iterator_split(infile, regex)</code>","text":"<p>Return an iterator of file chunks based on a known logical start point <code>regex</code> that splits the file into intuitive chunks.  This assumes the file is structured in some fashion.  For arbitrary number of bytes use file.read(<code>bytes</code>).  If a header is present it is returned as the first file chunk.</p> <p>infile must be either an open file handle or an iterable.</p> Source code in <code>cgatcore/iotools.py</code> <pre><code>def iterator_split(infile, regex):\n    '''Return an iterator of file chunks based on a known logical start\n    point `regex` that splits the file into intuitive chunks.  This\n    assumes the file is structured in some fashion.  For arbitrary\n    number of bytes use file.read(`bytes`).  If a header is present it\n    is returned as the first file chunk.\n\n    infile must be either an open file handle or an iterable.\n\n    '''\n    chunk_list = []\n\n    regex = re.compile(regex)\n\n    for x in infile:\n        if regex.search(x):\n            if len(chunk_list):\n                # return the current chunk and start a new one from this point\n                yield chunk_list\n            chunk_list = []\n            chunk_list.append(x)\n        else:\n            chunk_list.append(x)\n    yield chunk_list\n</code></pre>"},{"location":"function_doc/iotools/#cgatcore.iotools.nested_iter","title":"<code>nested_iter(nested)</code>","text":"<p>iterate over the contents of a nested data structure.</p> <p>The nesting can be done both as lists or as dictionaries.</p>"},{"location":"function_doc/iotools/#cgatcore.iotools.nested_iter--arguments","title":"Arguments","text":"<p>nested : dict     A nested dictionary</p>"},{"location":"function_doc/iotools/#cgatcore.iotools.nested_iter--yields","title":"Yields","text":"<p>pair: tuple     A container/key/value triple</p> Source code in <code>cgatcore/iotools.py</code> <pre><code>def nested_iter(nested):\n    \"\"\"iterate over the contents of a nested data structure.\n\n    The nesting can be done both as lists or as dictionaries.\n\n    Arguments\n    ---------\n    nested : dict\n        A nested dictionary\n\n    Yields\n    ------\n    pair: tuple\n        A container/key/value triple\n    \"\"\"\n\n    if isinstance(nested, collections.abc.Mapping):\n        for key, value in nested.items():\n            if not isinstance(value, collections.abc.Mapping) and \\\n               not isinstance(value, list):\n                yield nested, key, value\n            else:\n                for x in nested_iter(value):\n                    yield x\n    elif isinstance(nested, list):\n        for key, value in enumerate(nested):\n            if not isinstance(value, collections.abc.Mapping) and \\\n               not isinstance(value, list):\n                yield nested, key, value\n            else:\n                for x in nested_iter(value):\n                    yield x\n</code></pre>"},{"location":"function_doc/iotools/#cgatcore.iotools.open_file","title":"<code>open_file(filename, mode='r', create_dir=False, encoding='utf-8')</code>","text":"<p>open file called filename with mode mode.</p> <p>gzip - compressed files are recognized by the suffix <code>.gz</code> and opened transparently.</p> <p>Note that there are differences in the file like objects returned, for example in the ability to seek.</p>"},{"location":"function_doc/iotools/#cgatcore.iotools.open_file--arguments","title":"Arguments","text":"<p>filename : string mode : string    File opening mode create_dir : bool    If True, the directory containing filename    will be created if it does not exist.</p>"},{"location":"function_doc/iotools/#cgatcore.iotools.open_file--returns","title":"Returns","text":"<p>File or file-like object in case of gzip compressed files.</p> Source code in <code>cgatcore/iotools.py</code> <pre><code>def open_file(filename, mode=\"r\", create_dir=False, encoding=\"utf-8\"):\n    '''open file called *filename* with mode *mode*.\n\n    gzip - compressed files are recognized by the\n    suffix ``.gz`` and opened transparently.\n\n    Note that there are differences in the file\n    like objects returned, for example in the\n    ability to seek.\n\n    Arguments\n    ---------\n    filename : string\n    mode : string\n       File opening mode\n    create_dir : bool\n       If True, the directory containing filename\n       will be created if it does not exist.\n\n    Returns\n    -------\n    File or file-like object in case of gzip compressed files.\n    '''\n\n    _, ext = os.path.splitext(filename)\n\n    if create_dir:\n        dirname = os.path.dirname(filename)\n        if dirname and not os.path.exists(dirname):\n            os.makedirs(dirname)\n\n    if ext.lower() in (\".gz\", \".z\"):\n        if mode == \"r\":\n            return gzip.open(filename, 'rt', encoding=encoding)\n        elif mode == \"w\":\n            return gzip.open(filename, 'wt', encoding=encoding)\n        elif mode == \"a\":\n            return gzip.open(filename, 'wt', encoding=encoding)\n    else:\n        return open(filename, mode, encoding=encoding)\n</code></pre>"},{"location":"function_doc/iotools/#cgatcore.iotools.pickle","title":"<code>pickle(file_name, obj)</code>","text":"<p>dump a python object to a file using pickle</p> Source code in <code>cgatcore/iotools.py</code> <pre><code>def pickle(file_name, obj):\n    '''dump a python object to a file using pickle'''\n    with open(file_name, \"wb\") as pkl_file:\n        pickle.dump(obj, pkl_file)\n    return\n</code></pre>"},{"location":"function_doc/iotools/#cgatcore.iotools.pretty_percent","title":"<code>pretty_percent(numerator, denominator, format='%5.2f', na='na')</code>","text":"<p>output a percent value or \"na\" if not defined</p> Source code in <code>cgatcore/iotools.py</code> <pre><code>def pretty_percent(numerator, denominator, format=\"%5.2f\", na=\"na\"):\n    \"\"\"output a percent value or \"na\" if not defined\"\"\"\n    try:\n        x = format % (100.0 * numerator / denominator)\n    except (ValueError, ZeroDivisionError, TypeError):\n        x = na\n    return x\n</code></pre>"},{"location":"function_doc/iotools/#cgatcore.iotools.pretty_string","title":"<code>pretty_string(val)</code>","text":"<p>output val or na if val is None</p> Source code in <code>cgatcore/iotools.py</code> <pre><code>def pretty_string(val):\n    '''output val or na if val is None'''\n    if val is not None:\n        return val\n    else:\n        return \"na\"\n</code></pre>"},{"location":"function_doc/iotools/#cgatcore.iotools.readMultiMap","title":"<code>readMultiMap(infile, columns=(0, 1), map_functions=(str, str), both_directions=False, has_header=False, dtype=dict)</code>","text":"<p>read a map (pairs of values) from infile.</p> <p>In contrast to :func:<code>readMap</code>, this method permits multiple entries for the same key.</p>"},{"location":"function_doc/iotools/#cgatcore.iotools.readMultiMap--arguments","title":"Arguments","text":"<p>infile : File    File object to read from columns : tuple    Columns (A, B) to take from the file to create the mapping from    A to B. map_functions : tuple    Functions to convert the values in the rows to the desired    object types such as int or float. both_directions : bool    If true, both mapping directions are returned in a tuple, i.e.,    A-&gt;B and B-&gt;A. has_header : bool    If true, ignore first line with header. dtype : function    datatype to use for the dictionaries.</p>"},{"location":"function_doc/iotools/#cgatcore.iotools.readMultiMap--returns","title":"Returns","text":"<p>map : dict    A dictionary containing the mapping. If <code>both_directions</code> is true,    two dictionaries will be returned.</p> Source code in <code>cgatcore/iotools.py</code> <pre><code>def readMultiMap(infile,\n                 columns=(0, 1),\n                 map_functions=(str, str),\n                 both_directions=False,\n                 has_header=False,\n                 dtype=dict):\n    \"\"\"read a map (pairs of values) from infile.\n\n    In contrast to :func:`readMap`, this method permits multiple\n    entries for the same key.\n\n    Arguments\n    ---------\n    infile : File\n       File object to read from\n    columns : tuple\n       Columns (A, B) to take from the file to create the mapping from\n       A to B.\n    map_functions : tuple\n       Functions to convert the values in the rows to the desired\n       object types such as int or float.\n    both_directions : bool\n       If true, both mapping directions are returned in a tuple, i.e.,\n       A-&gt;B and B-&gt;A.\n    has_header : bool\n       If true, ignore first line with header.\n    dtype : function\n       datatype to use for the dictionaries.\n\n    Returns\n    -------\n    map : dict\n       A dictionary containing the mapping. If `both_directions` is true,\n       two dictionaries will be returned.\n\n    \"\"\"\n    m = dtype()\n    r = dtype()\n    n = 0\n    for line in infile:\n        if line[0] == \"#\":\n            continue\n        n += 1\n\n        if has_header and n == 1:\n            continue\n\n        d = line[:-1].split(\"\\t\")\n        try:\n            key = map_functions[0](d[columns[0]])\n            val = map_functions[1](d[columns[1]])\n        except (ValueError, IndexError) as msg:\n            raise ValueError(\"parsing error in line %s: %s\" % (line[:-1], msg))\n\n        if key not in m:\n            m[key] = []\n        m[key].append(val)\n        if val not in r:\n            r[val] = []\n        r[val].append(key)\n\n    if both_directions:\n        return m, r\n    else:\n        return m\n</code></pre>"},{"location":"function_doc/iotools/#cgatcore.iotools.read_list","title":"<code>read_list(infile, column=0, map_function=str, map_category={}, with_title=False)</code>","text":"<p>read a list of values from infile.</p>"},{"location":"function_doc/iotools/#cgatcore.iotools.read_list--arguments","title":"Arguments","text":"<p>infile : File    File object to read from columns : int    Column to take from the file. map_function : function    Function to convert the values in the rows to the desired    object types such as int or float. map_category : dict    When given, automatically transform/map the values given    this dictionary. with_title : bool    If true, first line of file is title and will be ignored.</p>"},{"location":"function_doc/iotools/#cgatcore.iotools.read_list--returns","title":"Returns","text":"<p>list : list    A list with the values.</p> Source code in <code>cgatcore/iotools.py</code> <pre><code>def read_list(infile,\n              column=0,\n              map_function=str,\n              map_category={},\n              with_title=False):\n    \"\"\"read a list of values from infile.\n\n    Arguments\n    ---------\n    infile : File\n       File object to read from\n    columns : int\n       Column to take from the file.\n    map_function : function\n       Function to convert the values in the rows to the desired\n       object types such as int or float.\n    map_category : dict\n       When given, automatically transform/map the values given\n       this dictionary.\n    with_title : bool\n       If true, first line of file is title and will be ignored.\n\n    Returns\n    -------\n    list : list\n       A list with the values.\n    \"\"\"\n\n    m = []\n    title = None\n    for line in infile:\n        if line[0] == \"#\":\n            continue\n        if with_title and not title:\n            title = line[:-1].split(\"\\t\")[column]\n            continue\n\n        try:\n            d = map_function(line[:-1].split(\"\\t\")[column])\n        except ValueError:\n            continue\n\n        if map_category:\n            d = map_category[d]\n        m.append(d)\n\n    return m\n</code></pre>"},{"location":"function_doc/iotools/#cgatcore.iotools.read_map","title":"<code>read_map(infile, columns=(0, 1), map_functions=(str, str), both_directions=False, has_header=True, dtype=dict)</code>","text":"<p>read a map (key, value pairs) from infile.</p> <p>If there are multiple entries for the same key, only the last entry will be recorded.</p>"},{"location":"function_doc/iotools/#cgatcore.iotools.read_map--arguments","title":"Arguments","text":"<p>infile : File    File object to read from columns : tuple    Columns (A, B) to take from the file to create the mapping from    A to B. map_functions : tuple    Functions to convert the values in the rows to the desired    object types such as int or float. both_directions : bool    If true, both mapping directions are returned. has_header : bool    If true, ignore first line with header. dtype : function    datatype to use for the dictionaries.</p>"},{"location":"function_doc/iotools/#cgatcore.iotools.read_map--returns","title":"Returns","text":"<p>map : dict    A dictionary containing the mapping. If <code>both_directions</code> is true,    two dictionaries will be returned.</p> Source code in <code>cgatcore/iotools.py</code> <pre><code>def read_map(infile,\n             columns=(0, 1),\n             map_functions=(str, str),\n             both_directions=False,\n             has_header=True,\n             dtype=dict):\n    \"\"\"read a map (key, value pairs) from infile.\n\n    If there are multiple entries for the same key, only the\n    last entry will be recorded.\n\n    Arguments\n    ---------\n    infile : File\n       File object to read from\n    columns : tuple\n       Columns (A, B) to take from the file to create the mapping from\n       A to B.\n    map_functions : tuple\n       Functions to convert the values in the rows to the desired\n       object types such as int or float.\n    both_directions : bool\n       If true, both mapping directions are returned.\n    has_header : bool\n       If true, ignore first line with header.\n    dtype : function\n       datatype to use for the dictionaries.\n\n    Returns\n    -------\n    map : dict\n       A dictionary containing the mapping. If `both_directions` is true,\n       two dictionaries will be returned.\n\n    \"\"\"\n    m = dtype()\n    r = dtype()\n    n = 0\n\n    if columns == \"all\":\n        key_column = 0\n        value_column = None\n    else:\n        key_column, value_column = columns\n\n    key_function, value_function = map_functions\n    # default is to return a tuple for multiple values\n    datatype = None\n\n    for line in infile:\n        if line[0] == \"#\":\n            continue\n        n += 1\n\n        if has_header and n == 1:\n            if columns == \"all\":\n                header = line[:-1].split(\"\\t\")\n                # remove the first column\n                datatype = collections.namedtuple(\"DATA\", header[1:])\n            continue\n\n        d = line[:-1].split(\"\\t\")\n        if len(d) &lt; 2:\n            continue\n        key = key_function(d[key_column])\n        if value_column:\n            val = value_function(d[value_column])\n        elif datatype:\n            val = datatype._make([d[x] for x in range(1, len(d))])\n        else:\n            val = tuple(map(value_function, [d[x] for x in range(1, len(d))]))\n\n        m[key] = val\n        if val not in r:\n            r[val] = []\n        r[val].append(key)\n\n    if both_directions:\n        return m, r\n    else:\n        return m\n</code></pre>"},{"location":"function_doc/iotools/#cgatcore.iotools.snip","title":"<code>snip(filename, extension=None, alt_extension=None, strip_path=False)</code>","text":"<p>return prefix of <code>filename</code>, that is the part without the extension.</p> <p>If <code>extension</code> is given, make sure that filename has the extension (or <code>alt_extension</code>). Both extension or alt_extension can be list of extensions.</p> <p>If <code>strip_path</code> is set to true, the path is stripped from the file name.</p> Source code in <code>cgatcore/iotools.py</code> <pre><code>def snip(filename, extension=None, alt_extension=None,\n         strip_path=False):\n    '''return prefix of `filename`, that is the part without the\n    extension.\n\n    If `extension` is given, make sure that filename has the\n    extension (or `alt_extension`). Both extension or alt_extension\n    can be list of extensions.\n\n    If `strip_path` is set to true, the path is stripped from the file\n    name.\n\n    '''\n    if extension is None:\n        extension = []\n    elif isinstance(extension, str):\n        extension = [extension]\n\n    if alt_extension is None:\n        alt_extension = []\n    elif isinstance(alt_extension, str):\n        alt_extension = [alt_extension]\n\n    if extension:\n        for ext in extension + alt_extension:\n            if filename.endswith(ext):\n                root = filename[:-len(ext)]\n                break\n        else:\n            raise ValueError(\"'%s' expected to end in '%s'\" %\n                             (filename, \",\".join(\n                                 extension + alt_extension)))\n    else:\n        root, ext = os.path.splitext(filename)\n\n    if strip_path:\n        snipped = os.path.basename(root)\n    else:\n        snipped = root\n\n    return snipped\n</code></pre>"},{"location":"function_doc/iotools/#cgatcore.iotools.str2val","title":"<code>str2val(val, na='na', list_detection=False)</code>","text":"<p>guess type (int, float) of value.</p> <p>If <code>val</code> is neither int nor float, the value itself is returned.</p> Source code in <code>cgatcore/iotools.py</code> <pre><code>def str2val(val, na=\"na\", list_detection=False):\n    \"\"\"guess type (int, float) of value.\n\n    If `val` is neither int nor float, the value\n    itself is returned.\n    \"\"\"\n\n    if val is None:\n        return val\n\n    def _convert(v):\n        try:\n            x = int(v)\n        except ValueError:\n            try:\n                x = float(v)\n            except ValueError:\n                if v.lower() == \"true\":\n                    return True\n                elif v.lower() == \"false\":\n                    return False\n                else:\n                    return v\n        return x\n\n    if list_detection and \",\" in val:\n        return [_convert(v) for v in val.split(\",\")]\n    else:\n        return _convert(val)\n</code></pre>"},{"location":"function_doc/iotools/#cgatcore.iotools.text_to_dict","title":"<code>text_to_dict(filename, key=None, sep='\\t')</code>","text":"<p>make a dictionary from a text file keyed on the specified column.</p> Source code in <code>cgatcore/iotools.py</code> <pre><code>def text_to_dict(filename, key=None, sep=\"\\t\"):\n    '''make a dictionary from a text file keyed\n    on the specified column.'''\n\n    # Please see function in readDict()\n    count = 0\n    result = {}\n    valueidx, keyidx = False, False\n    field_names = []\n\n    with open(filename, \"r\") as fh:\n        for line in fh:\n            if line.startswith(\"#\"):\n                continue\n            if count == 0:\n                fieldn = 0\n                for rawfield in line.split(sep):\n                    field = rawfield.strip()\n                    if field == key:\n                        keyidx = fieldn\n                    field_names.append(field)\n                    fieldn += 1\n\n                if not keyidx:\n                    raise ValueError(\"key name not found in header\")\n                # if not valueidx:\n                #   raise ValueError(\n                #     \"value name not found in header\")\n            else:\n                fields = [x.strip() for x in line.split(sep)]\n                fieldn = 0\n                thiskey = fields[keyidx]\n                result[thiskey] = {}\n                for field in fields:\n                    if fieldn == keyidx:\n                        pass\n                    else:\n                        colkey = field_names[fieldn]\n                        result[thiskey][colkey] = field\n                    fieldn += 1\n            count += 1\n\n    return result\n</code></pre>"},{"location":"function_doc/iotools/#cgatcore.iotools.touch_file","title":"<code>touch_file(filename, mode=438, times=None, dir_fd=None, ref=None, **kwargs)</code>","text":"<p>update/create a sentinel file.</p> <p>modified from: https://stackoverflow.com/questions/1158076/implement-touch-using-python</p> <p>Compressed files (ending in .gz) are created as empty 'gzip' files, i.e., with a header.</p> Source code in <code>cgatcore/iotools.py</code> <pre><code>def touch_file(filename, mode=0o666, times=None, dir_fd=None, ref=None, **kwargs):\n    '''update/create a sentinel file.\n\n    modified from: https://stackoverflow.com/questions/1158076/implement-touch-using-python\n\n    Compressed files (ending in .gz) are created as empty 'gzip'\n    files, i.e., with a header.\n\n    '''\n    flags = os.O_CREAT | os.O_APPEND\n    existed = os.path.exists(filename)\n\n    if filename.endswith(\".gz\") and not existed:\n        # this will automatically add a gzip header\n        with gzip.GzipFile(filename, \"w\") as fhandle:\n            pass\n\n    if ref:\n        stattime = os.stat(ref)\n        times = (stattime.st_atime, stattime.st_mtime)\n\n    with os.fdopen(os.open(\n            filename, flags=flags, mode=mode, dir_fd=dir_fd)) as fhandle:\n        os.utime(\n            fhandle.fileno() if os.utime in os.supports_fd else filename,\n            dir_fd=None if os.supports_fd else dir_fd,\n            **kwargs)\n</code></pre>"},{"location":"function_doc/iotools/#cgatcore.iotools.unpickle","title":"<code>unpickle(file_name)</code>","text":"<p>retrieve a pickled python object from a file</p> Source code in <code>cgatcore/iotools.py</code> <pre><code>def unpickle(file_name):\n    '''retrieve a pickled python object from a file'''\n    with open(file_name, \"r\") as pkl_file:\n        data = pickle.load(pkl_file)\n    return data\n</code></pre>"},{"location":"function_doc/iotools/#cgatcore.iotools.val2list","title":"<code>val2list(val)</code>","text":"<p>ensure that val is a list.</p> Source code in <code>cgatcore/iotools.py</code> <pre><code>def val2list(val):\n    '''ensure that val is a list.'''\n\n    if not isinstance(val, list):\n        return [val]\n    else:\n        return val\n</code></pre>"},{"location":"function_doc/iotools/#cgatcore.iotools.val2str","title":"<code>val2str(val, format='%5.2f', na='na')</code>","text":"<p>return a formatted value.</p> <p>If value does not fit format string, return \"na\"</p> Source code in <code>cgatcore/iotools.py</code> <pre><code>def val2str(val, format=\"%5.2f\", na=\"na\"):\n    '''return a formatted value.\n\n    If value does not fit format string, return \"na\"\n    '''\n    if isinstance(val, int):\n        return format % val\n    elif isinstance(val, float):\n        return format % val\n\n    try:\n        x = format % val\n    except (ValueError, TypeError):\n        x = na\n    return x\n</code></pre>"},{"location":"function_doc/iotools/#cgatcore.iotools.which","title":"<code>which(program)</code>","text":"<p>check if <code>program</code> is in PATH and is executable.</p>"},{"location":"function_doc/iotools/#cgatcore.iotools.which--returns","title":"Returns","text":"<p>string    The full path to the program. Returns None if not found.</p> Source code in <code>cgatcore/iotools.py</code> <pre><code>def which(program):\n    \"\"\"check if `program` is in PATH and is executable.\n\n    Returns\n    -------\n    string\n       The full path to the program. Returns None if not found.\n\n    \"\"\"\n    # see http://stackoverflow.com/questions/377017/\n    #            test-if-executable-exists-in-python\n\n    def is_exe(fpath):\n        return os.path.exists(fpath) and os.access(fpath, os.X_OK)\n\n    fpath, fname = os.path.split(program)\n    if fpath:\n        if is_exe(program):\n            return program\n    else:\n        for path in os.environ[\"PATH\"].split(os.pathsep):\n            exe_file = os.path.join(path, program)\n            if is_exe(exe_file):\n                return exe_file\n\n    return None\n</code></pre>"},{"location":"function_doc/iotools/#cgatcore.iotools.writeMatrix","title":"<code>writeMatrix(outfile, matrix, row_headers, col_headers, row_header='')</code>","text":"<p>write a numpy matrix to outfile.</p> <p>row_header gives the title of the rows</p> Source code in <code>cgatcore/iotools.py</code> <pre><code>def writeMatrix(outfile, matrix, row_headers, col_headers,\n                row_header=\"\"):\n    '''write a numpy matrix to outfile.\n\n    *row_header* gives the title of the rows\n    '''\n\n    outfile.write(\"%s\\t%s\\n\" % (row_header,\n                                \"\\t\".join(col_headers)))\n    for x, row in enumerate(matrix):\n        assert len(row) == len(col_headers)\n        outfile.write(\"%s\\t%s\\n\" %\n                      (row_headers[x], \"\\t\".join(map(str, row))))\n</code></pre>"},{"location":"function_doc/iotools/#cgatcore.iotools.write_lines","title":"<code>write_lines(outfile, lines, header=False)</code>","text":"<p>expects [[[line1-field1],[line1-field2 ] ],... ]</p> Source code in <code>cgatcore/iotools.py</code> <pre><code>def write_lines(outfile, lines, header=False):\n    ''' expects [[[line1-field1],[line1-field2 ] ],... ]'''\n    handle = open_file(outfile, \"w\")\n\n    if header:\n        handle.write(\"\\t\".join([str(title) for title in header]) + \"\\n\")\n\n    for line in lines:\n        handle.write(\"\\t\".join([str(field) for field in line]) + \"\\n\")\n\n    handle.close()\n</code></pre>"},{"location":"function_doc/iotools/#cgatcore.iotools.write_matrix","title":"<code>write_matrix(outfile, matrix, row_headers, col_headers, row_header='')</code>","text":"<p>write a numpy matrix to outfile. row_header gives the title of the rows</p> Source code in <code>cgatcore/iotools.py</code> <pre><code>def write_matrix(outfile, matrix, row_headers, col_headers,\n                 row_header=\"\"):\n    '''write a numpy matrix to outfile.\n    *row_header* gives the title of the rows\n    '''\n\n    outfile.write(\"%s\\t%s\\n\" % (row_header, \"\\t\".join(col_headers)))\n    for x, row in enumerate(matrix):\n        assert len(row) == len(col_headers)\n        outfile.write(\"%s\\t%s\\n\" % (row_headers[x], \"\\t\".join(map(str, row))))\n</code></pre>"},{"location":"function_doc/iotools/#cgatcore.iotools.write_table","title":"<code>write_table(outfile, table, columns=None, fillvalue='')</code>","text":"<p>write a table to outfile.</p> <p>If table is a dictionary, output columnwise. If columns is a list, only output columns in columns in the specified order.</p> <p>.. note:: Deprecated    use pandas dataframes instead</p> Source code in <code>cgatcore/iotools.py</code> <pre><code>def write_table(outfile, table, columns=None, fillvalue=\"\"):\n    '''write a table to outfile.\n\n    If table is a dictionary, output columnwise. If *columns* is a list,\n    only output columns in columns in the specified order.\n\n    .. note:: Deprecated\n       use pandas dataframes instead\n\n    '''\n\n    if isinstance(table, dict):\n        if columns is None:\n            columns = list(table.keys())\n        outfile.write(\"\\t\".join(columns) + \"\\n\")\n        # get data\n        data = [table[x] for x in columns]\n        # transpose\n        data = list(itertools.zip_longest(*data, fillvalue=fillvalue))\n\n        for d in data:\n            outfile.write(\"\\t\".join(map(str, d)) + \"\\n\")\n\n    else:\n        raise NotImplementedError\n</code></pre>"},{"location":"function_doc/iotools/#cgatcore.iotools.zap_file","title":"<code>zap_file(filename)</code>","text":"<p>replace filename with empty file.</p> <p>File attributes such as accession times are preserved.</p> <p>If the file is a link, the link will be broken and replaced with an empty file having the same attributes as the file linked to.</p>"},{"location":"function_doc/iotools/#cgatcore.iotools.zap_file--returns","title":"Returns","text":"<p>stat_object    A stat object of the file cleaned. link_destination : string    If the file was a link, the file being linked to.</p> Source code in <code>cgatcore/iotools.py</code> <pre><code>def zap_file(filename):\n    '''replace *filename* with empty file.\n\n    File attributes such as accession times are preserved.\n\n    If the file is a link, the link will be broken and replaced with\n    an empty file having the same attributes as the file linked to.\n\n    Returns\n    -------\n    stat_object\n       A stat object of the file cleaned.\n    link_destination : string\n       If the file was a link, the file being linked to.\n\n    '''\n    # stat follows times to links\n    original = os.stat(filename)\n\n    # return if file already has size 0\n    if original.st_size == 0:\n        return None, None\n\n    if os.path.islink(filename):\n        linkdest = os.readlink(filename)\n        os.unlink(filename)\n        f = open(filename, \"w\")\n        f.close()\n    else:\n        linkdest = None\n        f = open(filename, \"w\")\n        f.truncate()\n        f.close()\n\n    # Set original times\n    os.utime(filename, (original.st_atime, original.st_mtime))\n    os.chmod(filename, original.st_mode)\n\n    return original, linkdest\n</code></pre>"},{"location":"function_doc/kubernetes/","title":"CGATcore Kubernetes Module","text":""},{"location":"function_doc/kubernetes/#kubernetesexecutorrun","title":"KubernetesExecutor.run","text":"<p>Submits a job to the Kubernetes cluster to run the specified command.</p> <p>This method creates a Kubernetes Job object and submits it to the cluster. The job runs the specified command in a container, using the provided Conda environment.</p>"},{"location":"function_doc/kubernetes/#args","title":"Args:","text":"<ul> <li>statement (str): The command to execute in the job.</li> <li>job_path (str): The path to the job script.</li> <li>job_condaenv (str): The name of the Conda environment to use.</li> </ul>"},{"location":"function_doc/kubernetes/#example-usage","title":"Example Usage:","text":"<pre><code>executor = KubernetesExecutor(namespace='default')\nlogs = executor.run(statement='echo Hello World', job_path='path/to/job/script', job_condaenv='my_conda_env')\nprint(logs)\n</code></pre>"},{"location":"function_doc/kubernetes/#returns","title":"Returns:","text":"<ul> <li>Logs from the job execution.</li> </ul>"},{"location":"function_doc/logfile/","title":"CGATcore Logfile Module","text":""},{"location":"function_doc/logfile/#cgatcore.logfile--logfilepy-logfile-parsing","title":"Logfile.py - logfile parsing","text":"<p>:Tags: Python</p>"},{"location":"function_doc/logfile/#cgatcore.logfile--purpose","title":"Purpose","text":"<p>Parse logfiles</p>"},{"location":"function_doc/logfile/#cgatcore.logfile--usage","title":"Usage","text":"<p>Example::</p> <p>python cgat_script_template.py --help</p> <p>Type::</p> <p>python cgat_script_template.py --help</p> <p>for command line help.</p>"},{"location":"function_doc/logfile/#cgatcore.logfile--documentation","title":"Documentation","text":""},{"location":"function_doc/logfile/#cgatcore.logfile--code","title":"Code","text":""},{"location":"function_doc/logfile/#cgatcore.logfile.LogFileDataLines","title":"<code>LogFileDataLines</code>","text":"<p>               Bases: <code>LogFileData</code></p> <p>record lines.</p> Source code in <code>cgatcore/logfile.py</code> <pre><code>class LogFileDataLines(LogFileData):\n\n    \"\"\"record lines.\"\"\"\n\n    def __init__(self):\n        LogFileData.__init__(self)\n        self.mNLines = 0\n\n    def add(self, line):\n        if line[0] != \"#\":\n            self.mNLines += 1\n        else:\n            return LogFileData.add(self, line)\n\n    def __getitem__(self, key):\n        if key == \"lines\":\n            return self.mNLines\n        else:\n            return LogFileData.__getitem__(self, key)\n\n    def __add__(self, other):\n        self.mNLines += other.mNLines\n        return LogFileData.__add__(self, other)\n\n    def __str__(self):\n        return \"%s\\t%i\" % (LogFileData.__str__(self), self.mNLines)\n\n    def getHeader(self):\n        return \"%s\\t%s\" % (LogFileData.getHeader(self), \"lines\")\n</code></pre>"},{"location":"function_doc/pipeline/","title":"CGATcore Pipeline Module","text":"<p>The <code>pipeline</code> module is the core component of CGAT-core, providing essential functionality for building and executing computational pipelines.</p>"},{"location":"function_doc/pipeline/#core-functions","title":"Core Functions","text":""},{"location":"function_doc/pipeline/#pipeline-decorators","title":"Pipeline Decorators","text":"<pre><code>@transform(input_files, suffix(\".input\"), \".output\")\ndef task_function(infile, outfile):\n    \"\"\"Transform a single input file to an output file.\"\"\"\n    pass\n\n@merge(input_files, \"output.txt\")\ndef merge_task(infiles, outfile):\n    \"\"\"Merge multiple input files into a single output.\"\"\"\n    pass\n\n@split(input_file, \"*.split\")\ndef split_task(infile, outfiles):\n    \"\"\"Split a single input file into multiple outputs.\"\"\"\n    pass\n\n@follows(previous_task)\ndef dependent_task():\n    \"\"\"Execute after previous_task completes.\"\"\"\n    pass\n</code></pre>"},{"location":"function_doc/pipeline/#s3-aware-decorators","title":"S3-Aware Decorators","text":"<pre><code>@s3_transform(\"s3://bucket/input.txt\", suffix(\".txt\"), \".processed\")\ndef process_s3_file(infile, outfile):\n    \"\"\"Process files directly from S3.\"\"\"\n    pass\n\n@s3_merge([\"s3://bucket/*.txt\"], \"s3://bucket/merged.txt\")\ndef merge_s3_files(infiles, outfile):\n    \"\"\"Merge multiple S3 files.\"\"\"\n    pass\n</code></pre>"},{"location":"function_doc/pipeline/#configuration-functions","title":"Configuration Functions","text":""},{"location":"function_doc/pipeline/#pipeline-setup","title":"Pipeline Setup","text":"<pre><code># Initialize pipeline\npipeline.initialize(options)\n\n# Get pipeline parameters\nparams = pipeline.get_params()\n\n# Configure cluster execution\npipeline.setup_cluster()\n</code></pre>"},{"location":"function_doc/pipeline/#resource-management","title":"Resource Management","text":"<pre><code># Set memory requirements\npipeline.set_job_memory(\"4G\")\n\n# Set CPU requirements\npipeline.set_job_threads(4)\n\n# Configure temporary directory\npipeline.set_tmpdir(\"/path/to/tmp\")\n</code></pre>"},{"location":"function_doc/pipeline/#execution-functions","title":"Execution Functions","text":""},{"location":"function_doc/pipeline/#running-tasks","title":"Running Tasks","text":"<pre><code># Execute a command\npipeline.run(\"samtools sort input.bam\")\n\n# Submit a Python function\npipeline.submit(\n    module=\"my_module\",\n    function=\"process_data\",\n    infiles=\"input.txt\",\n    outfiles=\"output.txt\"\n)\n</code></pre>"},{"location":"function_doc/pipeline/#job-control","title":"Job Control","text":"<pre><code># Check job status\npipeline.is_running(job_id)\n\n# Wait for job completion\npipeline.wait_for_jobs()\n\n# Clean up temporary files\npipeline.cleanup()\n</code></pre>"},{"location":"function_doc/pipeline/#error-handling","title":"Error Handling","text":"<pre><code>try:\n    pipeline.run(\"risky_command\")\nexcept pipeline.PipelineError as e:\n    pipeline.handle_error(e)\n</code></pre>"},{"location":"function_doc/pipeline/#best-practices","title":"Best Practices","text":"<ol> <li>Resource Management</li> <li>Always specify memory and CPU requirements</li> <li>Use appropriate cluster queue settings</li> <li> <p>Clean up temporary files</p> </li> <li> <p>Error Handling</p> </li> <li>Implement proper error checking</li> <li>Use pipeline.log for logging</li> <li> <p>Handle temporary file cleanup</p> </li> <li> <p>Performance</p> </li> <li>Use appropriate chunk sizes for parallel processing</li> <li>Monitor resource usage</li> <li>Optimize cluster settings</li> </ol> <p>For more details, see the Pipeline Overview and Writing Workflows guides.</p>"},{"location":"function_doc/pipeline/#cgatcore.pipeline--pipelinepy-tools-for-cgat-ruffus-pipelines","title":"pipeline.py - Tools for CGAT Ruffus Pipelines","text":"<p>This module provides a comprehensive set of tools to facilitate the creation and management of data processing pipelines using CGAT Ruffus. It includes functionalities for:</p> <ol> <li>Pipeline Control</li> <li>Task execution and dependency management</li> <li>Command-line interface for pipeline operations</li> <li> <p>Logging and error handling</p> </li> <li> <p>Resource Management</p> </li> <li>Cluster job submission and monitoring</li> <li>Memory and CPU allocation</li> <li> <p>Temporary file handling</p> </li> <li> <p>Configuration</p> </li> <li>Parameter management via YAML configuration</li> <li>Cluster settings customization</li> <li> <p>Pipeline state persistence</p> </li> <li> <p>Cloud Integration</p> </li> <li>AWS S3 support for input/output files</li> <li>Cloud-aware pipeline decorators</li> <li>Remote file handling</li> </ol>"},{"location":"function_doc/pipeline/#cgatcore.pipeline--example-usage","title":"Example Usage","text":"<p>A basic pipeline using local files:</p> <p>.. code-block:: python</p> <pre><code>from cgatcore import pipeline as P\n\n# Standard pipeline task\n@P.transform(\"input.txt\", suffix(\".txt\"), \".processed\")\ndef process_local_file(infile, outfile):\n    # Processing logic here\n    pass\n</code></pre> <p>Using S3 integration:</p> <p>.. code-block:: python</p> <pre><code># S3-aware pipeline task\n@P.s3_transform(\"s3://bucket/input.txt\", suffix(\".txt\"), \".processed\")\ndef process_s3_file(infile, outfile):\n    # Processing logic here\n    pass\n</code></pre> <p>For detailed documentation, see: https://cgat-core.readthedocs.io/</p>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.ContainerConfig","title":"<code>ContainerConfig</code>","text":"<p>Container configuration for pipeline execution.</p> Source code in <code>cgatcore/pipeline/execution.py</code> <pre><code>class ContainerConfig:\n    \"\"\"Container configuration for pipeline execution.\"\"\"\n\n    def __init__(self, image=None, volumes=None, env_vars=None, runtime=\"docker\"):\n        \"\"\"\n        Args:\n            image (str): Container image (e.g., \"ubuntu:20.04\").\n            volumes (list): Volume mappings (e.g., ['/data:/data']).\n            env_vars (dict): Environment variables for the container.\n            runtime (str): Container runtime (\"docker\" or \"singularity\").\n        \"\"\"\n        self.image = image\n        self.volumes = volumes or []\n        self.env_vars = env_vars or {}\n        self.runtime = runtime.lower()  # Normalise to lowercase\n\n        if self.runtime not in [\"docker\", \"singularity\"]:\n            raise ValueError(\"Unsupported container runtime: {}\".format(self.runtime))\n\n    def get_container_command(self, statement):\n        \"\"\"Convert a statement to run inside a container.\"\"\"\n        if not self.image:\n            return statement\n\n        if self.runtime == \"docker\":\n            return self._get_docker_command(statement)\n        elif self.runtime == \"singularity\":\n            return self._get_singularity_command(statement)\n        else:\n            raise ValueError(\"Unsupported container runtime: {}\".format(self.runtime))\n\n    def _get_docker_command(self, statement):\n        \"\"\"Generate a Docker command.\"\"\"\n        volume_args = [f\"-v {volume}\" for volume in self.volumes]\n        env_args = [f\"-e {key}={value}\" for key, value in self.env_vars.items()]\n\n        return \" \".join([\n            \"docker\", \"run\", \"--rm\",\n            *volume_args, *env_args, self.image,\n            \"/bin/bash\", \"-c\", f\"'{statement}'\"\n        ])\n\n    def _get_singularity_command(self, statement):\n        \"\"\"Generate a Singularity command.\"\"\"\n        volume_args = [f\"--bind {volume}\" for volume in self.volumes]\n        env_args = [f\"--env {key}={value}\" for key, value in self.env_vars.items()]\n\n        return \" \".join([\n            \"singularity\", \"exec\",\n            *volume_args, *env_args, self.image,\n            \"bash\", \"-c\", f\"'{statement}'\"\n        ])\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.ContainerConfig.__init__","title":"<code>__init__(image=None, volumes=None, env_vars=None, runtime='docker')</code>","text":"<p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>Container image (e.g., \"ubuntu:20.04\").</p> <code>None</code> <code>volumes</code> <code>list</code> <p>Volume mappings (e.g., ['/data:/data']).</p> <code>None</code> <code>env_vars</code> <code>dict</code> <p>Environment variables for the container.</p> <code>None</code> <code>runtime</code> <code>str</code> <p>Container runtime (\"docker\" or \"singularity\").</p> <code>'docker'</code> Source code in <code>cgatcore/pipeline/execution.py</code> <pre><code>def __init__(self, image=None, volumes=None, env_vars=None, runtime=\"docker\"):\n    \"\"\"\n    Args:\n        image (str): Container image (e.g., \"ubuntu:20.04\").\n        volumes (list): Volume mappings (e.g., ['/data:/data']).\n        env_vars (dict): Environment variables for the container.\n        runtime (str): Container runtime (\"docker\" or \"singularity\").\n    \"\"\"\n    self.image = image\n    self.volumes = volumes or []\n    self.env_vars = env_vars or {}\n    self.runtime = runtime.lower()  # Normalise to lowercase\n\n    if self.runtime not in [\"docker\", \"singularity\"]:\n        raise ValueError(\"Unsupported container runtime: {}\".format(self.runtime))\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.ContainerConfig.get_container_command","title":"<code>get_container_command(statement)</code>","text":"<p>Convert a statement to run inside a container.</p> Source code in <code>cgatcore/pipeline/execution.py</code> <pre><code>def get_container_command(self, statement):\n    \"\"\"Convert a statement to run inside a container.\"\"\"\n    if not self.image:\n        return statement\n\n    if self.runtime == \"docker\":\n        return self._get_docker_command(statement)\n    elif self.runtime == \"singularity\":\n        return self._get_singularity_command(statement)\n    else:\n        raise ValueError(\"Unsupported container runtime: {}\".format(self.runtime))\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.DRMAACluster","title":"<code>DRMAACluster</code>","text":"<p>               Bases: <code>object</code></p> Source code in <code>cgatcore/pipeline/cluster.py</code> <pre><code>class DRMAACluster(object):\n\n    # dictionary mapping resource usage fields returned by DRMAA\n    # to a common set of names.\n    map_drmaa2benchmark_data = {}\n\n    def __init__(self, session, ignore_errors=False):\n        self.session = session\n        self.ignore_errors = ignore_errors\n\n    def get_resource_usage(self, job_id, retval, hostname):\n        retval.resourceUsage[\"hostname\"] = hostname\n        return [retval]\n\n    def setup_drmaa_job_template(self,\n                                 drmaa_session,\n                                 job_name,\n                                 job_memory,\n                                 job_threads,\n                                 working_directory,\n                                 **kwargs):\n        '''Sets up a Drmma job template. Currently SGE, SLURM, Torque and PBSPro are\n        supported'''\n        if not job_memory:\n            raise ValueError(\"Job memory must be specified when running\"\n                             \"DRMAA jobs\")\n\n        jt = drmaa_session.createJobTemplate()\n        jt.workingDirectory = working_directory\n        jt.jobEnvironment = {'BASH_ENV': '~/.bashrc'}\n        jt.args = []\n        if not re.match(\"[a-zA-Z]\", job_name[0]):\n            job_name = \"_\" + job_name\n\n        spec = self.get_native_specification(job_name,\n                                             job_memory,\n                                             job_threads,\n                                             **kwargs)\n\n        jt.nativeSpecification = \" \".join(spec)\n\n        # keep stdout and stderr separate\n        jt.joinFiles = False\n\n        self.update_template(jt)\n        return jt\n\n    def update_template(self, jt):\n        pass\n\n    def collect_single_job_from_cluster(self,\n                                        job_id,\n                                        statement,\n                                        stdout_path, stderr_path,\n                                        job_path):\n        '''collects a single job on the cluster.\n\n        This method waits until a job has completed and returns\n        stdout, stderr and resource usage.\n        '''\n        try:\n            retval = self.session.wait(\n                job_id, drmaa.Session.TIMEOUT_WAIT_FOREVER)\n        except Exception as msg:\n            # ignore message 24, indicates jobs that have been qdel'ed\n            if not str(msg).startswith(\"code 24\"):\n                raise\n            retval = None\n\n        stdout, stderr = self.get_drmaa_job_stdout_stderr(\n            stdout_path, stderr_path)\n\n        if retval is not None:\n            error_msg = None\n            if retval.exitStatus == 0:\n                if retval.wasAborted is True:\n                    error_msg = (\n                        \"Job {} has exit status 0, but marked as hasAborted=True, hasExited={} \"\n                        \"(Job may have been cancelled by the user or the scheduler due to memory constraints)\"\n                        \"The stderr was \\n{}\\nstatement = {}\".format(\n                            job_id, retval.hasExited, \"\".join(stderr), statement))\n                if retval.hasSignal is True:\n                    error_msg = (\"Job {} has zero exitStatus {} but received signal: hasExited={},  wasAborted={}\"\n                                 \"hasSignal={}, terminatedSignal='{}' \"\n                                 \"\\nstatement = {}\".format(\n                                     job_id, retval.exitStatus, retval.hasExited, retval.wasAborted,\n                                     retval.hasSignal, retval.terminatedSignal,\n                                     statement))\n            else:\n                error_msg = (\"Job {} has non-zero exitStatus {}: hasExited={},  wasAborted={}\"\n                             \"hasSignal={}, terminatedSignal='{}' \"\n                             \"\\nstatement = {}\".format(\n                                 job_id, retval.exitStatus, retval.hasExited, retval.wasAborted,\n                                 retval.hasSignal, retval.terminatedSignal,\n                                 statement))\n\n            if error_msg:\n                if stderr:\n                    error_msg += \"\\n stderr = {}\".format(\"\".join(stderr))\n                if self.ignore_errors:\n                    get_logger().warning(error_msg)\n                else:\n                    raise OSError(error_msg)\n        else:\n            retval = JobInfo(job_id, {})\n\n        # get hostname from job script\n        try:\n            hostname = stdout[-3][:-1]\n        except IndexError:\n            hostname = \"unknown\"\n\n        try:\n            resource_usage = self.get_resource_usage(job_id, retval, hostname)\n        except (ValueError, KeyError, TypeError, IndexError) as ex:\n            E.warn(\"could not collect resource usage for job {}: {}\".format(job_id, ex))\n            retval.resourceUsage[\"hostname\"] = hostname\n            resource_usage = [retval]\n\n        try:\n            os.unlink(job_path)\n        except OSError:\n            self.logger.warn(\n                (\"temporary job file %s not present for \"\n                 \"clean-up - ignored\") % job_path)\n\n        return stdout, stderr, resource_usage\n\n    def get_drmaa_job_stdout_stderr(self, stdout_path, stderr_path,\n                                    tries=5, encoding=\"utf-8\"):\n        '''get stdout/stderr allowing for some lag.\n\n        Try at most *tries* times. If unsuccessfull, throw OSError\n\n        Removes the files once they are read.\n\n        Returns tuple of stdout and stderr as unicode strings.\n        '''\n        x = tries\n        while x &gt;= 0:\n            if os.path.exists(stdout_path):\n                break\n            gevent.sleep(GEVENT_TIMEOUT_WAIT)\n            x -= 1\n\n        x = tries\n        while x &gt;= 0:\n            if os.path.exists(stderr_path):\n                break\n            gevent.sleep(GEVENT_TIMEOUT_WAIT)\n            x -= 1\n\n        try:\n            with open(stdout_path, \"r\", encoding=encoding) as inf:\n                stdout = inf.readlines()\n        except IOError as msg:\n            get_logger().warning(\"could not open stdout: %s\" % msg)\n            stdout = []\n\n        try:\n            with open(stderr_path, \"r\", encoding=encoding) as inf:\n                stderr = inf.readlines()\n        except IOError as msg:\n            get_logger().warning(\"could not open stdout: %s\" % msg)\n            stderr = []\n\n        try:\n            os.unlink(stdout_path)\n            os.unlink(stderr_path)\n        except OSError as msg:\n            pass\n\n        return stdout, stderr\n\n    def set_drmaa_job_paths(self, job_template, job_path):\n        '''Adds the job_path, stdout_path and stderr_paths\n           to the job_template.\n        '''\n        job_path = os.path.abspath(job_path)\n        os.chmod(job_path, stat.S_IRWXG | stat.S_IRWXU)\n\n        stdout_path = job_path + \".stdout\"\n        stderr_path = job_path + \".stderr\"\n\n        job_template.remoteCommand = job_path\n        job_template.outputPath = \":\" + stdout_path\n        job_template.errorPath = \":\" + stderr_path\n\n        return stdout_path, stderr_path\n\n    def map_resource_usage(self, resource_usage, data2type):\n        \"\"\"return job metrics mapped to common name and converted to right type.\"\"\"\n        def _convert(key, v, tpe):\n            if v is None:\n                return None\n            else:\n                try:\n                    return tpe(v)\n                except ValueError as ex:\n                    E.warning(\"could not convert {} with value '{}' to {}: {}\".format(\n                        key, v, tpe, ex))\n                    return v\n\n        return dict([(key,\n                      _convert(key, resource_usage.get(self.map_drmaa2benchmark_data.get(key, key), None), tpe))\n                     for key, tpe in data2type.items()])\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.DRMAACluster.collect_single_job_from_cluster","title":"<code>collect_single_job_from_cluster(job_id, statement, stdout_path, stderr_path, job_path)</code>","text":"<p>collects a single job on the cluster.</p> <p>This method waits until a job has completed and returns stdout, stderr and resource usage.</p> Source code in <code>cgatcore/pipeline/cluster.py</code> <pre><code>def collect_single_job_from_cluster(self,\n                                    job_id,\n                                    statement,\n                                    stdout_path, stderr_path,\n                                    job_path):\n    '''collects a single job on the cluster.\n\n    This method waits until a job has completed and returns\n    stdout, stderr and resource usage.\n    '''\n    try:\n        retval = self.session.wait(\n            job_id, drmaa.Session.TIMEOUT_WAIT_FOREVER)\n    except Exception as msg:\n        # ignore message 24, indicates jobs that have been qdel'ed\n        if not str(msg).startswith(\"code 24\"):\n            raise\n        retval = None\n\n    stdout, stderr = self.get_drmaa_job_stdout_stderr(\n        stdout_path, stderr_path)\n\n    if retval is not None:\n        error_msg = None\n        if retval.exitStatus == 0:\n            if retval.wasAborted is True:\n                error_msg = (\n                    \"Job {} has exit status 0, but marked as hasAborted=True, hasExited={} \"\n                    \"(Job may have been cancelled by the user or the scheduler due to memory constraints)\"\n                    \"The stderr was \\n{}\\nstatement = {}\".format(\n                        job_id, retval.hasExited, \"\".join(stderr), statement))\n            if retval.hasSignal is True:\n                error_msg = (\"Job {} has zero exitStatus {} but received signal: hasExited={},  wasAborted={}\"\n                             \"hasSignal={}, terminatedSignal='{}' \"\n                             \"\\nstatement = {}\".format(\n                                 job_id, retval.exitStatus, retval.hasExited, retval.wasAborted,\n                                 retval.hasSignal, retval.terminatedSignal,\n                                 statement))\n        else:\n            error_msg = (\"Job {} has non-zero exitStatus {}: hasExited={},  wasAborted={}\"\n                         \"hasSignal={}, terminatedSignal='{}' \"\n                         \"\\nstatement = {}\".format(\n                             job_id, retval.exitStatus, retval.hasExited, retval.wasAborted,\n                             retval.hasSignal, retval.terminatedSignal,\n                             statement))\n\n        if error_msg:\n            if stderr:\n                error_msg += \"\\n stderr = {}\".format(\"\".join(stderr))\n            if self.ignore_errors:\n                get_logger().warning(error_msg)\n            else:\n                raise OSError(error_msg)\n    else:\n        retval = JobInfo(job_id, {})\n\n    # get hostname from job script\n    try:\n        hostname = stdout[-3][:-1]\n    except IndexError:\n        hostname = \"unknown\"\n\n    try:\n        resource_usage = self.get_resource_usage(job_id, retval, hostname)\n    except (ValueError, KeyError, TypeError, IndexError) as ex:\n        E.warn(\"could not collect resource usage for job {}: {}\".format(job_id, ex))\n        retval.resourceUsage[\"hostname\"] = hostname\n        resource_usage = [retval]\n\n    try:\n        os.unlink(job_path)\n    except OSError:\n        self.logger.warn(\n            (\"temporary job file %s not present for \"\n             \"clean-up - ignored\") % job_path)\n\n    return stdout, stderr, resource_usage\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.DRMAACluster.get_drmaa_job_stdout_stderr","title":"<code>get_drmaa_job_stdout_stderr(stdout_path, stderr_path, tries=5, encoding='utf-8')</code>","text":"<p>get stdout/stderr allowing for some lag.</p> <p>Try at most tries times. If unsuccessfull, throw OSError</p> <p>Removes the files once they are read.</p> <p>Returns tuple of stdout and stderr as unicode strings.</p> Source code in <code>cgatcore/pipeline/cluster.py</code> <pre><code>def get_drmaa_job_stdout_stderr(self, stdout_path, stderr_path,\n                                tries=5, encoding=\"utf-8\"):\n    '''get stdout/stderr allowing for some lag.\n\n    Try at most *tries* times. If unsuccessfull, throw OSError\n\n    Removes the files once they are read.\n\n    Returns tuple of stdout and stderr as unicode strings.\n    '''\n    x = tries\n    while x &gt;= 0:\n        if os.path.exists(stdout_path):\n            break\n        gevent.sleep(GEVENT_TIMEOUT_WAIT)\n        x -= 1\n\n    x = tries\n    while x &gt;= 0:\n        if os.path.exists(stderr_path):\n            break\n        gevent.sleep(GEVENT_TIMEOUT_WAIT)\n        x -= 1\n\n    try:\n        with open(stdout_path, \"r\", encoding=encoding) as inf:\n            stdout = inf.readlines()\n    except IOError as msg:\n        get_logger().warning(\"could not open stdout: %s\" % msg)\n        stdout = []\n\n    try:\n        with open(stderr_path, \"r\", encoding=encoding) as inf:\n            stderr = inf.readlines()\n    except IOError as msg:\n        get_logger().warning(\"could not open stdout: %s\" % msg)\n        stderr = []\n\n    try:\n        os.unlink(stdout_path)\n        os.unlink(stderr_path)\n    except OSError as msg:\n        pass\n\n    return stdout, stderr\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.DRMAACluster.map_resource_usage","title":"<code>map_resource_usage(resource_usage, data2type)</code>","text":"<p>return job metrics mapped to common name and converted to right type.</p> Source code in <code>cgatcore/pipeline/cluster.py</code> <pre><code>def map_resource_usage(self, resource_usage, data2type):\n    \"\"\"return job metrics mapped to common name and converted to right type.\"\"\"\n    def _convert(key, v, tpe):\n        if v is None:\n            return None\n        else:\n            try:\n                return tpe(v)\n            except ValueError as ex:\n                E.warning(\"could not convert {} with value '{}' to {}: {}\".format(\n                    key, v, tpe, ex))\n                return v\n\n    return dict([(key,\n                  _convert(key, resource_usage.get(self.map_drmaa2benchmark_data.get(key, key), None), tpe))\n                 for key, tpe in data2type.items()])\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.DRMAACluster.set_drmaa_job_paths","title":"<code>set_drmaa_job_paths(job_template, job_path)</code>","text":"<p>Adds the job_path, stdout_path and stderr_paths to the job_template.</p> Source code in <code>cgatcore/pipeline/cluster.py</code> <pre><code>def set_drmaa_job_paths(self, job_template, job_path):\n    '''Adds the job_path, stdout_path and stderr_paths\n       to the job_template.\n    '''\n    job_path = os.path.abspath(job_path)\n    os.chmod(job_path, stat.S_IRWXG | stat.S_IRWXU)\n\n    stdout_path = job_path + \".stdout\"\n    stderr_path = job_path + \".stderr\"\n\n    job_template.remoteCommand = job_path\n    job_template.outputPath = \":\" + stdout_path\n    job_template.errorPath = \":\" + stderr_path\n\n    return stdout_path, stderr_path\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.DRMAACluster.setup_drmaa_job_template","title":"<code>setup_drmaa_job_template(drmaa_session, job_name, job_memory, job_threads, working_directory, **kwargs)</code>","text":"<p>Sets up a Drmma job template. Currently SGE, SLURM, Torque and PBSPro are supported</p> Source code in <code>cgatcore/pipeline/cluster.py</code> <pre><code>def setup_drmaa_job_template(self,\n                             drmaa_session,\n                             job_name,\n                             job_memory,\n                             job_threads,\n                             working_directory,\n                             **kwargs):\n    '''Sets up a Drmma job template. Currently SGE, SLURM, Torque and PBSPro are\n    supported'''\n    if not job_memory:\n        raise ValueError(\"Job memory must be specified when running\"\n                         \"DRMAA jobs\")\n\n    jt = drmaa_session.createJobTemplate()\n    jt.workingDirectory = working_directory\n    jt.jobEnvironment = {'BASH_ENV': '~/.bashrc'}\n    jt.args = []\n    if not re.match(\"[a-zA-Z]\", job_name[0]):\n        job_name = \"_\" + job_name\n\n    spec = self.get_native_specification(job_name,\n                                         job_memory,\n                                         job_threads,\n                                         **kwargs)\n\n    jt.nativeSpecification = \" \".join(spec)\n\n    # keep stdout and stderr separate\n    jt.joinFiles = False\n\n    self.update_template(jt)\n    return jt\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.EventPool","title":"<code>EventPool</code>","text":"<p>               Bases: <code>Pool</code></p> Source code in <code>cgatcore/pipeline/control.py</code> <pre><code>class EventPool(gevent.pool.Pool):\n\n    def __len__(self):\n        \"\"\"make sure that pool always evaluates to true.\"\"\"\n        line = gevent.pool.Pool.__len__(self)\n        if not line:\n            return 1\n        return line\n\n    def close(self):\n        pass\n\n    def terminate(self):\n        self.kill()\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.EventPool.__len__","title":"<code>__len__()</code>","text":"<p>make sure that pool always evaluates to true.</p> Source code in <code>cgatcore/pipeline/control.py</code> <pre><code>def __len__(self):\n    \"\"\"make sure that pool always evaluates to true.\"\"\"\n    line = gevent.pool.Pool.__len__(self)\n    if not line:\n        return 1\n    return line\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.Executor","title":"<code>Executor</code>","text":"<p>               Bases: <code>object</code></p> Source code in <code>cgatcore/pipeline/execution.py</code> <pre><code>class Executor(object):\n\n    def __init__(self, **kwargs):\n\n        self.logger = get_logger()\n        self.queue_manager = None\n        self.run_on_cluster = will_run_on_cluster(kwargs)\n        self.job_threads = kwargs.get(\"job_threads\", 1)\n        self.active_jobs = []  # List to track active jobs\n\n        if \"job_memory\" in kwargs and \"job_total_memory\" in kwargs:\n            raise ValueError(\n                \"both job_memory and job_total_memory have been given\")\n\n        self.job_total_memory = kwargs.get('job_total_memory', None)\n        self.job_memory = kwargs.get('job_memory', None)\n\n        if self.job_total_memory == \"unlimited\" or self.job_memory == \"unlimited\":\n            self.job_total_memory = self.job_memory = \"unlimited\"\n        else:\n            if self.job_total_memory:\n                self.job_memory = iotools.bytes2human(\n                    iotools.human2bytes(self.job_total_memory) / self.job_threads)\n            elif self.job_memory:\n                self.job_total_memory = self.job_memory * self.job_threads\n            else:\n                self.job_memory = get_params()[\"cluster\"].get(\n                    \"memory_default\", \"4G\")\n                if self.job_memory == \"unlimited\":\n                    self.job_total_memory = \"unlimited\"\n                else:\n                    self.job_total_memory = self.job_memory * self.job_threads\n\n        self.ignore_pipe_errors = kwargs.get('ignore_pipe_errors', False)\n        self.ignore_errors = kwargs.get('ignore_errors', False)\n\n        self.job_name = kwargs.get(\"job_name\", \"unknow_job_name\")\n        self.task_name = kwargs.get(\"task_name\", \"unknown_task_name\")\n\n        # deduce output directory/directories, requires somewhat\n        # consistent naming in the calling function.\n        outfiles = []\n        if \"outfile\" in kwargs:\n            outfiles.append(kwargs[\"outfile\"])\n        if \"outfiles\" in kwargs:\n            outfiles.extend(kwargs[\"outfiles\"])\n\n        self.output_directories = set(sorted(\n            [os.path.dirname(x) for x in outfiles]))\n\n        self.options = kwargs\n\n        self.work_dir = get_params()[\"work_dir\"]\n\n        self.shellfile = kwargs.get(\"shell_logfile\", None)\n        if self.shellfile:\n            if not self.shellfile.startswith(os.sep):\n                self.shellfile = os.path.join(\n                    self.work_dir, os.path.basename(self.shellfile))\n\n        self.monitor_interval_queued = kwargs.get('monitor_interval_queued', None)\n        if self.monitor_interval_queued is None:\n            self.monitor_interval_queued = get_params()[\"cluster\"].get(\n                'monitor_interval_queued_default', GEVENT_TIMEOUT_WAIT)\n        self.monitor_interval_running = kwargs.get('monitor_interval_running', None)\n        if self.monitor_interval_running is None:\n            self.monitor_interval_running = get_params()[\"cluster\"].get(\n                'monitor_interval_running_default', GEVENT_TIMEOUT_WAIT)\n        # Set up signal handlers for clean-up on interruption\n        self.setup_signal_handlers()\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        pass\n\n    def expand_statement(self, statement):\n        '''add generic commands before and after statement.\n\n        The method scans the statement for arvados mount points and\n        inserts appropriate prefixes to make sure that the mount point\n        exists.\n\n        Arguments\n        ---------\n        statement : string\n            Command line statement to expand\n\n        Returns\n        -------\n        statement : string\n            The expanded statement.\n\n        '''\n\n        setup_cmds = []\n        teardown_cmds = []\n        cleanup_funcs = []\n\n        setup_cmds.append(\"umask 002\")\n\n        for var in [\"MKL_NUM_THREADS\",\n                    \"OPENBLAS_NUM_THREADS\",\n                    \"OMP_NUM_THREADS\"]:\n            setup_cmds.append(\"export {}={}\".format(\n                var, self.options.get(\"job_threads\", 1)))\n\n        if \"arv=\" in statement:\n\n            # Todo: permit setting this in params\n            arvados_api_token = os.environ.get(\"ARVADOS_API_TOKEN\", None)\n            arvados_api_host = os.environ.get(\"ARVADOS_API_HOST\", None)\n            if not arvados_api_token:\n                raise ValueError(\n                    \"arvados mount encountered in statement {}, \"\n                    \"but ARVADOS_API_TOKEN not defined\".format(statement))\n\n            if not arvados_api_host:\n                raise ValueError(\n                    \"arvados mount encountered in statement {}, \"\n                    \"but ARVADOS_API_HOST not defined\".format(statement))\n\n            mountpoint = get_temp_filename(clear=True)\n\n            arvados_options = \"--disable-event-listening --read-only\"\n            setup_cmds.append(\"\\n\".join(\n                ('export ARVADOS_API_TOKEN=\"{arvados_api_token}\"',\n                 'export ARVADOS_API_HOST=\"{arvados_api_host}\"',\n                 'export ARVADOS_API_HOST_INSECURE=true',\n                 'export ARVADOS_MOUNT_POINT=\"{mountpoint}\"',\n                 'mkdir -p \"{mountpoint}\"',\n                 'arv-mount {arvados_options} \"{mountpoint}\" 2&gt;&gt; /dev/null')).format(**locals()))\n\n            statement = re.sub(\"arv=\", mountpoint + \"/\", statement)\n\n            # \"arv-mount --unmount {mountpoint}\" not available in newer\n            # arvados installs (0.1.20170707152712), so keep using\n            # fusermount. However, do not fail if you can't clean up, as\n            # there are arvados racing issues.\n            cleanup_funcs.append((\"unmount_arvados\",\n                                  '''{{\n                                  set +e &amp;&amp;\n                                  fusermount -u {mountpoint} &amp;&amp;\n                                  rm -rf {mountpoint} &amp;&amp;\n                                  set -e\n                                  }}'''.format(**locals())))\n\n        if \"job_condaenv\" in self.options:\n            # In conda &lt; 4.4 there is an issue with parallel activations,\n            # see https://github.com/conda/conda/issues/2837 .\n            # This has been fixed in conda 4.4, but we are on conda\n            # 4.3, presumably because we are still on py35. A work-around\n            # to source activate is to add the explicit path of the environment\n            # in version &gt;= 4.4, do\n            # setup_cmds.append(\n            #     \"conda activate {}\".format(self.options[\"job_condaenv\"]))\n            # For old conda versions (note this will not work for tools that require\n            # additional environment variables)\n            setup_cmds.append(\n                \"export PATH={}:$PATH\".format(\n                    os.path.join(\n                        get_conda_environment_directory(\n                            self.options[\"job_condaenv\"]),\n                        \"bin\")))\n\n        statement = \"\\n\".join((\n            \"\\n\".join(setup_cmds),\n            statement,\n            \"\\n\".join(teardown_cmds)))\n\n        return statement, cleanup_funcs\n\n    def build_job_script(self,\n                         statement):\n        '''build job script from statement.\n\n        returns (name_of_script, stdout_path, stderr_path)\n        '''\n        tmpfilename = get_temp_filename(dir=self.work_dir, clear=True)\n        tmpfilename = tmpfilename + \".sh\"\n\n        expanded_statement, cleanup_funcs = self.expand_statement(statement)\n\n        with open(tmpfilename, \"w\") as tmpfile:\n            # disabled: -l -O expand_aliases\\n\" )\n\n            # make executable\n            tmpfile.write(\"#!/bin/bash -eu\\n\")\n            if not self.ignore_pipe_errors:\n                tmpfile.write(\"set -o pipefail\\n\")\n\n            os.chmod(tmpfilename, stat.S_IRWXG | stat.S_IRWXU)\n\n            tmpfile.write(\"\\ncd {}\\n\".format(self.work_dir))\n            if self.output_directories is not None:\n                for outdir in self.output_directories:\n                    if outdir:\n                        tmpfile.write(\"\\nmkdir -p {}\\n\".format(outdir))\n\n            # create and set system scratch dir for temporary files\n            tmpfile.write(\"umask 002\\n\")\n\n            cluster_tmpdir = get_params()[\"cluster_tmpdir\"]\n\n            if self.run_on_cluster and cluster_tmpdir:\n                tmpdir = cluster_tmpdir\n                tmpfile.write(\"TMPDIR=`mktemp -d -p {}`\\n\".format(tmpdir))\n                tmpfile.write(\"export TMPDIR\\n\")\n            else:\n                tmpdir = get_temp_dir(dir=get_params()[\"tmpdir\"],\n                                      clear=True)\n                tmpfile.write(\"mkdir -p {}\\n\".format(tmpdir))\n                tmpfile.write(\"export TMPDIR={}\\n\".format(tmpdir))\n\n            cleanup_funcs.append(\n                (\"clean_temp\",\n                 \"{{ rm -rf {}; }}\".format(tmpdir)))\n\n            # output times whenever script exits, preserving\n            # return status\n            cleanup_funcs.append((\"info\",\n                                  \"{ echo 'benchmark'; hostname; times; }\"))\n            for cleanup_func, cleanup_code in cleanup_funcs:\n                tmpfile.write(\"\\n{}() {}\\n\".format(cleanup_func, cleanup_code))\n\n            tmpfile.write(\"\\nclean_all() {{ {}; }}\\n\".format(\n                \"; \".join([x[0] for x in cleanup_funcs])))\n\n            tmpfile.write(\"\\ntrap clean_all EXIT\\n\\n\")\n\n            if self.job_memory not in (\"unlimited\", \"etc\") and \\\n               self.options.get(\"cluster_memory_ulimit\", False):\n                # restrict virtual memory\n                # Note that there are resources in SGE which could do this directly\n                # such as v_hmem.\n                # Note that limiting resident set sizes (RSS) with ulimit is not\n                # possible in newer kernels.\n                # -v and -m accept memory in kb\n                requested_memory_kb = max(\n                    1000,\n                    int(math.ceil(\n                        iotools.human2bytes(self.job_memory) / 1024 * self.job_threads)))\n                # unsetting error exit as often not permissions\n                tmpfile.write(\"set +e\\n\")\n                tmpfile.write(\"ulimit -v {} &gt; /dev/null \\n\".format(\n                    requested_memory_kb))\n                tmpfile.write(\"ulimit -m {} &gt; /dev/null \\n\".format(\n                    requested_memory_kb))\n                # set as hard limit\n                tmpfile.write(\"ulimit -H -v &gt; /dev/null \\n\")\n                tmpfile.write(\"set -e\\n\")\n\n            if self.shellfile:\n\n                # make sure path exists that we want to write to\n                tmpfile.write(\"mkdir -p $(dirname \\\"{}\\\")\\n\".format(\n                    self.shellfile))\n\n                # output low-level debugging information to a shell log file\n                tmpfile.write(\n                    'echo \"%s : START -&gt; %s\" &gt;&gt; %s\\n' %\n                    (self.job_name, tmpfilename, self.shellfile))\n                # disabled - problems with quoting\n                # tmpfile.write( '''echo 'statement=%s' &gt;&gt; %s\\n''' %\n                # (shellquote(statement), self.shellfile) )\n                tmpfile.write(\"set | sed 's/^/%s : /' &gt;&gt; %s\\n\" %\n                              (self.job_name, self.shellfile))\n                tmpfile.write(\"pwd | sed 's/^/%s : /' &gt;&gt; %s\\n\" %\n                              (self.job_name, self.shellfile))\n                tmpfile.write(\"hostname | sed 's/^/%s: /' &gt;&gt; %s\\n\" %\n                              (self.job_name, self.shellfile))\n                # cat /proc/meminfo is Linux specific\n                if get_params()['os'] == 'Linux':\n                    tmpfile.write(\"cat /proc/meminfo | sed 's/^/%s: /' &gt;&gt; %s\\n\" %\n                                  (self.job_name, self.shellfile))\n                elif get_params()['os'] == 'Darwin':\n                    tmpfile.write(\"vm_stat | sed 's/^/%s: /' &gt;&gt; %s\\n\" %\n                                  (self.job_name, self.shellfile))\n                tmpfile.write(\n                    'echo \"%s : END -&gt; %s\" &gt;&gt; %s\\n' %\n                    (self.job_name, tmpfilename, self.shellfile))\n                tmpfile.write(\"ulimit | sed 's/^/%s: /' &gt;&gt; %s\\n\" %\n                              (self.job_name, self.shellfile))\n\n            job_path = os.path.abspath(tmpfilename)\n\n            tmpfile.write(expanded_statement)\n            tmpfile.write(\"\\n\\n\")\n            tmpfile.close()\n\n        return statement, job_path\n\n    def collect_benchmark_data(self,\n                               statements,\n                               resource_usage):\n        \"\"\"collect benchmark data from a job's stdout and any resource usage\n        information that might be present.\n\n        If time_data is given, read output from time command.\n        \"\"\"\n\n        benchmark_data = []\n\n        def get_val(d, v, alt):\n            val = d.get(v, alt)\n            if val == \"unknown\" or val is None:\n                val = alt\n            return val\n\n        # build resource usage data structure - part native, part\n        # mapped to common fields\n        for jobinfo, statement in zip(resource_usage, statements):\n\n            if resource_usage is None:\n                E.warn(\"no resource usage for {}\".format(self.task_name))\n                continue\n\n            # add some common fields\n            data = {\"task\": self.task_name,\n                    \"engine\": self.__class__.__name__,\n                    \"statement\": statement,\n                    \"job_id\": jobinfo.jobId,\n                    \"slots\": self.job_threads}\n\n            # native specs\n            data.update(jobinfo.resourceUsage)\n\n            # translate specs\n            if self.queue_manager:\n                data.update(\n                    self.queue_manager.map_resource_usage(data, DATA2TYPE))\n\n            cpu_time = float(get_val(data, \"cpu_t\", 0))\n            start_time = float(get_val(data, \"start_time\", 0))\n            end_time = float(get_val(data, \"end_time\", 0))\n            data.update({\n                # avoid division by 0 error\n                \"percent_cpu\": (\n                    100.0 * cpu_time / max(1.0, (end_time - start_time)) / self.job_threads),\n                \"total_t\": end_time - start_time\n            })\n            benchmark_data.append(data)\n\n        return benchmark_data\n\n    def set_container_config(self, image, volumes=None, env_vars=None, runtime=\"docker\"):\n        \"\"\"Set container configuration for all tasks executed by this executor.\"\"\"\n\n        if not image:\n            raise ValueError(\"An image must be specified for the container configuration.\")\n        self.container_config = ContainerConfig(image=image, volumes=volumes, env_vars=env_vars, runtime=runtime)\n\n    def start_job(self, job_info):\n        \"\"\"Add a job to active_jobs list when it starts.\"\"\"\n        self.active_jobs.append(job_info)\n        self.logger.info(f\"Job started: {job_info}\")\n\n    def finish_job(self, job_info):\n        \"\"\"Remove a job from active_jobs list when it finishes.\"\"\"\n        if job_info in self.active_jobs:\n            self.active_jobs.remove(job_info)\n            self.logger.info(f\"Job completed: {job_info}\")\n\n    def cleanup_all_jobs(self):\n        \"\"\"Clean up all remaining active jobs on interruption.\"\"\"\n        self.logger.info(\"Cleaning up all job outputs due to pipeline interruption\")\n        for job_info in self.active_jobs:\n            self.cleanup_failed_job(job_info)\n        self.active_jobs.clear()  # Clear the list after cleanup\n\n    def setup_signal_handlers(self):\n        \"\"\"Set up signal handlers to clean up jobs on SIGINT and SIGTERM.\"\"\"\n\n        def signal_handler(signum, frame):\n            self.logger.info(f\"Received signal {signum}. Starting clean-up.\")\n            self.cleanup_all_jobs()\n            exit(1)\n\n        signal.signal(signal.SIGINT, signal_handler)\n        signal.signal(signal.SIGTERM, signal_handler)\n\n    def cleanup_failed_job(self, job_info):\n        \"\"\"Clean up files generated by a failed job.\"\"\"\n        if \"outfile\" in job_info:\n            outfiles = [job_info[\"outfile\"]]\n        elif \"outfiles\" in job_info:\n            outfiles = job_info[\"outfiles\"]\n        else:\n            self.logger.warning(f\"No output files found for job {job_info.get('job_name', 'unknown')}\")\n            return\n\n        for outfile in outfiles:\n            if os.path.exists(outfile):\n                try:\n                    os.remove(outfile)\n                    self.logger.info(f\"Removed failed job output file: {outfile}\")\n                except OSError as e:\n                    self.logger.error(f\"Error removing file {outfile}: {str(e)}\")\n            else:\n                self.logger.info(f\"Output file not found (already removed or not created): {outfile}\")\n\n    def run(\n            self,\n            statement_list,\n            job_memory=None,\n            job_threads=None,\n            container_runtime=None,\n            image=None,\n            volumes=None,\n            env_vars=None,\n            **kwargs,):\n\n        \"\"\"\n        Execute a list of statements with optional container support.\n\n            Args:\n                statement_list (list): List of commands to execute.\n                job_memory (str): Memory requirements (e.g., \"4G\").\n                job_threads (int): Number of threads to use.\n                container_runtime (str): Container runtime (\"docker\" or \"singularity\").\n                image (str): Container image to use.\n                volumes (list): Volume mappings (e.g., ['/data:/data']).\n                env_vars (dict): Environment variables for the container.\n                **kwargs: Additional arguments.\n        \"\"\"\n        # Validation checks\n        if container_runtime and container_runtime not in [\"docker\", \"singularity\"]:\n            self.logger.error(f\"Invalid container_runtime: {container_runtime}\")\n            raise ValueError(\"Container runtime must be 'docker' or 'singularity'\")\n\n        if container_runtime and not image:\n            self.logger.error(f\"Container runtime specified without an image: {container_runtime}\")\n            raise ValueError(\"An image must be specified when using a container runtime\")\n\n        benchmark_data = []\n\n        for statement in statement_list:\n            job_info = {\"statement\": statement}\n            self.start_job(job_info)\n\n            try:\n                # Prepare containerized execution\n                if container_runtime:\n                    self.set_container_config(image=image, volumes=volumes, env_vars=env_vars, runtime=container_runtime)\n                    statement = self.container_config.get_container_command(statement)\n\n                # Add memory and thread environment variables\n                if job_memory:\n                    env_vars = env_vars or {}\n                    env_vars[\"JOB_MEMORY\"] = job_memory\n                if job_threads:\n                    env_vars = env_vars or {}\n                    env_vars[\"JOB_THREADS\"] = job_threads\n\n                # Debugging: Log the constructed command\n                self.logger.info(f\"Executing command: {statement}\")\n\n                # Build and execute the statement\n                full_statement, job_path = self.build_job_script(statement)\n                process = subprocess.Popen(\n                    full_statement, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE\n                )\n                stdout, stderr = process.communicate()\n\n                if process.returncode != 0:\n                    raise OSError(\n                        f\"Job failed with return code {process.returncode}.\\n\"\n                        f\"stderr: {stderr.decode('utf-8')}\\ncommand: {statement}\"\n                    )\n\n                # Collect benchmark data for successful jobs\n                benchmark_data.append(\n                    self.collect_benchmark_data(\n                        statement, resource_usage={\"job_id\": process.pid}\n                    )\n                )\n                self.finish_job(job_info)\n\n            except Exception as e:\n                self.logger.error(f\"Job failed: {e}\")\n                self.cleanup_failed_job(job_info)\n                if not self.ignore_errors:\n                    raise\n\n        return benchmark_data\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.Executor.build_job_script","title":"<code>build_job_script(statement)</code>","text":"<p>build job script from statement.</p> <p>returns (name_of_script, stdout_path, stderr_path)</p> Source code in <code>cgatcore/pipeline/execution.py</code> <pre><code>def build_job_script(self,\n                     statement):\n    '''build job script from statement.\n\n    returns (name_of_script, stdout_path, stderr_path)\n    '''\n    tmpfilename = get_temp_filename(dir=self.work_dir, clear=True)\n    tmpfilename = tmpfilename + \".sh\"\n\n    expanded_statement, cleanup_funcs = self.expand_statement(statement)\n\n    with open(tmpfilename, \"w\") as tmpfile:\n        # disabled: -l -O expand_aliases\\n\" )\n\n        # make executable\n        tmpfile.write(\"#!/bin/bash -eu\\n\")\n        if not self.ignore_pipe_errors:\n            tmpfile.write(\"set -o pipefail\\n\")\n\n        os.chmod(tmpfilename, stat.S_IRWXG | stat.S_IRWXU)\n\n        tmpfile.write(\"\\ncd {}\\n\".format(self.work_dir))\n        if self.output_directories is not None:\n            for outdir in self.output_directories:\n                if outdir:\n                    tmpfile.write(\"\\nmkdir -p {}\\n\".format(outdir))\n\n        # create and set system scratch dir for temporary files\n        tmpfile.write(\"umask 002\\n\")\n\n        cluster_tmpdir = get_params()[\"cluster_tmpdir\"]\n\n        if self.run_on_cluster and cluster_tmpdir:\n            tmpdir = cluster_tmpdir\n            tmpfile.write(\"TMPDIR=`mktemp -d -p {}`\\n\".format(tmpdir))\n            tmpfile.write(\"export TMPDIR\\n\")\n        else:\n            tmpdir = get_temp_dir(dir=get_params()[\"tmpdir\"],\n                                  clear=True)\n            tmpfile.write(\"mkdir -p {}\\n\".format(tmpdir))\n            tmpfile.write(\"export TMPDIR={}\\n\".format(tmpdir))\n\n        cleanup_funcs.append(\n            (\"clean_temp\",\n             \"{{ rm -rf {}; }}\".format(tmpdir)))\n\n        # output times whenever script exits, preserving\n        # return status\n        cleanup_funcs.append((\"info\",\n                              \"{ echo 'benchmark'; hostname; times; }\"))\n        for cleanup_func, cleanup_code in cleanup_funcs:\n            tmpfile.write(\"\\n{}() {}\\n\".format(cleanup_func, cleanup_code))\n\n        tmpfile.write(\"\\nclean_all() {{ {}; }}\\n\".format(\n            \"; \".join([x[0] for x in cleanup_funcs])))\n\n        tmpfile.write(\"\\ntrap clean_all EXIT\\n\\n\")\n\n        if self.job_memory not in (\"unlimited\", \"etc\") and \\\n           self.options.get(\"cluster_memory_ulimit\", False):\n            # restrict virtual memory\n            # Note that there are resources in SGE which could do this directly\n            # such as v_hmem.\n            # Note that limiting resident set sizes (RSS) with ulimit is not\n            # possible in newer kernels.\n            # -v and -m accept memory in kb\n            requested_memory_kb = max(\n                1000,\n                int(math.ceil(\n                    iotools.human2bytes(self.job_memory) / 1024 * self.job_threads)))\n            # unsetting error exit as often not permissions\n            tmpfile.write(\"set +e\\n\")\n            tmpfile.write(\"ulimit -v {} &gt; /dev/null \\n\".format(\n                requested_memory_kb))\n            tmpfile.write(\"ulimit -m {} &gt; /dev/null \\n\".format(\n                requested_memory_kb))\n            # set as hard limit\n            tmpfile.write(\"ulimit -H -v &gt; /dev/null \\n\")\n            tmpfile.write(\"set -e\\n\")\n\n        if self.shellfile:\n\n            # make sure path exists that we want to write to\n            tmpfile.write(\"mkdir -p $(dirname \\\"{}\\\")\\n\".format(\n                self.shellfile))\n\n            # output low-level debugging information to a shell log file\n            tmpfile.write(\n                'echo \"%s : START -&gt; %s\" &gt;&gt; %s\\n' %\n                (self.job_name, tmpfilename, self.shellfile))\n            # disabled - problems with quoting\n            # tmpfile.write( '''echo 'statement=%s' &gt;&gt; %s\\n''' %\n            # (shellquote(statement), self.shellfile) )\n            tmpfile.write(\"set | sed 's/^/%s : /' &gt;&gt; %s\\n\" %\n                          (self.job_name, self.shellfile))\n            tmpfile.write(\"pwd | sed 's/^/%s : /' &gt;&gt; %s\\n\" %\n                          (self.job_name, self.shellfile))\n            tmpfile.write(\"hostname | sed 's/^/%s: /' &gt;&gt; %s\\n\" %\n                          (self.job_name, self.shellfile))\n            # cat /proc/meminfo is Linux specific\n            if get_params()['os'] == 'Linux':\n                tmpfile.write(\"cat /proc/meminfo | sed 's/^/%s: /' &gt;&gt; %s\\n\" %\n                              (self.job_name, self.shellfile))\n            elif get_params()['os'] == 'Darwin':\n                tmpfile.write(\"vm_stat | sed 's/^/%s: /' &gt;&gt; %s\\n\" %\n                              (self.job_name, self.shellfile))\n            tmpfile.write(\n                'echo \"%s : END -&gt; %s\" &gt;&gt; %s\\n' %\n                (self.job_name, tmpfilename, self.shellfile))\n            tmpfile.write(\"ulimit | sed 's/^/%s: /' &gt;&gt; %s\\n\" %\n                          (self.job_name, self.shellfile))\n\n        job_path = os.path.abspath(tmpfilename)\n\n        tmpfile.write(expanded_statement)\n        tmpfile.write(\"\\n\\n\")\n        tmpfile.close()\n\n    return statement, job_path\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.Executor.cleanup_all_jobs","title":"<code>cleanup_all_jobs()</code>","text":"<p>Clean up all remaining active jobs on interruption.</p> Source code in <code>cgatcore/pipeline/execution.py</code> <pre><code>def cleanup_all_jobs(self):\n    \"\"\"Clean up all remaining active jobs on interruption.\"\"\"\n    self.logger.info(\"Cleaning up all job outputs due to pipeline interruption\")\n    for job_info in self.active_jobs:\n        self.cleanup_failed_job(job_info)\n    self.active_jobs.clear()  # Clear the list after cleanup\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.Executor.cleanup_failed_job","title":"<code>cleanup_failed_job(job_info)</code>","text":"<p>Clean up files generated by a failed job.</p> Source code in <code>cgatcore/pipeline/execution.py</code> <pre><code>def cleanup_failed_job(self, job_info):\n    \"\"\"Clean up files generated by a failed job.\"\"\"\n    if \"outfile\" in job_info:\n        outfiles = [job_info[\"outfile\"]]\n    elif \"outfiles\" in job_info:\n        outfiles = job_info[\"outfiles\"]\n    else:\n        self.logger.warning(f\"No output files found for job {job_info.get('job_name', 'unknown')}\")\n        return\n\n    for outfile in outfiles:\n        if os.path.exists(outfile):\n            try:\n                os.remove(outfile)\n                self.logger.info(f\"Removed failed job output file: {outfile}\")\n            except OSError as e:\n                self.logger.error(f\"Error removing file {outfile}: {str(e)}\")\n        else:\n            self.logger.info(f\"Output file not found (already removed or not created): {outfile}\")\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.Executor.collect_benchmark_data","title":"<code>collect_benchmark_data(statements, resource_usage)</code>","text":"<p>collect benchmark data from a job's stdout and any resource usage information that might be present.</p> <p>If time_data is given, read output from time command.</p> Source code in <code>cgatcore/pipeline/execution.py</code> <pre><code>def collect_benchmark_data(self,\n                           statements,\n                           resource_usage):\n    \"\"\"collect benchmark data from a job's stdout and any resource usage\n    information that might be present.\n\n    If time_data is given, read output from time command.\n    \"\"\"\n\n    benchmark_data = []\n\n    def get_val(d, v, alt):\n        val = d.get(v, alt)\n        if val == \"unknown\" or val is None:\n            val = alt\n        return val\n\n    # build resource usage data structure - part native, part\n    # mapped to common fields\n    for jobinfo, statement in zip(resource_usage, statements):\n\n        if resource_usage is None:\n            E.warn(\"no resource usage for {}\".format(self.task_name))\n            continue\n\n        # add some common fields\n        data = {\"task\": self.task_name,\n                \"engine\": self.__class__.__name__,\n                \"statement\": statement,\n                \"job_id\": jobinfo.jobId,\n                \"slots\": self.job_threads}\n\n        # native specs\n        data.update(jobinfo.resourceUsage)\n\n        # translate specs\n        if self.queue_manager:\n            data.update(\n                self.queue_manager.map_resource_usage(data, DATA2TYPE))\n\n        cpu_time = float(get_val(data, \"cpu_t\", 0))\n        start_time = float(get_val(data, \"start_time\", 0))\n        end_time = float(get_val(data, \"end_time\", 0))\n        data.update({\n            # avoid division by 0 error\n            \"percent_cpu\": (\n                100.0 * cpu_time / max(1.0, (end_time - start_time)) / self.job_threads),\n            \"total_t\": end_time - start_time\n        })\n        benchmark_data.append(data)\n\n    return benchmark_data\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.Executor.expand_statement","title":"<code>expand_statement(statement)</code>","text":"<p>add generic commands before and after statement.</p> <p>The method scans the statement for arvados mount points and inserts appropriate prefixes to make sure that the mount point exists.</p>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.Executor.expand_statement--arguments","title":"Arguments","text":"<p>statement : string     Command line statement to expand</p>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.Executor.expand_statement--returns","title":"Returns","text":"<p>statement : string     The expanded statement.</p> Source code in <code>cgatcore/pipeline/execution.py</code> <pre><code>def expand_statement(self, statement):\n    '''add generic commands before and after statement.\n\n    The method scans the statement for arvados mount points and\n    inserts appropriate prefixes to make sure that the mount point\n    exists.\n\n    Arguments\n    ---------\n    statement : string\n        Command line statement to expand\n\n    Returns\n    -------\n    statement : string\n        The expanded statement.\n\n    '''\n\n    setup_cmds = []\n    teardown_cmds = []\n    cleanup_funcs = []\n\n    setup_cmds.append(\"umask 002\")\n\n    for var in [\"MKL_NUM_THREADS\",\n                \"OPENBLAS_NUM_THREADS\",\n                \"OMP_NUM_THREADS\"]:\n        setup_cmds.append(\"export {}={}\".format(\n            var, self.options.get(\"job_threads\", 1)))\n\n    if \"arv=\" in statement:\n\n        # Todo: permit setting this in params\n        arvados_api_token = os.environ.get(\"ARVADOS_API_TOKEN\", None)\n        arvados_api_host = os.environ.get(\"ARVADOS_API_HOST\", None)\n        if not arvados_api_token:\n            raise ValueError(\n                \"arvados mount encountered in statement {}, \"\n                \"but ARVADOS_API_TOKEN not defined\".format(statement))\n\n        if not arvados_api_host:\n            raise ValueError(\n                \"arvados mount encountered in statement {}, \"\n                \"but ARVADOS_API_HOST not defined\".format(statement))\n\n        mountpoint = get_temp_filename(clear=True)\n\n        arvados_options = \"--disable-event-listening --read-only\"\n        setup_cmds.append(\"\\n\".join(\n            ('export ARVADOS_API_TOKEN=\"{arvados_api_token}\"',\n             'export ARVADOS_API_HOST=\"{arvados_api_host}\"',\n             'export ARVADOS_API_HOST_INSECURE=true',\n             'export ARVADOS_MOUNT_POINT=\"{mountpoint}\"',\n             'mkdir -p \"{mountpoint}\"',\n             'arv-mount {arvados_options} \"{mountpoint}\" 2&gt;&gt; /dev/null')).format(**locals()))\n\n        statement = re.sub(\"arv=\", mountpoint + \"/\", statement)\n\n        # \"arv-mount --unmount {mountpoint}\" not available in newer\n        # arvados installs (0.1.20170707152712), so keep using\n        # fusermount. However, do not fail if you can't clean up, as\n        # there are arvados racing issues.\n        cleanup_funcs.append((\"unmount_arvados\",\n                              '''{{\n                              set +e &amp;&amp;\n                              fusermount -u {mountpoint} &amp;&amp;\n                              rm -rf {mountpoint} &amp;&amp;\n                              set -e\n                              }}'''.format(**locals())))\n\n    if \"job_condaenv\" in self.options:\n        # In conda &lt; 4.4 there is an issue with parallel activations,\n        # see https://github.com/conda/conda/issues/2837 .\n        # This has been fixed in conda 4.4, but we are on conda\n        # 4.3, presumably because we are still on py35. A work-around\n        # to source activate is to add the explicit path of the environment\n        # in version &gt;= 4.4, do\n        # setup_cmds.append(\n        #     \"conda activate {}\".format(self.options[\"job_condaenv\"]))\n        # For old conda versions (note this will not work for tools that require\n        # additional environment variables)\n        setup_cmds.append(\n            \"export PATH={}:$PATH\".format(\n                os.path.join(\n                    get_conda_environment_directory(\n                        self.options[\"job_condaenv\"]),\n                    \"bin\")))\n\n    statement = \"\\n\".join((\n        \"\\n\".join(setup_cmds),\n        statement,\n        \"\\n\".join(teardown_cmds)))\n\n    return statement, cleanup_funcs\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.Executor.finish_job","title":"<code>finish_job(job_info)</code>","text":"<p>Remove a job from active_jobs list when it finishes.</p> Source code in <code>cgatcore/pipeline/execution.py</code> <pre><code>def finish_job(self, job_info):\n    \"\"\"Remove a job from active_jobs list when it finishes.\"\"\"\n    if job_info in self.active_jobs:\n        self.active_jobs.remove(job_info)\n        self.logger.info(f\"Job completed: {job_info}\")\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.Executor.run","title":"<code>run(statement_list, job_memory=None, job_threads=None, container_runtime=None, image=None, volumes=None, env_vars=None, **kwargs)</code>","text":"<p>Execute a list of statements with optional container support.</p> <pre><code>Args:\n    statement_list (list): List of commands to execute.\n    job_memory (str): Memory requirements (e.g., \"4G\").\n    job_threads (int): Number of threads to use.\n    container_runtime (str): Container runtime (\"docker\" or \"singularity\").\n    image (str): Container image to use.\n    volumes (list): Volume mappings (e.g., ['/data:/data']).\n    env_vars (dict): Environment variables for the container.\n    **kwargs: Additional arguments.\n</code></pre> Source code in <code>cgatcore/pipeline/execution.py</code> <pre><code>def run(\n        self,\n        statement_list,\n        job_memory=None,\n        job_threads=None,\n        container_runtime=None,\n        image=None,\n        volumes=None,\n        env_vars=None,\n        **kwargs,):\n\n    \"\"\"\n    Execute a list of statements with optional container support.\n\n        Args:\n            statement_list (list): List of commands to execute.\n            job_memory (str): Memory requirements (e.g., \"4G\").\n            job_threads (int): Number of threads to use.\n            container_runtime (str): Container runtime (\"docker\" or \"singularity\").\n            image (str): Container image to use.\n            volumes (list): Volume mappings (e.g., ['/data:/data']).\n            env_vars (dict): Environment variables for the container.\n            **kwargs: Additional arguments.\n    \"\"\"\n    # Validation checks\n    if container_runtime and container_runtime not in [\"docker\", \"singularity\"]:\n        self.logger.error(f\"Invalid container_runtime: {container_runtime}\")\n        raise ValueError(\"Container runtime must be 'docker' or 'singularity'\")\n\n    if container_runtime and not image:\n        self.logger.error(f\"Container runtime specified without an image: {container_runtime}\")\n        raise ValueError(\"An image must be specified when using a container runtime\")\n\n    benchmark_data = []\n\n    for statement in statement_list:\n        job_info = {\"statement\": statement}\n        self.start_job(job_info)\n\n        try:\n            # Prepare containerized execution\n            if container_runtime:\n                self.set_container_config(image=image, volumes=volumes, env_vars=env_vars, runtime=container_runtime)\n                statement = self.container_config.get_container_command(statement)\n\n            # Add memory and thread environment variables\n            if job_memory:\n                env_vars = env_vars or {}\n                env_vars[\"JOB_MEMORY\"] = job_memory\n            if job_threads:\n                env_vars = env_vars or {}\n                env_vars[\"JOB_THREADS\"] = job_threads\n\n            # Debugging: Log the constructed command\n            self.logger.info(f\"Executing command: {statement}\")\n\n            # Build and execute the statement\n            full_statement, job_path = self.build_job_script(statement)\n            process = subprocess.Popen(\n                full_statement, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE\n            )\n            stdout, stderr = process.communicate()\n\n            if process.returncode != 0:\n                raise OSError(\n                    f\"Job failed with return code {process.returncode}.\\n\"\n                    f\"stderr: {stderr.decode('utf-8')}\\ncommand: {statement}\"\n                )\n\n            # Collect benchmark data for successful jobs\n            benchmark_data.append(\n                self.collect_benchmark_data(\n                    statement, resource_usage={\"job_id\": process.pid}\n                )\n            )\n            self.finish_job(job_info)\n\n        except Exception as e:\n            self.logger.error(f\"Job failed: {e}\")\n            self.cleanup_failed_job(job_info)\n            if not self.ignore_errors:\n                raise\n\n    return benchmark_data\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.Executor.set_container_config","title":"<code>set_container_config(image, volumes=None, env_vars=None, runtime='docker')</code>","text":"<p>Set container configuration for all tasks executed by this executor.</p> Source code in <code>cgatcore/pipeline/execution.py</code> <pre><code>def set_container_config(self, image, volumes=None, env_vars=None, runtime=\"docker\"):\n    \"\"\"Set container configuration for all tasks executed by this executor.\"\"\"\n\n    if not image:\n        raise ValueError(\"An image must be specified for the container configuration.\")\n    self.container_config = ContainerConfig(image=image, volumes=volumes, env_vars=env_vars, runtime=runtime)\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.Executor.setup_signal_handlers","title":"<code>setup_signal_handlers()</code>","text":"<p>Set up signal handlers to clean up jobs on SIGINT and SIGTERM.</p> Source code in <code>cgatcore/pipeline/execution.py</code> <pre><code>def setup_signal_handlers(self):\n    \"\"\"Set up signal handlers to clean up jobs on SIGINT and SIGTERM.\"\"\"\n\n    def signal_handler(signum, frame):\n        self.logger.info(f\"Received signal {signum}. Starting clean-up.\")\n        self.cleanup_all_jobs()\n        exit(1)\n\n    signal.signal(signal.SIGINT, signal_handler)\n    signal.signal(signal.SIGTERM, signal_handler)\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.Executor.start_job","title":"<code>start_job(job_info)</code>","text":"<p>Add a job to active_jobs list when it starts.</p> Source code in <code>cgatcore/pipeline/execution.py</code> <pre><code>def start_job(self, job_info):\n    \"\"\"Add a job to active_jobs list when it starts.\"\"\"\n    self.active_jobs.append(job_info)\n    self.logger.info(f\"Job started: {job_info}\")\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.KubernetesExecutor","title":"<code>KubernetesExecutor</code>","text":"<p>               Bases: <code>BaseExecutor</code></p> <p>Executor for managing and running jobs on a Kubernetes cluster.</p> <p>This class is responsible for submitting jobs to a Kubernetes cluster, monitoring their execution, and collecting benchmark data related to their performance.</p> <p>Attributes:</p> Name Type Description <code>namespace</code> <code>str</code> <p>The Kubernetes namespace in which to run the jobs. Defaults to 'default'.</p> <code>api</code> <code>CoreV1Api</code> <p>The Kubernetes Core API client for interacting with the cluster.</p> <code>batch_api</code> <code>BatchV1Api</code> <p>The Kubernetes Batch API client for managing jobs.</p> Source code in <code>cgatcore/pipeline/kubernetes.py</code> <pre><code>class KubernetesExecutor(BaseExecutor):\n    \"\"\"Executor for managing and running jobs on a Kubernetes cluster.\n\n    This class is responsible for submitting jobs to a Kubernetes cluster, monitoring their execution,\n    and collecting benchmark data related to their performance.\n\n    Attributes:\n        namespace (str): The Kubernetes namespace in which to run the jobs. Defaults to 'default'.\n        api (CoreV1Api): The Kubernetes Core API client for interacting with the cluster.\n        batch_api (BatchV1Api): The Kubernetes Batch API client for managing jobs.\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Initializes the KubernetesExecutor with the specified configuration options.\n\n        Args:\n            **kwargs: Additional configuration options, including the namespace.\n        \"\"\"\n        super().__init__(**kwargs)\n        self.namespace = kwargs.get(\"namespace\", \"default\")\n\n        # Load Kubernetes configuration\n        try:\n            config.load_kube_config()\n            self.api = client.CoreV1Api()\n            self.batch_api = client.BatchV1Api()\n            logger.info(\"Kubernetes configuration loaded successfully.\")\n        except exceptions.ConfigException as e:\n            logger.error(\"Failed to load Kubernetes configuration\", exc_info=True)\n            raise e\n\n    def run(self, statement, job_path, job_condaenv):\n        \"\"\"Submits a job to the Kubernetes cluster to run the specified command.\n\n        This method creates a Kubernetes Job object and submits it to the cluster. The job runs the\n        specified command in a container, using the provided Conda environment.\n\n        Args:\n            statement (str): The command to execute in the job.\n            job_path (str): The path to the job script.\n            job_condaenv (str): The name of the Conda environment to use.\n        \"\"\"\n        job_name = f\"cgat-{os.path.basename(job_path)}-{int(time.time())}\"\n        container_image = \"your-docker-image:tag\"  # Replace with your Docker image\n\n        # Define Kubernetes Job spec\n        job_spec = client.V1Job(\n            metadata=client.V1ObjectMeta(name=job_name),\n            spec=client.V1JobSpec(\n                template=client.V1PodTemplateSpec(\n                    spec=client.V1PodSpec(\n                        containers=[\n                            client.V1Container(\n                                name=\"cgat-job\",\n                                image=container_image,\n                                command=[\"/bin/bash\", \"-c\", statement],\n                                env=[client.V1EnvVar(name=\"CONDA_ENV\", value=job_condaenv)],\n                            )\n                        ],\n                        restart_policy=\"Never\"\n                    )\n                ),\n                backoff_limit=4  # Retry policy in case of transient failures\n            )\n        )\n\n        # Create and monitor Kubernetes Job\n        try:\n            logger.info(f\"Creating Kubernetes Job '{job_name}' in namespace '{self.namespace}'.\")\n            start_time = datetime.now()\n            self.batch_api.create_namespaced_job(self.namespace, job_spec)\n            self._wait_for_job_completion(job_name)\n            end_time = datetime.now()\n            logs = self._get_pod_logs(job_name)\n            self.collect_metric_data(\"Kubernetes Job\", start_time, end_time, \"time_data.json\")\n        finally:\n            self._cleanup_job(job_name)\n\n        return logs\n\n    def _wait_for_job_completion(self, job_name):\n        \"\"\"Wait until the job completes or fails.\"\"\"\n        while True:\n            job_status = self.batch_api.read_namespaced_job_status(job_name, self.namespace).status\n            if job_status.succeeded:\n                logger.info(f\"Job '{job_name}' completed successfully.\")\n                return\n            if job_status.failed:\n                logger.error(f\"Job '{job_name}' failed.\")\n                raise RuntimeError(f\"Kubernetes Job {job_name} failed.\")\n            time.sleep(5)\n\n    def _get_pod_logs(self, job_name):\n        \"\"\"Retrieve logs from the Job's pod.\"\"\"\n        pods = self.api.list_namespaced_pod(self.namespace, label_selector=f\"job-name={job_name}\").items\n        if not pods:\n            logger.error(f\"No pod found for job '{job_name}'.\")\n            raise RuntimeError(f\"No pod found for job '{job_name}'.\")\n\n        pod_name = pods[0].metadata.name\n        logger.info(f\"Fetching logs from pod '{pod_name}'.\")\n        return self.api.read_namespaced_pod_log(pod_name, self.namespace)\n\n    def _cleanup_job(self, job_name):\n        \"\"\"Delete the Job and its pods.\"\"\"\n        try:\n            self.batch_api.delete_namespaced_job(job_name, self.namespace, propagation_policy=\"Background\")\n            logger.info(f\"Job '{job_name}' cleaned up successfully.\")\n        except exceptions.ApiException as e:\n            logger.warning(f\"Failed to delete Job '{job_name}'\", exc_info=True)\n\n    def collect_benchmark_data(self, statements, resource_usage=None):\n        \"\"\"Collect benchmark data for Kubernetes jobs.\n\n        This method gathers information about the executed statements and any resource usage data.\n\n        Args:\n            statements (list): List of executed statements.\n            resource_usage (list, optional): Resource usage data.\n\n        Returns:\n            dict: A dictionary containing the task name, total execution time, executed statements,\n                  and resource usage data.\n        \"\"\"\n        return {\n            \"task\": \"kubernetes_task\",\n            \"total_t\": 12,  # Example value, adjust as needed\n            \"statements\": statements,\n            \"resource_usage\": resource_usage or []\n        }\n\n    def collect_metric_data(self, process, start_time, end_time, time_data_file):\n        \"\"\"\n        Collects metric data related to job duration and writes it to a file.\n\n        Parameters:\n        - process (str): Process name for tracking purposes.\n        - start_time (datetime): Timestamp when the job started.\n        - end_time (datetime): Timestamp when the job ended.\n        - time_data_file (str): Path to a file where timing data will be saved.\n        \"\"\"\n        duration = (end_time - start_time).total_seconds()\n        metric_data = {\n            \"process\": process,\n            \"start_time\": start_time.isoformat(),\n            \"end_time\": end_time.isoformat(),\n            \"duration_seconds\": duration\n        }\n\n        # Log metric data\n        logger.info(\n            f\"Metric data collected for process '{process}': start time = {start_time}, end time = {end_time}, \"\n            f\"duration = {duration} seconds.\"\n        )\n\n        # Write metric data to file\n        try:\n            with open(time_data_file, \"w\") as f:\n                json.dump(metric_data, f, indent=4)\n            logger.info(f\"Metric data saved to {time_data_file}\")\n\n        except Exception as e:\n            logger.error(\"Error writing metric data to file\", exc_info=True)\n            raise e\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.KubernetesExecutor.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initializes the KubernetesExecutor with the specified configuration options.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional configuration options, including the namespace.</p> <code>{}</code> Source code in <code>cgatcore/pipeline/kubernetes.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initializes the KubernetesExecutor with the specified configuration options.\n\n    Args:\n        **kwargs: Additional configuration options, including the namespace.\n    \"\"\"\n    super().__init__(**kwargs)\n    self.namespace = kwargs.get(\"namespace\", \"default\")\n\n    # Load Kubernetes configuration\n    try:\n        config.load_kube_config()\n        self.api = client.CoreV1Api()\n        self.batch_api = client.BatchV1Api()\n        logger.info(\"Kubernetes configuration loaded successfully.\")\n    except exceptions.ConfigException as e:\n        logger.error(\"Failed to load Kubernetes configuration\", exc_info=True)\n        raise e\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.KubernetesExecutor.collect_benchmark_data","title":"<code>collect_benchmark_data(statements, resource_usage=None)</code>","text":"<p>Collect benchmark data for Kubernetes jobs.</p> <p>This method gathers information about the executed statements and any resource usage data.</p> <p>Parameters:</p> Name Type Description Default <code>statements</code> <code>list</code> <p>List of executed statements.</p> required <code>resource_usage</code> <code>list</code> <p>Resource usage data.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the task name, total execution time, executed statements,   and resource usage data.</p> Source code in <code>cgatcore/pipeline/kubernetes.py</code> <pre><code>def collect_benchmark_data(self, statements, resource_usage=None):\n    \"\"\"Collect benchmark data for Kubernetes jobs.\n\n    This method gathers information about the executed statements and any resource usage data.\n\n    Args:\n        statements (list): List of executed statements.\n        resource_usage (list, optional): Resource usage data.\n\n    Returns:\n        dict: A dictionary containing the task name, total execution time, executed statements,\n              and resource usage data.\n    \"\"\"\n    return {\n        \"task\": \"kubernetes_task\",\n        \"total_t\": 12,  # Example value, adjust as needed\n        \"statements\": statements,\n        \"resource_usage\": resource_usage or []\n    }\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.KubernetesExecutor.collect_metric_data","title":"<code>collect_metric_data(process, start_time, end_time, time_data_file)</code>","text":"<p>Collects metric data related to job duration and writes it to a file.</p> <p>Parameters: - process (str): Process name for tracking purposes. - start_time (datetime): Timestamp when the job started. - end_time (datetime): Timestamp when the job ended. - time_data_file (str): Path to a file where timing data will be saved.</p> Source code in <code>cgatcore/pipeline/kubernetes.py</code> <pre><code>def collect_metric_data(self, process, start_time, end_time, time_data_file):\n    \"\"\"\n    Collects metric data related to job duration and writes it to a file.\n\n    Parameters:\n    - process (str): Process name for tracking purposes.\n    - start_time (datetime): Timestamp when the job started.\n    - end_time (datetime): Timestamp when the job ended.\n    - time_data_file (str): Path to a file where timing data will be saved.\n    \"\"\"\n    duration = (end_time - start_time).total_seconds()\n    metric_data = {\n        \"process\": process,\n        \"start_time\": start_time.isoformat(),\n        \"end_time\": end_time.isoformat(),\n        \"duration_seconds\": duration\n    }\n\n    # Log metric data\n    logger.info(\n        f\"Metric data collected for process '{process}': start time = {start_time}, end time = {end_time}, \"\n        f\"duration = {duration} seconds.\"\n    )\n\n    # Write metric data to file\n    try:\n        with open(time_data_file, \"w\") as f:\n            json.dump(metric_data, f, indent=4)\n        logger.info(f\"Metric data saved to {time_data_file}\")\n\n    except Exception as e:\n        logger.error(\"Error writing metric data to file\", exc_info=True)\n        raise e\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.KubernetesExecutor.run","title":"<code>run(statement, job_path, job_condaenv)</code>","text":"<p>Submits a job to the Kubernetes cluster to run the specified command.</p> <p>This method creates a Kubernetes Job object and submits it to the cluster. The job runs the specified command in a container, using the provided Conda environment.</p> <p>Parameters:</p> Name Type Description Default <code>statement</code> <code>str</code> <p>The command to execute in the job.</p> required <code>job_path</code> <code>str</code> <p>The path to the job script.</p> required <code>job_condaenv</code> <code>str</code> <p>The name of the Conda environment to use.</p> required Source code in <code>cgatcore/pipeline/kubernetes.py</code> <pre><code>def run(self, statement, job_path, job_condaenv):\n    \"\"\"Submits a job to the Kubernetes cluster to run the specified command.\n\n    This method creates a Kubernetes Job object and submits it to the cluster. The job runs the\n    specified command in a container, using the provided Conda environment.\n\n    Args:\n        statement (str): The command to execute in the job.\n        job_path (str): The path to the job script.\n        job_condaenv (str): The name of the Conda environment to use.\n    \"\"\"\n    job_name = f\"cgat-{os.path.basename(job_path)}-{int(time.time())}\"\n    container_image = \"your-docker-image:tag\"  # Replace with your Docker image\n\n    # Define Kubernetes Job spec\n    job_spec = client.V1Job(\n        metadata=client.V1ObjectMeta(name=job_name),\n        spec=client.V1JobSpec(\n            template=client.V1PodTemplateSpec(\n                spec=client.V1PodSpec(\n                    containers=[\n                        client.V1Container(\n                            name=\"cgat-job\",\n                            image=container_image,\n                            command=[\"/bin/bash\", \"-c\", statement],\n                            env=[client.V1EnvVar(name=\"CONDA_ENV\", value=job_condaenv)],\n                        )\n                    ],\n                    restart_policy=\"Never\"\n                )\n            ),\n            backoff_limit=4  # Retry policy in case of transient failures\n        )\n    )\n\n    # Create and monitor Kubernetes Job\n    try:\n        logger.info(f\"Creating Kubernetes Job '{job_name}' in namespace '{self.namespace}'.\")\n        start_time = datetime.now()\n        self.batch_api.create_namespaced_job(self.namespace, job_spec)\n        self._wait_for_job_completion(job_name)\n        end_time = datetime.now()\n        logs = self._get_pod_logs(job_name)\n        self.collect_metric_data(\"Kubernetes Job\", start_time, end_time, \"time_data.json\")\n    finally:\n        self._cleanup_job(job_name)\n\n    return logs\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.LoggingFilterProgress","title":"<code>LoggingFilterProgress</code>","text":"<p>               Bases: <code>Filter</code></p> <p>add progress information to the log-stream.</p> <p>A :term:<code>task</code> is a ruffus_ decorated function, which will execute one or more :term:<code>jobs</code>.</p> <p>Valid task/job status: update    task/job needs updating completed    task/job completed successfully failed    task/job failed running    task/job is running ignore    ignore task/job (is up-to-date)</p> <p>This filter adds the following context to a log record:</p> <p>task    task_name</p> <p>task_status    task status</p> <p>task_total    number of jobs in task</p> <p>task_completed    number of jobs in task completed</p> <p>task_completed_percent    percentage of task completed</p> <p>The filter will also generate an additional log message in json format with the fields above.</p>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.LoggingFilterProgress--arguments","title":"Arguments","text":"<p>ruffus_text : string     Log messages from ruffus.pipeline_printout. These are used     to collect all tasks that will be executed during pipeline     execution.</p> Source code in <code>cgatcore/pipeline/control.py</code> <pre><code>class LoggingFilterProgress(logging.Filter):\n    \"\"\"add progress information to the log-stream.\n\n    A :term:`task` is a ruffus_ decorated function, which will execute\n    one or more :term:`jobs`.\n\n    Valid task/job status:\n    update\n       task/job needs updating\n    completed\n       task/job completed successfully\n    failed\n       task/job failed\n    running\n       task/job is running\n    ignore\n       ignore task/job (is up-to-date)\n\n    This filter adds the following context to a log record:\n\n    task\n       task_name\n\n    task_status\n       task status\n\n    task_total\n       number of jobs in task\n\n    task_completed\n       number of jobs in task completed\n\n    task_completed_percent\n       percentage of task completed\n\n    The filter will also generate an additional log message in json format\n    with the fields above.\n\n    Arguments\n    ---------\n    ruffus_text : string\n        Log messages from ruffus.pipeline_printout. These are used\n        to collect all tasks that will be executed during pipeline\n        execution.\n\n    \"\"\"\n\n    def __init__(self,\n                 ruffus_text):\n\n        # dictionary of jobs to run\n        self.jobs = {}\n        self.tasks = {}\n        self.map_job2task = {}\n        self.logger = get_logger()\n\n        def split_by_job(text):\n            # ignore optional docstring at beginning (is bracketed by '\"')\n            text = re.sub(r'^\\\"[^\"]+\\\"', \"\", \"\".join(text))\n            for line in re.split(r\"Job\\s+=\", text):\n                if not line.strip():\n                    continue\n                if \"Make missing directories\" in line:\n                    continue\n                try:\n                    # long file names cause additional wrapping and\n                    # additional white-space characters\n                    job_name = re.search(\n                        r\"\\[.*-&gt; ([^\\]]+)\\]\", line).groups()[0]\n                except AttributeError:\n                    continue\n                    # raise AttributeError(\"could not parse '%s'\" % line)\n                job_status = \"ignore\"\n                if \"Job needs update\" in line:\n                    job_status = \"update\"\n\n                yield job_name, job_status\n\n        def split_by_task(text):\n            block, task_name = [], None\n            task_status = None\n            for line in text.splitlines():\n                line = line.strip()\n\n                if line.startswith(\"Tasks which will be run\"):\n                    task_status = \"update\"\n                    block = []\n                    continue\n                elif line.startswith(\"Tasks which are up-to-date\"):\n                    task_status = \"ignore\"\n                    block = []\n                    continue\n\n                if line.startswith(\"Task = \"):\n                    if task_name:\n                        yield task_name, task_status, list(split_by_job(block))\n                    block = []\n                    task_name = re.match(\"Task = (.*)\", line).groups()[0]\n                    continue\n                if line:\n                    block.append(line)\n            if task_name:\n                yield task_name, task_status, list(split_by_job(block))\n\n        # populate with initial messages\n        for task_name, task_status, jobs in split_by_task(ruffus_text):\n            if task_name.startswith(\"(mkdir\"):\n                continue\n\n            to_run = 0\n            for job_name, job_status in jobs:\n                self.jobs[job_name] = (task_name, job_name)\n                if job_status == \"update\":\n                    to_run += 1\n                self.map_job2task[re.sub(r\"\\s\", \"\", job_name)] = task_name\n\n            self.tasks[task_name] = [task_status,\n                                     len(jobs),\n                                     len(jobs) - to_run]\n\n    def filter(self, record):\n\n        if not record.filename.endswith(\"task.py\"):\n            return True\n\n        # update task counts and status\n        job_name, task_name = None, None\n        if re.search(r\"Job\\s+=\", record.msg):\n            try:\n                job_name = re.search(\n                    r\"\\[.*-&gt; ([^\\]]+)\\]\", record.msg).groups()[0]\n            except AttributeError:\n                return True\n            job_name = re.sub(r\"\\s\", \"\", job_name)\n            task_name = self.map_job2task.get(job_name, None)\n            if task_name is None:\n                return\n            if \"completed\" in record.msg:\n                self.tasks[task_name][2] += 1\n\n        elif re.search(r\"Task\\s+=\", record.msg):\n            try:\n                before, task_name = record.msg.strip().split(\" = \")\n            except ValueError:\n                return True\n\n            # ignore the mkdir, etc tasks\n            if task_name not in self.tasks:\n                return True\n\n            if before == \"Task enters queue\":\n                self.tasks[task_name][0] = \"running\"\n            elif before == \"Completed Task\":\n                self.tasks[task_name][0] = \"completed\"\n            elif before == \"Uptodate Task\":\n                self.tasks[task_name][0] = \"uptodate\"\n            else:\n                return True\n        else:\n            return True\n\n        if task_name is None:\n            return\n\n        # update log record\n        task_status, task_total, task_completed = self.tasks[task_name]\n        if task_total &gt; 0:\n            task_completed_percent = 100.0 * task_completed / task_total\n        else:\n            task_completed_percent = 0\n\n        # ignore prefix:: in task_name for output\n        task_name = re.sub(\"^[^:]+::\", \"\", task_name)\n        data = {\n            \"task\": task_name,\n            \"task_status\": task_status,\n            \"task_total\": task_total,\n            \"task_completed\": task_completed,\n            \"task_completed_percent\": task_completed_percent}\n\n        record.task_status = task_status\n        record.task_total = task_total\n        record.task_completed = task_completed\n        record.task_completed_percent = task_completed_percent\n\n        # log status\n        self.logger.info(json.dumps(data))\n\n        return True\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.LoggingFilterpipelineName","title":"<code>LoggingFilterpipelineName</code>","text":"<p>               Bases: <code>Filter</code></p> <p>add pipeline name to log message.</p> <p>With this filter, %(app_name)s can be used in log formats.</p> Source code in <code>cgatcore/pipeline/control.py</code> <pre><code>class LoggingFilterpipelineName(logging.Filter):\n    \"\"\"add pipeline name to log message.\n\n    With this filter, %(app_name)s can be used in log formats.\n    \"\"\"\n\n    def __init__(self, name, *args, **kwargs):\n        logging.Filter.__init__(self, *args, **kwargs)\n        self.app_name = name\n\n    def filter(self, record):\n        record.app_name = self.app_name\n        message = record.getMessage()\n        if message.startswith(\"- {\"):\n            json_message = json.loads(message[2:])\n        elif message.startswith(\"{\"):\n            json_message = json.loads(message)\n        else:\n            json_message = None\n        if json_message:\n            for k, v in list(json_message.items()):\n                setattr(record, k, v)\n\n        return True\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.MultiLineFormatter","title":"<code>MultiLineFormatter</code>","text":"<p>               Bases: <code>Formatter</code></p> <p>logfile formatter: add identation for multi-line entries.</p> Source code in <code>cgatcore/experiment.py</code> <pre><code>class MultiLineFormatter(logging.Formatter):\n    '''logfile formatter: add identation for multi-line entries.'''\n\n    def format(self, record):\n\n        s = logging.Formatter.format(self, record)\n        if s.startswith(\"#\"):\n            prefix = \"#\"\n        else:\n            prefix = \"\"\n        if record.message:\n            header, footer = s.split(record.message)\n            s = s.replace(\"\\n\", \" \\\\\\n%s\" % prefix + \" \" * (len(header) - 1))\n        return s\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.SGEExecutor","title":"<code>SGEExecutor</code>","text":"<p>               Bases: <code>BaseExecutor</code></p> Source code in <code>cgatcore/pipeline/executors.py</code> <pre><code>class SGEExecutor(BaseExecutor):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.logger = logging.getLogger(__name__)\n        self.task_name = \"sge_task\"\n        self.default_total_time = 8\n\n    def run(self, statement_list):\n        benchmark_data = []\n        for statement in statement_list:\n            self.logger.info(f\"Running statement on SGE: {statement}\")\n\n            full_statement, job_path = self.build_job_script(statement)\n\n            # Build the SGE job submission command\n            sge_command = f\"qsub -N {self.config.get('job_name', 'default_job')} -cwd -o {job_path}.o -e {job_path}.e {job_path}\"\n\n            process = subprocess.run(sge_command, shell=True, capture_output=True, text=True)\n\n            if process.returncode != 0:\n                self.logger.error(f\"SGE job submission failed: {process.stderr}\")\n                raise RuntimeError(f\"SGE job submission failed: {process.stderr}\")\n\n            self.logger.info(f\"SGE job submitted: {process.stdout.strip()}\")\n\n            # Monitor job completion\n            self.monitor_job_completion(process.stdout.strip())\n\n            benchmark_data.append(self.collect_benchmark_data([statement], resource_usage=[]))\n\n        return benchmark_data\n\n    def build_job_script(self, statement):\n        \"\"\"Custom build job script for SGE.\"\"\"\n        return super().build_job_script(statement)\n\n    def monitor_job_completion(self, job_id):\n        \"\"\"Monitor the completion of an SGE job.\n\n        Args:\n            job_id (str): The SGE job ID to monitor.\n\n        Raises:\n            RuntimeError: If the job fails or times out.\n        \"\"\"\n        while True:\n            # Use qstat to get job status\n            cmd = f\"qstat -j {job_id}\"\n            process = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n\n            if process.returncode != 0:\n                # Job not found in qstat could mean it's completed\n                # Use qacct to get final status\n                cmd = f\"qacct -j {job_id}\"\n                process = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n\n                if \"exit_status\" in process.stdout:\n                    exit_status = process.stdout.split(\"exit_status\")[1].split()[0]\n                    if exit_status == \"0\":\n                        self.logger.info(f\"Job {job_id} completed successfully\")\n                        break\n                    else:\n                        self.logger.error(f\"Job {job_id} failed with exit status: {exit_status}\")\n                        raise RuntimeError(f\"Job {job_id} failed with exit status: {exit_status}\")\n\n                self.logger.error(f\"Failed to get job status: {process.stderr}\")\n                raise RuntimeError(f\"Failed to get job status: {process.stderr}\")\n\n            # Wait before checking again\n            time.sleep(10)\n\n    def collect_benchmark_data(self, statements, resource_usage=None):\n        \"\"\"Collect benchmark data for SGE jobs.\n\n        Args:\n            statements (list): List of executed statements\n            resource_usage (list, optional): Resource usage data\n\n        Returns:\n            dict: Benchmark data including task name and execution time\n        \"\"\"\n        return {\n            \"task\": self.task_name,\n            \"total_t\": self.default_total_time,\n            \"statements\": statements,\n            \"resource_usage\": resource_usage or []\n        }\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.SGEExecutor.build_job_script","title":"<code>build_job_script(statement)</code>","text":"<p>Custom build job script for SGE.</p> Source code in <code>cgatcore/pipeline/executors.py</code> <pre><code>def build_job_script(self, statement):\n    \"\"\"Custom build job script for SGE.\"\"\"\n    return super().build_job_script(statement)\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.SGEExecutor.collect_benchmark_data","title":"<code>collect_benchmark_data(statements, resource_usage=None)</code>","text":"<p>Collect benchmark data for SGE jobs.</p> <p>Parameters:</p> Name Type Description Default <code>statements</code> <code>list</code> <p>List of executed statements</p> required <code>resource_usage</code> <code>list</code> <p>Resource usage data</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>Benchmark data including task name and execution time</p> Source code in <code>cgatcore/pipeline/executors.py</code> <pre><code>def collect_benchmark_data(self, statements, resource_usage=None):\n    \"\"\"Collect benchmark data for SGE jobs.\n\n    Args:\n        statements (list): List of executed statements\n        resource_usage (list, optional): Resource usage data\n\n    Returns:\n        dict: Benchmark data including task name and execution time\n    \"\"\"\n    return {\n        \"task\": self.task_name,\n        \"total_t\": self.default_total_time,\n        \"statements\": statements,\n        \"resource_usage\": resource_usage or []\n    }\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.SGEExecutor.monitor_job_completion","title":"<code>monitor_job_completion(job_id)</code>","text":"<p>Monitor the completion of an SGE job.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>str</code> <p>The SGE job ID to monitor.</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the job fails or times out.</p> Source code in <code>cgatcore/pipeline/executors.py</code> <pre><code>def monitor_job_completion(self, job_id):\n    \"\"\"Monitor the completion of an SGE job.\n\n    Args:\n        job_id (str): The SGE job ID to monitor.\n\n    Raises:\n        RuntimeError: If the job fails or times out.\n    \"\"\"\n    while True:\n        # Use qstat to get job status\n        cmd = f\"qstat -j {job_id}\"\n        process = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n\n        if process.returncode != 0:\n            # Job not found in qstat could mean it's completed\n            # Use qacct to get final status\n            cmd = f\"qacct -j {job_id}\"\n            process = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n\n            if \"exit_status\" in process.stdout:\n                exit_status = process.stdout.split(\"exit_status\")[1].split()[0]\n                if exit_status == \"0\":\n                    self.logger.info(f\"Job {job_id} completed successfully\")\n                    break\n                else:\n                    self.logger.error(f\"Job {job_id} failed with exit status: {exit_status}\")\n                    raise RuntimeError(f\"Job {job_id} failed with exit status: {exit_status}\")\n\n            self.logger.error(f\"Failed to get job status: {process.stderr}\")\n            raise RuntimeError(f\"Failed to get job status: {process.stderr}\")\n\n        # Wait before checking again\n        time.sleep(10)\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.SlurmExecutor","title":"<code>SlurmExecutor</code>","text":"<p>               Bases: <code>BaseExecutor</code></p> <p>Executor for running jobs on Slurm cluster.</p> Source code in <code>cgatcore/pipeline/executors.py</code> <pre><code>class SlurmExecutor(BaseExecutor):\n    \"\"\"Executor for running jobs on Slurm cluster.\"\"\"\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.logger = logging.getLogger(__name__)\n        self.task_name = \"slurm_task\"\n        self.default_total_time = 10\n\n    def run(self, statement_list):\n        benchmark_data = []\n        for statement in statement_list:\n            self.logger.info(f\"Running statement on Slurm: {statement}\")\n\n            full_statement, job_path = self.build_job_script(statement)\n\n            # Build the Slurm job submission command\n            slurm_command = f\"sbatch --job-name={self.config.get('job_name', 'default_job')} --output={job_path}.o --error={job_path}.e {job_path}\"\n\n            process = subprocess.run(slurm_command, shell=True, capture_output=True, text=True)\n\n            if process.returncode != 0:\n                self.logger.error(f\"Slurm job submission failed: {process.stderr}\")\n                raise RuntimeError(f\"Slurm job submission failed: {process.stderr}\")\n\n            job_id = process.stdout.strip()\n            self.logger.info(f\"Slurm job submitted with ID: {job_id}\")\n\n            # Monitor job completion\n            self.monitor_job_completion(job_id)\n\n            benchmark_data.append(self.collect_benchmark_data([statement], resource_usage=[]))\n\n        return benchmark_data\n\n    def build_job_script(self, statement):\n        \"\"\"Custom build job script for Slurm.\"\"\"\n        return super().build_job_script(statement)\n\n    def monitor_job_completion(self, job_id):\n        \"\"\"Monitor the completion of a Slurm job.\n\n        Args:\n            job_id (str): The Slurm job ID to monitor.\n\n        Raises:\n            RuntimeError: If the job fails or times out.\n        \"\"\"\n        while True:\n            # Use sacct to get job status\n            cmd = f\"sacct -j {job_id} --format=State --noheader --parsable2\"\n            process = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n\n            if process.returncode != 0:\n                self.logger.error(f\"Failed to get job status: {process.stderr}\")\n                raise RuntimeError(f\"Failed to get job status: {process.stderr}\")\n\n            status = process.stdout.strip()\n\n            # Check job status\n            if status in [\"COMPLETED\", \"COMPLETED+\"]:\n                self.logger.info(f\"Job {job_id} completed successfully\")\n                break\n            elif status in [\"FAILED\", \"TIMEOUT\", \"CANCELLED\", \"NODE_FAIL\"]:\n                self.logger.error(f\"Job {job_id} failed with status: {status}\")\n                raise RuntimeError(f\"Job {job_id} failed with status: {status}\")\n\n            # Wait before checking again\n            time.sleep(10)\n\n    def collect_benchmark_data(self, statements, resource_usage=None):\n        \"\"\"Collect benchmark data for Slurm jobs.\n\n        Args:\n            statements (list): List of executed statements\n            resource_usage (list, optional): Resource usage data\n\n        Returns:\n            dict: Benchmark data including task name and execution time\n        \"\"\"\n        return {\n            \"task\": self.task_name,\n            \"total_t\": self.default_total_time,\n            \"statements\": statements,\n            \"resource_usage\": resource_usage or []\n        }\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.SlurmExecutor.build_job_script","title":"<code>build_job_script(statement)</code>","text":"<p>Custom build job script for Slurm.</p> Source code in <code>cgatcore/pipeline/executors.py</code> <pre><code>def build_job_script(self, statement):\n    \"\"\"Custom build job script for Slurm.\"\"\"\n    return super().build_job_script(statement)\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.SlurmExecutor.collect_benchmark_data","title":"<code>collect_benchmark_data(statements, resource_usage=None)</code>","text":"<p>Collect benchmark data for Slurm jobs.</p> <p>Parameters:</p> Name Type Description Default <code>statements</code> <code>list</code> <p>List of executed statements</p> required <code>resource_usage</code> <code>list</code> <p>Resource usage data</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>Benchmark data including task name and execution time</p> Source code in <code>cgatcore/pipeline/executors.py</code> <pre><code>def collect_benchmark_data(self, statements, resource_usage=None):\n    \"\"\"Collect benchmark data for Slurm jobs.\n\n    Args:\n        statements (list): List of executed statements\n        resource_usage (list, optional): Resource usage data\n\n    Returns:\n        dict: Benchmark data including task name and execution time\n    \"\"\"\n    return {\n        \"task\": self.task_name,\n        \"total_t\": self.default_total_time,\n        \"statements\": statements,\n        \"resource_usage\": resource_usage or []\n    }\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.SlurmExecutor.monitor_job_completion","title":"<code>monitor_job_completion(job_id)</code>","text":"<p>Monitor the completion of a Slurm job.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>str</code> <p>The Slurm job ID to monitor.</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the job fails or times out.</p> Source code in <code>cgatcore/pipeline/executors.py</code> <pre><code>def monitor_job_completion(self, job_id):\n    \"\"\"Monitor the completion of a Slurm job.\n\n    Args:\n        job_id (str): The Slurm job ID to monitor.\n\n    Raises:\n        RuntimeError: If the job fails or times out.\n    \"\"\"\n    while True:\n        # Use sacct to get job status\n        cmd = f\"sacct -j {job_id} --format=State --noheader --parsable2\"\n        process = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n\n        if process.returncode != 0:\n            self.logger.error(f\"Failed to get job status: {process.stderr}\")\n            raise RuntimeError(f\"Failed to get job status: {process.stderr}\")\n\n        status = process.stdout.strip()\n\n        # Check job status\n        if status in [\"COMPLETED\", \"COMPLETED+\"]:\n            self.logger.info(f\"Job {job_id} completed successfully\")\n            break\n        elif status in [\"FAILED\", \"TIMEOUT\", \"CANCELLED\", \"NODE_FAIL\"]:\n            self.logger.error(f\"Job {job_id} failed with status: {status}\")\n            raise RuntimeError(f\"Job {job_id} failed with status: {status}\")\n\n        # Wait before checking again\n        time.sleep(10)\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.TorqueExecutor","title":"<code>TorqueExecutor</code>","text":"<p>               Bases: <code>BaseExecutor</code></p> <p>Executor for running jobs on Torque cluster.</p> Source code in <code>cgatcore/pipeline/executors.py</code> <pre><code>class TorqueExecutor(BaseExecutor):\n    \"\"\"Executor for running jobs on Torque cluster.\"\"\"\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.logger = logging.getLogger(__name__)\n        self.task_name = \"torque_task\"\n        self.default_total_time = 7\n\n    def run(self, statement_list):\n        benchmark_data = []\n        for statement in statement_list:\n            self.logger.info(f\"Running statement on Torque: {statement}\")\n\n            full_statement, job_path = self.build_job_script(statement)\n\n            # Build the Torque job submission command\n            torque_command = f\"qsub -N {self.config.get('job_name', 'default_job')} -o {job_path}.o -e {job_path}.e {job_path}\"\n\n            process = subprocess.run(torque_command, shell=True, capture_output=True, text=True)\n\n            if process.returncode != 0:\n                self.logger.error(f\"Torque job submission failed: {process.stderr}\")\n                raise RuntimeError(f\"Torque job submission failed: {process.stderr}\")\n\n            job_id = process.stdout.strip()\n            self.logger.info(f\"Torque job submitted with ID: {job_id}\")\n\n            # Monitor job completion\n            self.monitor_job_completion(job_id)\n\n            benchmark_data.append(self.collect_benchmark_data([statement], resource_usage=[]))\n\n        return benchmark_data\n\n    def build_job_script(self, statement):\n        \"\"\"Custom build job script for Torque.\"\"\"\n        return super().build_job_script(statement)\n\n    def monitor_job_completion(self, job_id):\n        \"\"\"Monitor the completion of a Torque job.\n\n        Args:\n            job_id (str): The Torque job ID to monitor.\n\n        Raises:\n            RuntimeError: If the job fails or times out.\n        \"\"\"\n        while True:\n            # Use qstat to get job status\n            cmd = f\"qstat -f {job_id}\"\n            process = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n\n            if process.returncode != 0:\n                # Job not found in qstat could mean it's completed\n                # Use tracejob to get final status\n                cmd = f\"tracejob {job_id}\"\n                process = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n\n                if \"Exit_status=\" in process.stdout:\n                    if \"Exit_status=0\" in process.stdout:\n                        self.logger.info(f\"Job {job_id} completed successfully\")\n                        break\n                    else:\n                        status = process.stdout.split(\"Exit_status=\")[1].split()[0]\n                        self.logger.error(f\"Job {job_id} failed with exit status: {status}\")\n                        raise RuntimeError(f\"Job {job_id} failed with exit status: {status}\")\n\n                self.logger.error(f\"Failed to get job status: {process.stderr}\")\n                raise RuntimeError(f\"Failed to get job status: {process.stderr}\")\n\n            # Wait before checking again\n            time.sleep(10)\n\n    def collect_benchmark_data(self, statements, resource_usage=None):\n        \"\"\"Collect benchmark data for Torque jobs.\n\n        Args:\n            statements (list): List of executed statements\n            resource_usage (list, optional): Resource usage data\n\n        Returns:\n            dict: Benchmark data including task name and execution time\n        \"\"\"\n        return {\n            \"task\": self.task_name,\n            \"total_t\": self.default_total_time,\n            \"statements\": statements,\n            \"resource_usage\": resource_usage or []\n        }\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.TorqueExecutor.build_job_script","title":"<code>build_job_script(statement)</code>","text":"<p>Custom build job script for Torque.</p> Source code in <code>cgatcore/pipeline/executors.py</code> <pre><code>def build_job_script(self, statement):\n    \"\"\"Custom build job script for Torque.\"\"\"\n    return super().build_job_script(statement)\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.TorqueExecutor.collect_benchmark_data","title":"<code>collect_benchmark_data(statements, resource_usage=None)</code>","text":"<p>Collect benchmark data for Torque jobs.</p> <p>Parameters:</p> Name Type Description Default <code>statements</code> <code>list</code> <p>List of executed statements</p> required <code>resource_usage</code> <code>list</code> <p>Resource usage data</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>Benchmark data including task name and execution time</p> Source code in <code>cgatcore/pipeline/executors.py</code> <pre><code>def collect_benchmark_data(self, statements, resource_usage=None):\n    \"\"\"Collect benchmark data for Torque jobs.\n\n    Args:\n        statements (list): List of executed statements\n        resource_usage (list, optional): Resource usage data\n\n    Returns:\n        dict: Benchmark data including task name and execution time\n    \"\"\"\n    return {\n        \"task\": self.task_name,\n        \"total_t\": self.default_total_time,\n        \"statements\": statements,\n        \"resource_usage\": resource_usage or []\n    }\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.TorqueExecutor.monitor_job_completion","title":"<code>monitor_job_completion(job_id)</code>","text":"<p>Monitor the completion of a Torque job.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>str</code> <p>The Torque job ID to monitor.</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the job fails or times out.</p> Source code in <code>cgatcore/pipeline/executors.py</code> <pre><code>def monitor_job_completion(self, job_id):\n    \"\"\"Monitor the completion of a Torque job.\n\n    Args:\n        job_id (str): The Torque job ID to monitor.\n\n    Raises:\n        RuntimeError: If the job fails or times out.\n    \"\"\"\n    while True:\n        # Use qstat to get job status\n        cmd = f\"qstat -f {job_id}\"\n        process = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n\n        if process.returncode != 0:\n            # Job not found in qstat could mean it's completed\n            # Use tracejob to get final status\n            cmd = f\"tracejob {job_id}\"\n            process = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n\n            if \"Exit_status=\" in process.stdout:\n                if \"Exit_status=0\" in process.stdout:\n                    self.logger.info(f\"Job {job_id} completed successfully\")\n                    break\n                else:\n                    status = process.stdout.split(\"Exit_status=\")[1].split()[0]\n                    self.logger.error(f\"Job {job_id} failed with exit status: {status}\")\n                    raise RuntimeError(f\"Job {job_id} failed with exit status: {status}\")\n\n            self.logger.error(f\"Failed to get job status: {process.stderr}\")\n            raise RuntimeError(f\"Failed to get job status: {process.stderr}\")\n\n        # Wait before checking again\n        time.sleep(10)\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.add_doc","title":"<code>add_doc(value, replace=False)</code>","text":"<p>add doc string of value to function that is decorated.</p> <p>The original doc-string is added as the first paragraph(s) inside the new doc-string.</p>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.add_doc--parameter","title":"Parameter","text":"bool <p>If True, replace documentation rather than appending</p> Source code in <code>cgatcore/pipeline/utils.py</code> <pre><code>def add_doc(value, replace=False):\n    \"\"\"add doc string of value to function that is decorated.\n\n    The original doc-string is added as the first paragraph(s)\n    inside the new doc-string.\n\n    Parameter\n    ---------\n\n    replace : bool\n       If True, replace documentation rather than appending\n    \"\"\"\n    def _doc(func):\n        if func.__doc__:\n            lines = value.__doc__.split(\"\\n\")\n            for x, line in enumerate(lines):\n                if line.strip() == \"\":\n                    break\n            # insert appropriate indentiation\n            # currently hard-coded, can be derived\n            # from doc string?\n            if not replace:\n                lines.insert(x + 1, \" \" * 4 + func.__doc__)\n                func.__doc__ = \"\\n\".join(lines)\n            else:\n                func.__doc__ = value.__doc__\n        else:\n            func.__doc__ = value.__doc__\n        return func\n    return _doc\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.as_list","title":"<code>as_list(value)</code>","text":"<p>return a value as a list.</p> <p>If the value is a string and contains a <code>,</code>, the string will be split at <code>,</code>.</p>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.as_list--returns","title":"Returns","text":"<p>list</p> Source code in <code>cgatcore/pipeline/parameters.py</code> <pre><code>def as_list(value):\n    '''return a value as a list.\n\n    If the value is a string and contains a ``,``, the string will\n    be split at ``,``.\n\n    Returns\n    -------\n    list\n\n    '''\n    if isinstance(value, str):\n        try:\n            values = [x.strip() for x in value.strip().split(\",\")]\n        except AttributeError:\n            values = [value.strip()]\n        return [x for x in values if x != \"\"]\n    elif type(value) in (list, tuple):\n        return value\n    else:\n        return [value]\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.build_load_statement","title":"<code>build_load_statement(tablename, retry=True, options='')</code>","text":"<p>build a command line statement to upload data.</p> <p>Upload is performed via the :doc:<code>csv2db</code> script.</p> <p>The returned statement is suitable to use in pipe expression. This method is aware of the configuration values for database access and the chosen database backend.</p> <p>For example::</p> <pre><code>load_statement = P.build_load_statement(\"data\")\nstatement = \"cat data.txt | %(load_statement)s\"\nP.run(statement)\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.build_load_statement--arguments","title":"Arguments","text":"<p>tablename : string     Tablename for upload retry : bool     Add the <code>--retry</code> option to <code>csv2db.py</code> options : string     Command line options to be passed on to <code>csv2db.py</code></p>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.build_load_statement--returns","title":"Returns","text":"<p>string</p> Source code in <code>cgatcore/pipeline/database.py</code> <pre><code>def build_load_statement(tablename, retry=True, options=\"\"):\n    \"\"\"build a command line statement to upload data.\n\n    Upload is performed via the :doc:`csv2db` script.\n\n    The returned statement is suitable to use in pipe expression.\n    This method is aware of the configuration values for database\n    access and the chosen database backend.\n\n    For example::\n\n        load_statement = P.build_load_statement(\"data\")\n        statement = \"cat data.txt | %(load_statement)s\"\n        P.run(statement)\n\n    Arguments\n    ---------\n    tablename : string\n        Tablename for upload\n    retry : bool\n        Add the ``--retry`` option to `csv2db.py`\n    options : string\n        Command line options to be passed on to `csv2db.py`\n\n    Returns\n    -------\n    string\n\n    \"\"\"\n\n    opts = []\n\n    if retry:\n        opts.append(\" --retry \")\n\n    params = get_params()\n    opts.append(\"--database-url={}\".format(params[\"database\"][\"url\"]))\n\n    db_options = \" \".join(opts)\n    load_statement = (\n        \"python -m cgatcore.csv2db {db_options} {options} --table={tablename}\".format(**locals()))\n\n    return load_statement\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.check_executables","title":"<code>check_executables(filenames)</code>","text":"<p>check for the presence/absence of executables</p> Source code in <code>cgatcore/pipeline/files.py</code> <pre><code>def check_executables(filenames):\n    \"\"\"check for the presence/absence of executables\"\"\"\n\n    missing = []\n\n    for filename in filenames:\n        if not iotools.which(filename):\n            missing.append(filename)\n\n    if missing:\n        raise ValueError(\"missing executables: %s\" % \",\".join(missing))\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.check_parameter","title":"<code>check_parameter(param)</code>","text":"<p>check if parameter <code>key</code> is set</p> Source code in <code>cgatcore/pipeline/parameters.py</code> <pre><code>def check_parameter(param):\n    \"\"\"check if parameter ``key`` is set\"\"\"\n    if param not in PARAMS:\n        raise ValueError(\"need `%s` to be set\" % param)\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.check_scripts","title":"<code>check_scripts(filenames)</code>","text":"<p>check for the presence/absence of scripts</p> Source code in <code>cgatcore/pipeline/files.py</code> <pre><code>def check_scripts(filenames):\n    \"\"\"check for the presence/absence of scripts\"\"\"\n    missing = []\n    for filename in filenames:\n        if not os.path.exists(filename):\n            missing.append(filename)\n\n    if missing:\n        raise ValueError(\"missing scripts: %s\" % \",\".join(missing))\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.clean","title":"<code>clean(files, logfile)</code>","text":"<p>clean up files given by glob expressions.</p> <p>Files are cleaned up by zapping, i.e. the files are set to size 0. Links to files are replaced with place-holders.</p> <p>Information about the original file is written to <code>logfile</code>.</p>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.clean--arguments","title":"Arguments","text":"<p>files : list     List of glob expressions of files to clean up. logfile : string     Filename of logfile.</p> Source code in <code>cgatcore/pipeline/control.py</code> <pre><code>def clean(files, logfile):\n    '''clean up files given by glob expressions.\n\n    Files are cleaned up by zapping, i.e. the files are set to size\n    0. Links to files are replaced with place-holders.\n\n    Information about the original file is written to `logfile`.\n\n    Arguments\n    ---------\n    files : list\n        List of glob expressions of files to clean up.\n    logfile : string\n        Filename of logfile.\n\n    '''\n    fields = ('st_atime', 'st_blksize', 'st_blocks',\n              'st_ctime', 'st_dev', 'st_gid', 'st_ino',\n              'st_mode', 'st_mtime', 'st_nlink',\n              'st_rdev', 'st_size', 'st_uid')\n\n    dry_run = get_params().get(\"dryrun\", False)\n\n    if not dry_run:\n        if not os.path.exists(logfile):\n            outfile = iotools.open_file(logfile, \"w\")\n            outfile.write(\"filename\\tzapped\\tlinkdest\\t%s\\n\" %\n                          \"\\t\".join(fields))\n        else:\n            outfile = iotools.open_file(logfile, \"a\")\n\n    c = E.Counter()\n    for fn in files:\n        c.files += 1\n        if not dry_run:\n            stat, linkdest = iotools.zap_file(fn)\n            if stat is not None:\n                c.zapped += 1\n                if linkdest is not None:\n                    c.links += 1\n                outfile.write(\"%s\\t%s\\t%s\\t%s\\n\" % (\n                    fn,\n                    time.asctime(time.localtime(time.time())),\n                    linkdest,\n                    \"\\t\".join([str(getattr(stat, x)) for x in fields])))\n\n    get_logger().info(\"zapped: %s\" % (c))\n    outfile.close()\n\n    return c\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.clone_pipeline","title":"<code>clone_pipeline(srcdir, destdir=None)</code>","text":"<p>clone a pipeline.</p> <p>Cloning entails creating a mirror of the source pipeline. Generally, data files are mirrored by linking. Configuration files and the pipeline database will be copied.</p> <p>Without modification of any files, building the cloned pipeline in <code>destdir</code> should not re-run any commands. However, on deleting selected files, the pipeline should run from the appropriate point.  Newly created files will not affect the original pipeline.</p> <p>Cloning pipelines permits sharing partial results between pipelines, for example for parameter optimization.</p>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.clone_pipeline--arguments","title":"Arguments","text":"<p>scrdir : string     Source directory destdir : string     Destination directory. If None, use the current directory.</p> Source code in <code>cgatcore/pipeline/control.py</code> <pre><code>def clone_pipeline(srcdir, destdir=None):\n    '''clone a pipeline.\n\n    Cloning entails creating a mirror of the source pipeline.\n    Generally, data files are mirrored by linking. Configuration\n    files and the pipeline database will be copied.\n\n    Without modification of any files, building the cloned pipeline in\n    `destdir` should not re-run any commands. However, on deleting\n    selected files, the pipeline should run from the appropriate\n    point.  Newly created files will not affect the original pipeline.\n\n    Cloning pipelines permits sharing partial results between\n    pipelines, for example for parameter optimization.\n\n    Arguments\n    ---------\n    scrdir : string\n        Source directory\n    destdir : string\n        Destination directory. If None, use the current directory.\n\n    '''\n\n    if destdir is None:\n        destdir = os.path.curdir\n\n    get_logger().info(\"cloning pipeline from %s to %s\" % (srcdir, destdir))\n\n    copy_files = (\"conf.py\", \"pipeline.yml\", \"benchmark.yml\", \"csvdb\")\n    ignore_prefix = (\n        \"report\", \"_cache\", \"export\", \"tmp\", \"ctmp\",\n        \"_static\", \"_templates\", \"shell.log\", \"pipeline.log\",\n        \"results.commit\")\n\n    def _ignore(p):\n        for x in ignore_prefix:\n            if p.startswith(x):\n                return True\n        return False\n\n    for root, dirs, files in os.walk(srcdir):\n\n        relpath = os.path.relpath(root, srcdir)\n        if _ignore(relpath):\n            continue\n\n        for d in dirs:\n            if _ignore(d):\n                continue\n            dest = os.path.join(os.path.join(destdir, relpath, d))\n            os.mkdir(dest)\n            # touch\n            s = os.stat(os.path.join(root, d))\n            os.utime(dest, (s.st_atime, s.st_mtime))\n\n        for f in files:\n            if _ignore(f):\n                continue\n\n            fn = os.path.join(root, f)\n            dest_fn = os.path.join(destdir, relpath, f)\n            if f in copy_files:\n                shutil.copyfile(fn, dest_fn)\n            else:\n                # realpath resolves links - thus links will be linked to\n                # the original target\n                os.symlink(os.path.realpath(fn),\n                           dest_fn)\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.close_session","title":"<code>close_session()</code>","text":"<p>close the global DRMAA session.</p> Source code in <code>cgatcore/pipeline/execution.py</code> <pre><code>def close_session():\n    \"\"\"close the global DRMAA session.\"\"\"\n    global GLOBAL_SESSION\n\n    if GLOBAL_SESSION is not None:\n        GLOBAL_SESSION.exit()\n        GLOBAL_SESSION = None\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.cluster_runnable","title":"<code>cluster_runnable(func)</code>","text":"<p>A dectorator that allows a function to be run on the cluster.</p> <p>The decorated function now takes extra arguments. The most important is submit. If set to true, it will submit the function to the cluster via the pipeline.submit framework. Arguments to the function are pickled, so this will only work if arguments are picklable. Other arguments to submit are also accepted.</p> <p>Note that this allows the unusal combination of submit false, and to_cluster true. This will submit the function as an external job, but run it on the local machine.</p> <p>Note: all arguments in the decorated function must be passed as key-word arguments.</p> Source code in <code>cgatcore/pipeline/execution.py</code> <pre><code>def cluster_runnable(func):\n    '''A dectorator that allows a function to be run on the cluster.\n\n    The decorated function now takes extra arguments. The most important\n    is *submit*. If set to true, it will submit the function to the cluster\n    via the pipeline.submit framework. Arguments to the function are\n    pickled, so this will only work if arguments are picklable. Other\n    arguments to submit are also accepted.\n\n    Note that this allows the unusal combination of *submit* false,\n    and *to_cluster* true. This will submit the function as an external\n    job, but run it on the local machine.\n\n    Note: all arguments in the decorated function must be passed as\n    key-word arguments.\n    '''\n\n    # MM: when decorating functions with cluster_runnable, provide\n    # them as kwargs, else will throw attribute error\n\n    function_name = func.__name__\n\n    def submit_function(*args, **kwargs):\n\n        if \"submit\" in kwargs and kwargs[\"submit\"]:\n            del kwargs[\"submit\"]\n            submit_args, args_file = _pickle_args(args, kwargs)\n            module_file = os.path.abspath(\n                sys.modules[func.__module__].__file__)\n            submit(iotools.snip(__file__),\n                   \"run_pickled\",\n                   args=[iotools.snip(module_file), function_name, args_file],\n                   **submit_args)\n        else:\n            # remove job contral options before running function\n            for x in (\"submit\", \"job_options\", \"job_queue\"):\n                if x in kwargs:\n                    del kwargs[x]\n            return func(*args, **kwargs)\n\n    return submit_function\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.concatenate_and_load","title":"<code>concatenate_and_load(infiles, outfile, regex_filename=None, header=None, cat='track', has_titles=True, missing_value='na', retry=True, tablename=None, options='', job_memory=None, to_cluster=True)</code>","text":"<p>concatenate multiple tab-separated files and upload into database.</p> <p>The table name is given by outfile without the \".load\" suffix.</p> <p>A typical concatenate and load task in ruffus would look like this::</p> <pre><code>@merge(\"*.tsv.gz\", \".load\")\ndef loadData(infile, outfile):\n    P.concatenateAndLoad(infiles, outfile)\n</code></pre> <p>Upload is performed via the :doc:<code>csv2db</code> script.</p>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.concatenate_and_load--arguments","title":"Arguments","text":"<p>infiles : list     Filenames of the input data outfile : string     Output filename. This will contain the logging information. The     table name is derived from <code>outfile</code>. regex_filename : string     If given, regex_filename is applied to the filename to extract     the track name. If the pattern contains multiple groups, they are     added as additional columns. For example, if <code>cat</code> is set to     <code>track,method</code> and <code>regex_filename</code> is <code>(.*)_(.*).tsv.gz</code>     it will add the columns <code>track</code> and method to the table. header : string     Comma-separated list of values for header. cat : string     Column title for column containing the track name. The track name     is derived from the filename, see <code>regex_filename</code>. has_titles : bool     If True, files are expected to have column titles in their first row. missing_value : string     String to use for missing values. retry : bool     If True, multiple attempts will be made if the data can     not be loaded at the first try, for example if a table is locked. tablename: string     Name to use for table. If unset derive from outfile. options : string     Command line options for the <code>csv2db.py</code> script. job_memory : string     Amount of memory to allocate for job. If unset, uses the global     default. Implies to_cluster=True. to_cluster : bool     By default load jobs are not submitted to the cluster as they sometimes     become blocked. Setting this true will override this behavoir.</p> Source code in <code>cgatcore/pipeline/database.py</code> <pre><code>def concatenate_and_load(infiles,\n                         outfile,\n                         regex_filename=None,\n                         header=None,\n                         cat=\"track\",\n                         has_titles=True,\n                         missing_value=\"na\",\n                         retry=True,\n                         tablename=None,\n                         options=\"\",\n                         job_memory=None,\n                         to_cluster=True):\n    \"\"\"concatenate multiple tab-separated files and upload into database.\n\n    The table name is given by outfile without the\n    \".load\" suffix.\n\n    A typical concatenate and load task in ruffus would look like this::\n\n        @merge(\"*.tsv.gz\", \".load\")\n        def loadData(infile, outfile):\n            P.concatenateAndLoad(infiles, outfile)\n\n    Upload is performed via the :doc:`csv2db` script.\n\n    Arguments\n    ---------\n    infiles : list\n        Filenames of the input data\n    outfile : string\n        Output filename. This will contain the logging information. The\n        table name is derived from `outfile`.\n    regex_filename : string\n        If given, *regex_filename* is applied to the filename to extract\n        the track name. If the pattern contains multiple groups, they are\n        added as additional columns. For example, if `cat` is set to\n        ``track,method`` and `regex_filename` is ``(.*)_(.*).tsv.gz``\n        it will add the columns ``track`` and method to the table.\n    header : string\n        Comma-separated list of values for header.\n    cat : string\n        Column title for column containing the track name. The track name\n        is derived from the filename, see `regex_filename`.\n    has_titles : bool\n        If True, files are expected to have column titles in their first row.\n    missing_value : string\n        String to use for missing values.\n    retry : bool\n        If True, multiple attempts will be made if the data can\n        not be loaded at the first try, for example if a table is locked.\n    tablename: string\n        Name to use for table. If unset derive from outfile.\n    options : string\n        Command line options for the `csv2db.py` script.\n    job_memory : string\n        Amount of memory to allocate for job. If unset, uses the global\n        default. Implies to_cluster=True.\n    to_cluster : bool\n        By default load jobs are not submitted to the cluster as they sometimes\n        become blocked. Setting this true will override this behavoir.\n    \"\"\"\n    if job_memory is None:\n        job_memory = get_params()[\"cluster_memory_default\"]\n\n    if tablename is None:\n        tablename = to_table(outfile)\n\n    infiles = \" \".join(infiles)\n\n    passed_options = options\n    load_options, cat_options = [\"--add-index=track\"], []\n\n    if regex_filename:\n        cat_options.append(\"--regex-filename='%s'\" % regex_filename)\n\n    if header:\n        load_options.append(\"--header-names=%s\" % header)\n\n    if not has_titles:\n        cat_options.append(\"--no-titles\")\n\n    cat_options = \" \".join(cat_options)\n    load_options = \" \".join(load_options) + \" \" + passed_options\n\n    load_statement = build_load_statement(tablename,\n                                          options=load_options,\n                                          retry=retry)\n\n    statement = '''python -m cgatcore.tables\n    --cat=%(cat)s\n    --missing-value=%(missing_value)s\n    %(cat_options)s\n    %(infiles)s\n    | %(load_statement)s\n    &gt; %(outfile)s'''\n\n    run(statement)\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.config_to_dictionary","title":"<code>config_to_dictionary(config)</code>","text":"<p>convert the contents of a :py:class:<code>ConfigParser.ConfigParser</code> object to a dictionary</p> <p>This method works by iterating over all configuration values in a :py:class:<code>ConfigParser.ConfigParser</code> object and inserting values into a dictionary. Section names are prefixed using and underscore. Thus::</p> <pre><code>[sample]\nname=12\n</code></pre> <p>is entered as <code>sample_name=12</code> into the dictionary. The sections <code>general</code> and <code>DEFAULT</code> are treated specially in that both the prefixed and the unprefixed values are inserted: ::</p> <p>[general]    genome=hg19</p> <p>will be added as <code>general_genome=hg19</code> and <code>genome=hg19</code>.</p> <p>Numbers will be automatically recognized as such and converted into integers or floats.</p>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.config_to_dictionary--returns","title":"Returns","text":"<p>config : dict     A dictionary of configuration values</p> Source code in <code>cgatcore/pipeline/parameters.py</code> <pre><code>def config_to_dictionary(config):\n    \"\"\"convert the contents of a :py:class:`ConfigParser.ConfigParser`\n    object to a dictionary\n\n    This method works by iterating over all configuration values in a\n    :py:class:`ConfigParser.ConfigParser` object and inserting values\n    into a dictionary. Section names are prefixed using and underscore.\n    Thus::\n\n        [sample]\n        name=12\n\n    is entered as ``sample_name=12`` into the dictionary. The sections\n    ``general`` and ``DEFAULT`` are treated specially in that both\n    the prefixed and the unprefixed values are inserted: ::\n\n       [general]\n       genome=hg19\n\n    will be added as ``general_genome=hg19`` and ``genome=hg19``.\n\n    Numbers will be automatically recognized as such and converted into\n    integers or floats.\n\n    Returns\n    -------\n    config : dict\n        A dictionary of configuration values\n\n    \"\"\"\n    p = defaultdict(lambda: defaultdict(TriggeredDefaultFactory()))\n    for section in config.sections():\n        for key, value in config.items(section):\n            try:\n                v = iotools.str2val(value)\n            except TypeError:\n                E.error(\"error converting key %s, value %s\" % (key, value))\n                E.error(\"Possible multiple concurrent attempts to \"\n                        \"read configuration\")\n                raise\n\n            p[\"%s_%s\" % (section, key)] = v\n\n            # IMS: new heirarchical format\n            try:\n                p[section][key] = v\n            except TypeError:\n                # fails with things like genome_dir=abc\n                # if [genome] does not exist.\n                continue\n\n            if section in (\"general\", \"DEFAULT\"):\n                p[\"%s\" % (key)] = v\n\n    for key, value in config.defaults().items():\n        p[\"%s\" % (key)] = iotools.str2val(value)\n\n    return p\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.connect","title":"<code>connect()</code>","text":"<p>connect to SQLite database used in this pipeline.</p> <p>.. note::    This method is currently only implemented for sqlite    databases. It needs refactoring for generic access.    Alternatively, use an full or partial ORM.</p> <p>If <code>annotations_database</code> is in params, this method will attach the named database as <code>annotations</code>.</p>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.connect--returns","title":"Returns","text":"<p>dbh    a database handle</p> Source code in <code>cgatcore/pipeline/database.py</code> <pre><code>def connect():\n    \"\"\"connect to SQLite database used in this pipeline.\n\n    .. note::\n       This method is currently only implemented for sqlite\n       databases. It needs refactoring for generic access.\n       Alternatively, use an full or partial ORM.\n\n    If ``annotations_database`` is in params, this method\n    will attach the named database as ``annotations``.\n\n    Returns\n    -------\n    dbh\n       a database handle\n\n    \"\"\"\n\n    # Note that in the future this might return an sqlalchemy or\n    # db.py handle.\n    url = get_params()[\"database\"][\"url\"]\n    is_sqlite3 = url.startswith(\"sqlite\")\n\n    if is_sqlite3:\n        connect_args = {'check_same_thread': False}\n    else:\n        connect_args = {}\n\n    creator = None\n    if is_sqlite3 and \"annotations_dir\" in get_params():\n        # not sure what the correct way is for url\n        # sqlite:///./csvdb -&gt; ./csvdb\n        # sqlite:////path/to/csvdb -&gt; /path/to/csvdb\n        filename = os.path.abspath(url[len(\"sqlite:///\"):])\n\n        def creator():\n            conn = sqlite3.connect(filename)\n            conn.execute(\"ATTACH DATABASE '{}' as annotations\".format(\n                os.path.join(get_params()[\"annotations_dir\"], \"csvdb\")))\n            return conn\n\n    engine = sqlalchemy.create_engine(\n        url,\n        connect_args=connect_args,\n        creator=creator)\n\n    return engine\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.create_view","title":"<code>create_view(dbhandle, tables, tablename, outfile, view_type='TABLE', ignore_duplicates=True)</code>","text":"<p>create a database view for a list of tables.</p> <p>This method performs a join across multiple tables and stores the result either as a view or a table in the database.</p>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.create_view--arguments","title":"Arguments","text":"<p>dbhandle :     A database handle. tables : list of tuples     Tables to merge. Each tuple contains the name of a table and     the field to join with the first table. For example::</p> <pre><code>    tables = (\n        \"reads_summary\", \"track\",\n        \"bam_stats\", \"track\",\n        \"context_stats\", \"track\",\n        \"picard_stats_alignment_summary_metrics\", \"track\")\n</code></pre> string <p>Name of the view or table to be created.</p> <p>outfile : string     Output filename for status information. view_type : string     Type of view, either <code>VIEW</code> or <code>TABLE</code>.  If a view is to be     created across multiple databases, use <code>TABLE</code>. ignore_duplicates : bool     If set to False, duplicate column names will be added with the     tablename as prefix. The default is to ignore.</p> Source code in <code>cgatcore/pipeline/database.py</code> <pre><code>def create_view(dbhandle, tables, tablename, outfile,\n                view_type=\"TABLE\",\n                ignore_duplicates=True):\n    '''create a database view for a list of tables.\n\n    This method performs a join across multiple tables and stores the\n    result either as a view or a table in the database.\n\n    Arguments\n    ---------\n    dbhandle :\n        A database handle.\n    tables : list of tuples\n        Tables to merge. Each tuple contains the name of a table and\n        the field to join with the first table. For example::\n\n            tables = (\n                \"reads_summary\", \"track\",\n                \"bam_stats\", \"track\",\n                \"context_stats\", \"track\",\n                \"picard_stats_alignment_summary_metrics\", \"track\")\n\n    tablename : string\n        Name of the view or table to be created.\n    outfile : string\n        Output filename for status information.\n    view_type : string\n        Type of view, either ``VIEW`` or ``TABLE``.  If a view is to be\n        created across multiple databases, use ``TABLE``.\n    ignore_duplicates : bool\n        If set to False, duplicate column names will be added with the\n        tablename as prefix. The default is to ignore.\n\n    '''\n\n    database.executewait(\n        dbhandle,\n        \"DROP %(view_type)s IF EXISTS %(tablename)s\" % locals())\n\n    tracks, columns = [], []\n    tablenames = [x[0] for x in tables]\n    for table, track in tables:\n        d = database.executewait(\n            dbhandle,\n            \"SELECT COUNT(DISTINCT %s) FROM %s\" % (track, table))\n        tracks.append(d.fetchone()[0])\n        columns.append(\n            [x.lower() for x in database.getColumnNames(dbhandle, table)\n             if x != track])\n\n    E.info(\"creating %s from the following tables: %s\" %\n           (tablename, str(list(zip(tablenames, tracks)))))\n    if min(tracks) != max(tracks):\n        raise ValueError(\n            \"number of rows not identical - will not create view\")\n\n    from_statement = \" , \".join(\n        [\"%s as t%i\" % (y[0], x) for x, y in enumerate(tables)])\n    f = tables[0][1]\n    where_statement = \" AND \".join(\n        [\"t0.%s = t%i.%s\" % (f, x + 1, y[1])\n         for x, y in enumerate(tables[1:])])\n\n    all_columns, taken = [], set()\n    for x, c in enumerate(columns):\n        i = set(taken).intersection(set(c))\n        if i:\n            E.warn(\"duplicate column names: %s \" % i)\n            if not ignore_duplicates:\n                table = tables[x][0]\n                all_columns.extend(\n                    [\"t%i.%s AS %s_%s\" % (x, y, table, y) for y in i])\n                c = [y for y in c if y not in i]\n\n        all_columns.extend([\"t%i.%s\" % (x, y) for y in c])\n        taken.update(set(c))\n\n    all_columns = \",\".join(all_columns)\n    statement = '''\n    CREATE %(view_type)s %(tablename)s AS SELECT t0.track, %(all_columns)s\n    FROM %(from_statement)s\n    WHERE %(where_statement)s\n    ''' % locals()\n    database.executewait(dbhandle, statement)\n\n    nrows = database.executewait(\n        dbhandle, \"SELECT COUNT(*) FROM view_mapping\").fetchone()[0]\n\n    if nrows == 0:\n        raise ValueError(\n            \"empty view mapping, check statement = %s\" %\n            (statement % locals()))\n    if nrows != min(tracks):\n        E.warn(\"view creates duplicate rows, got %i, expected %i\" %\n               (nrows, min(tracks)))\n\n    E.info(\"created view_mapping with %i rows\" % nrows)\n    touch_file(outfile)\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.execute","title":"<code>execute(statement, **kwargs)</code>","text":"<p>execute a statement locally.</p> <p>This method implements the same parameter interpolation as the function :func:<code>run</code>.</p>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.execute--arguments","title":"Arguments","text":"<p>statement : string     Command line statement to run.</p>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.execute--returns","title":"Returns","text":"<p>stdout : string     Data sent to standard output by command stderr : string     Data sent to standard error by command</p> Source code in <code>cgatcore/pipeline/execution.py</code> <pre><code>def execute(statement, **kwargs):\n    '''execute a statement locally.\n\n    This method implements the same parameter interpolation\n    as the function :func:`run`.\n\n    Arguments\n    ---------\n    statement : string\n        Command line statement to run.\n\n    Returns\n    -------\n    stdout : string\n        Data sent to standard output by command\n    stderr : string\n        Data sent to standard error by command\n    '''\n\n    if not kwargs:\n        kwargs = get_caller_locals()\n\n    kwargs = dict(list(get_params().items()) + list(kwargs.items()))\n\n    logger = get_logger()\n    logger.info(\"running %s\" % (statement % kwargs))\n\n    if \"cwd\" not in kwargs:\n        cwd = get_params()[\"work_dir\"]\n    else:\n        cwd = kwargs[\"cwd\"]\n\n    # cleaning up of statement\n    # remove new lines and superfluous spaces and tabs\n    statement = \" \".join(re.sub(\"\\t+\", \" \", statement).split(\"\\n\")).strip()\n    if statement.endswith(\";\"):\n        statement = statement[:-1]\n\n    # always use bash\n    os.environ.update(\n        {'BASH_ENV': os.path.join(os.environ['HOME'], '.bashrc')})\n    process = subprocess.Popen(statement % kwargs,\n                               cwd=cwd,\n                               shell=True,\n                               stdin=sys.stdin,\n                               stdout=sys.stdout,\n                               stderr=sys.stderr,\n                               env=os.environ.copy(),\n                               executable=\"/bin/bash\")\n\n    # process.stdin.close()\n    stdout, stderr = process.communicate()\n\n    if process.returncode != 0:\n        raise OSError(\n            \"Child was terminated by signal %i: \\n\"\n            \"The stderr was: \\n%s\\n%s\\n\" %\n            (-process.returncode, stderr, statement))\n\n    return stdout, stderr\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.file_is_mounted","title":"<code>file_is_mounted(filename)</code>","text":"<p>return True if filename is mounted.</p> <p>A file is likely to be mounted if it is located inside a subdirectory of the local scratch directory.</p> Source code in <code>cgatcore/pipeline/execution.py</code> <pre><code>def file_is_mounted(filename):\n    \"\"\"return True if filename is mounted.\n\n    A file is likely to be mounted if it is located\n    inside a subdirectory of the local scratch directory.\n    \"\"\"\n    if get_params()[\"mount_point\"]:\n        return os.path.abspath(filename).startswith(get_params()[\"mount_point\"])\n    else:\n        return False\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.get_caller","title":"<code>get_caller(decorators=0)</code>","text":"<p>return the name of the calling class/module</p>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.get_caller--arguments","title":"Arguments","text":"<p>decorators : int     Number of contexts to go up to reach calling function     of interest.</p>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.get_caller--returns","title":"Returns","text":"<p>mod : object     The calling module/class</p> Source code in <code>cgatcore/pipeline/utils.py</code> <pre><code>def get_caller(decorators=0):\n    \"\"\"return the name of the calling class/module\n\n    Arguments\n    ---------\n    decorators : int\n        Number of contexts to go up to reach calling function\n        of interest.\n\n    Returns\n    -------\n    mod : object\n        The calling module/class\n    \"\"\"\n\n    frm = inspect.stack()\n    return inspect.getmodule(frm[2 + decorators].frame)\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.get_caller_locals","title":"<code>get_caller_locals(decorators=0)</code>","text":"<p>returns the locals of the calling function.</p> <p>from http://pylab.blogspot.com/2009/02/      python-accessing-caller-locals-from.html</p>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.get_caller_locals--arguments","title":"Arguments","text":"<p>decorators : int     Number of contexts to go up to reach calling function     of interest.</p>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.get_caller_locals--returns","title":"Returns","text":"<p>locals : dict     Dictionary of variable defined in the context of the     calling function.</p> Source code in <code>cgatcore/pipeline/utils.py</code> <pre><code>def get_caller_locals(decorators=0):\n    '''returns the locals of the calling function.\n\n    from http://pylab.blogspot.com/2009/02/\n         python-accessing-caller-locals-from.html\n\n    Arguments\n    ---------\n    decorators : int\n        Number of contexts to go up to reach calling function\n        of interest.\n\n    Returns\n    -------\n    locals : dict\n        Dictionary of variable defined in the context of the\n        calling function.\n    '''\n    f = sys._getframe(2 + decorators)\n    args = inspect.getargvalues(f)\n    return args[3]\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.get_calling_function","title":"<code>get_calling_function(decorators=0)</code>","text":"<p>return the name of the calling function</p>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.get_calling_function--arguments","title":"Arguments","text":"<p>decorators : int     Number of contexts to go up to reach calling function     of interest.</p>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.get_calling_function--returns","title":"Returns","text":"<p>mod : object     The calling module</p> Source code in <code>cgatcore/pipeline/utils.py</code> <pre><code>def get_calling_function(decorators=0):\n    \"\"\"return the name of the calling function\n\n    Arguments\n    ---------\n    decorators : int\n        Number of contexts to go up to reach calling function\n        of interest.\n\n    Returns\n    -------\n    mod : object\n        The calling module\n    \"\"\"\n\n    frm = inspect.stack()\n    return frm[2 + decorators].function\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.get_database_name","title":"<code>get_database_name()</code>","text":"<p>Return the database name associated with the pipeline.</p> <p>This method lookis in different sections in the ini file to permit both old style <code>database</code> and new style <code>database_name</code>.</p> <p>This method has been implemented for backwards compatibility.</p>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.get_database_name--returns","title":"Returns","text":"<p>databasename : string     database name. Returns empty string if not found.</p>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.get_database_name--raises","title":"Raises","text":"<p>KeyError    If no database name is found</p> Source code in <code>cgatcore/pipeline/database.py</code> <pre><code>def get_database_name():\n    '''Return the database name associated with the pipeline.\n\n    This method lookis in different sections in the ini file to permit\n    both old style ``database`` and new style ``database_name``.\n\n    This method has been implemented for backwards compatibility.\n\n    Returns\n    -------\n    databasename : string\n        database name. Returns empty string if not found.\n\n    Raises\n    ------\n    KeyError\n       If no database name is found\n\n    '''\n\n    locations = [\"database_name\", \"database\"]\n    params = get_params()\n    for location in locations:\n        database = params.get(location, None)\n        if database is not None:\n            return database\n\n    raise KeyError(\"database name not found\")\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.get_executor","title":"<code>get_executor(options=None)</code>","text":"<p>Return an executor instance based on the specified queue manager in options.</p> <ul> <li>options (dict): Dictionary containing execution options,                    including \"cluster_queue_manager\".</li> </ul> <p>Returns: - Executor instance appropriate for the specified queue manager.</p> Source code in <code>cgatcore/pipeline/execution.py</code> <pre><code>def get_executor(options=None):\n    \"\"\"\n    Return an executor instance based on the specified queue manager in options.\n\n    Parameters:\n    - options (dict): Dictionary containing execution options, \n                      including \"cluster_queue_manager\".\n\n    Returns:\n    - Executor instance appropriate for the specified queue manager.\n    \"\"\"\n    if options is None:\n        options = get_params()\n\n    if options.get(\"testing\", False):\n        return LocalExecutor(**options)\n\n    # Check if to_cluster is explicitly set to False\n    if not options.get(\"to_cluster\", True):  # Defaults to True if not specified\n        return LocalExecutor(**options)\n\n    queue_manager = options.get(\"cluster_queue_manager\", None)\n\n    # Check for KubernetesExecutor\n    if queue_manager == \"kubernetes\" and KubernetesExecutor is not None:\n        return KubernetesExecutor(**options)\n\n    # Check for SGEExecutor (Sun Grid Engine)\n    elif queue_manager == \"sge\" and shutil.which(\"qsub\") is not None:\n        return SGEExecutor(**options)\n\n    # Check for SlurmExecutor\n    elif queue_manager == \"slurm\" and shutil.which(\"sbatch\") is not None:\n        return SlurmExecutor(**options)\n\n    # Check for TorqueExecutor\n    elif queue_manager == \"torque\" and shutil.which(\"qsub\") is not None:\n        return TorqueExecutor(**options)\n\n    # Fallback to LocalExecutor, not sure if this should raise an error though, feels like it should\n    else:\n        return LocalExecutor(**options)\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.get_header","title":"<code>get_header()</code>","text":"<p>return a header string with command line options and timestamp</p> Source code in <code>cgatcore/experiment.py</code> <pre><code>def get_header():\n    \"\"\"return a header string with command line options and timestamp\n\n    \"\"\"\n    system, host, release, version, machine = os.uname()\n\n    return \"output generated by %s\\njob started at %s on %s -- %s\\npid: %i, system: %s %s %s %s\" %\\\n           (\" \".join(sys.argv),\n            time.asctime(time.localtime(time.time())),\n            host,\n            global_id,\n            os.getpid(),\n            system, release, version, machine)\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.get_mounted_location","title":"<code>get_mounted_location(filename)</code>","text":"<p>return location of filename within mounted directory</p> Source code in <code>cgatcore/pipeline/execution.py</code> <pre><code>def get_mounted_location(filename):\n    \"\"\"return location of filename within mounted directory\n\n    \"\"\"\n    return os.path.abspath(filename)[len(get_params()[\"mount_point\"]):]\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.get_param_output","title":"<code>get_param_output(options=None)</code>","text":"<p>return a string containing script parameters.</p> <p>Parameters are all variables that start with <code>param_</code>.</p> Source code in <code>cgatcore/pipeline/control.py</code> <pre><code>def get_param_output(options=None):\n    \"\"\"return a string containing script parameters.\n\n    Parameters are all variables that start with ``param_``.\n    \"\"\"\n    result = []\n    if options:\n        members = options\n        for k, v in sorted(members.items()):\n            result.append(\"%-40s: %s\" % (k, str(v)))\n    else:\n        vars = inspect.currentframe().f_back.f_locals\n        for var in [x for x in list(vars.keys()) if re.match(\"param_\", x)]:\n            result.append(\"%-40s: %s\" %\n                          (var, str(vars[var])))\n\n    if result:\n        return \"\\n\".join(result)\n    else:\n        return \"# no parameters.\"\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.get_param_section","title":"<code>get_param_section(section)</code>","text":"<p>return config values in section</p> <p>Sections are built by common prefixes.</p> Source code in <code>cgatcore/pipeline/parameters.py</code> <pre><code>def get_param_section(section):\n    \"\"\"return config values in section\n\n    Sections are built by common prefixes.\n    \"\"\"\n    if not section.endswith(\"_\"):\n        section = section + \"_\"\n    n = len(section)\n    return [(x[n:], y) for x, y in PARAMS.items() if x.startswith(section)]\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.get_parameters","title":"<code>get_parameters(filenames=None, defaults=None, site_ini=True, user=True, only_import=None)</code>","text":"<p>read one or more config files and build global PARAMS configuration dictionary.</p>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.get_parameters--arguments","title":"Arguments","text":"<p>filenames : list    List of filenames of the configuration files to read. defaults : dict    Dictionary with default values. These will be overwrite    any hard-coded parameters, but will be overwritten by user    specified parameters in the configuration files. user : bool    If set, configuration files will also be read from a    file called :file:<code>.cgat.yml</code> in the user<code>s    home directory. only_import : bool    If set to a boolean, the parameter dictionary will be a    defaultcollection. This is useful for pipelines that are    imported (for example for documentation generation) but not    executed as there might not be an appropriate .yml file    available. If</code>only_import` is None, it will be set to the    default, which is to raise an exception unless the calling    script is imported or the option <code>--is-test</code> has been passed    at the command line.</p>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.get_parameters--returns","title":"Returns","text":"<p>params : dict    Global configuration dictionary.</p> Source code in <code>cgatcore/pipeline/parameters.py</code> <pre><code>def get_parameters(filenames=None,\n                   defaults=None,\n                   site_ini=True,\n                   user=True,\n                   only_import=None):\n    '''read one or more config files and build global PARAMS configuration\n    dictionary.\n\n    Arguments\n    ---------\n    filenames : list\n       List of filenames of the configuration files to read.\n    defaults : dict\n       Dictionary with default values. These will be overwrite\n       any hard-coded parameters, but will be overwritten by user\n       specified parameters in the configuration files.\n    user : bool\n       If set, configuration files will also be read from a\n       file called :file:`.cgat.yml` in the user`s\n       home directory.\n    only_import : bool\n       If set to a boolean, the parameter dictionary will be a\n       defaultcollection. This is useful for pipelines that are\n       imported (for example for documentation generation) but not\n       executed as there might not be an appropriate .yml file\n       available. If `only_import` is None, it will be set to the\n       default, which is to raise an exception unless the calling\n       script is imported or the option ``--is-test`` has been passed\n       at the command line.\n\n    Returns\n    -------\n    params : dict\n       Global configuration dictionary.\n    '''\n    global PARAMS, HAVE_INITIALIZED\n    # only execute function once\n    if HAVE_INITIALIZED:\n        return PARAMS\n\n    if filenames is None:\n        filenames = [\"pipeline.yml\", \"cgat.yml\"]\n    elif isinstance(filenames, str):\n        filenames = [filenames]\n\n    old_id = id(PARAMS)\n\n    caller_locals = get_caller_locals()\n\n    # check if this is only for import\n    if only_import is None:\n        only_import = is_test() or \"__name__\" not in caller_locals or \\\n            caller_locals[\"__name__\"] != \"__main__\"\n\n    # important: only update the PARAMS variable as\n    # it is referenced in other modules. Thus the type\n    # needs to be fixed at import. Raise error where this\n    # is not the case.\n    # Note: Parameter sharing in the pipeline module needs\n    # to be reorganized.\n    if only_import:\n        # turn on default dictionary\n        TriggeredDefaultFactory.with_default = True\n\n    # check if the pipeline is in testing mode\n    found = False\n    if 'argv' in caller_locals and caller_locals['argv'] is not None:\n        for e in caller_locals['argv']:\n            if 'template_pipeline.py' in e:\n                found = True\n    PARAMS['testing'] = 'self' in caller_locals or found\n\n    if site_ini:\n        # read configuration from /etc/cgat/pipeline.yml\n        fn = \"/etc/cgat/pipeline.yml\"\n        if os.path.exists(fn):\n            filenames.insert(0, fn)\n\n    if user:\n        # read configuration from a users home directory\n        fn = os.path.join(os.path.expanduser(\"~\"),\n                          \".cgat.yml\")\n        if os.path.exists(fn):\n            if 'pipeline.yml' in filenames:\n                filenames.insert(filenames.index('pipeline.yml'), fn)\n            else:\n                filenames.append(fn)\n\n    filenames = [x.strip() for x in filenames if os.path.exists(x)]\n\n    # save list of config files\n    PARAMS[\"pipeline_yml\"] = filenames\n\n    # update with hard-coded PARAMS\n    nested_update(PARAMS, HARDCODED_PARAMS)\n    if defaults:\n        nested_update(PARAMS, defaults)\n\n    # reset working directory. Set in PARAMS to prevent repeated calls to\n    # os.getcwd() failing if network is busy\n    PARAMS[\"start_dir\"] = os.path.abspath(os.getcwd())\n    # location of pipelines - set via location of top frame (cgatflow command)\n    if '__file__' in caller_locals:\n        PARAMS[\"pipelinedir\"] = os.path.dirname(caller_locals[\"__file__\"])\n    else:\n        PARAMS[\"pipelinedir\"] = 'unknown'\n\n    for filename in filenames:\n        if not os.path.exists(filename):\n            continue\n        get_logger().info(\"reading config from file {}\".format(\n            filename))\n\n        with open(filename, 'rt', encoding='utf8') as inf:\n            p = yaml.load(inf, Loader=yaml.FullLoader)\n            if p:\n                nested_update(PARAMS, p)\n\n    # for backwards compatibility - normalize dictionaries\n    p = {}\n    for k, v in PARAMS.items():\n        if isinstance(v, Mapping):\n            for kk, vv in v.items():\n                new_key = \"{}_{}\".format(k, kk)\n                if new_key in p:\n                    raise ValueError(\n                        \"key {} does already exist\".format(new_key))\n                p[new_key] = vv\n    nested_update(PARAMS, p)\n\n    # interpolate some params with other parameters\n    for param in INTERPOLATE_PARAMS:\n        try:\n            PARAMS[param] = PARAMS[param] % PARAMS\n        except TypeError as msg:\n            raise TypeError('could not interpolate %s: %s' %\n                            (PARAMS[param], msg))\n\n    # expand directory pathnames\n    for param, value in list(PARAMS.items()):\n        if (param.endswith(\"dir\") and isinstance(value, str) and value.startswith(\".\")):\n            PARAMS[param] = os.path.abspath(value)\n\n    # make sure that the dictionary reference has not changed\n    assert id(PARAMS) == old_id\n    HAVE_INITIALIZED = True\n    return PARAMS\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.get_parameters_as_namedtuple","title":"<code>get_parameters_as_namedtuple(*args, **kwargs)</code>","text":"<p>return PARAM dictionary as a namedtuple.</p> Source code in <code>cgatcore/pipeline/parameters.py</code> <pre><code>def get_parameters_as_namedtuple(*args, **kwargs):\n    \"\"\"return PARAM dictionary as a namedtuple.\n    \"\"\"\n    d = get_parameters(*args, **kwargs)\n    return collections.namedtuple('GenericDict', list(d.keys()))(**d)\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.get_params","title":"<code>get_params()</code>","text":"<p>return handle to global parameter dictionary</p> Source code in <code>cgatcore/pipeline/parameters.py</code> <pre><code>def get_params():\n    \"\"\"return handle to global parameter dictionary\"\"\"\n    return PARAMS\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.get_s3_pipeline","title":"<code>get_s3_pipeline()</code>","text":"<p>Instantiate and return the S3Pipeline instance, lazy-loaded to avoid circular imports.</p> Source code in <code>cgatcore/pipeline/__init__.py</code> <pre><code>def get_s3_pipeline():\n    \"\"\"Instantiate and return the S3Pipeline instance, lazy-loaded to avoid circular imports.\"\"\"\n    # Use get_remote() to access the remote functionality\n    remote = cgatcore.get_remote()  # Now properly calls the method to initialize remote if needed\n    return remote.file_handler.S3Pipeline()\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.get_temp_dir","title":"<code>get_temp_dir(dir=None, shared=False, clear=False)</code>","text":"<p>get a temporary directory.</p> <p>The directory is created and the caller needs to delete the temporary directory once it is not used any more.</p> <p>If dir does not exist, it will be created.</p>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.get_temp_dir--arguments","title":"Arguments","text":"<p>dir : string     Directory of the temporary directory and if not given is set to the     default temporary location in the global configuration dictionary. shared : bool     If set, the tempory directory will be in a shared temporary     location.</p>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.get_temp_dir--returns","title":"Returns","text":"<p>filename : string     Absolute pathname of temporary file.</p> Source code in <code>cgatcore/pipeline/files.py</code> <pre><code>def get_temp_dir(dir=None, shared=False, clear=False):\n    '''get a temporary directory.\n\n    The directory is created and the caller needs to delete the temporary\n    directory once it is not used any more.\n\n    If dir does not exist, it will be created.\n\n    Arguments\n    ---------\n    dir : string\n        Directory of the temporary directory and if not given is set to the\n        default temporary location in the global configuration dictionary.\n    shared : bool\n        If set, the tempory directory will be in a shared temporary\n        location.\n\n    Returns\n    -------\n    filename : string\n        Absolute pathname of temporary file.\n\n    '''\n    if dir is None:\n        if shared:\n            dir = get_params()['shared_tmpdir']\n        else:\n            dir = get_params()['tmpdir']\n\n    if not os.path.exists(dir):\n        os.makedirs(dir)\n\n    tmpdir = tempfile.mkdtemp(dir=dir, prefix=\"ctmp\")\n    if clear:\n        os.rmdir(tmpdir)\n    return tmpdir\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.get_temp_file","title":"<code>get_temp_file(dir=None, shared=False, suffix='', mode='w+', encoding='utf-8')</code>","text":"<p>get a temporary file.</p> <p>The file is created and the caller needs to close and delete the temporary file once it is not used any more. By default, the file is opened as a text file (mode <code>w+</code>) with encoding <code>utf-8</code> instead of the default mode <code>w+b</code> used in :class:<code>tempfile.NamedTemporaryFile</code></p> <p>If dir does not exist, it will be created.</p>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.get_temp_file--arguments","title":"Arguments","text":"<p>dir : string     Directory of the temporary file and if not given is set to the     default temporary location in the global configuration dictionary. shared : bool     If set, the tempory file will be in a shared temporary     location (given by the global configuration directory). suffix : string     Filename suffix</p>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.get_temp_file--returns","title":"Returns","text":"<p>file : File     A file object of the temporary file.</p> Source code in <code>cgatcore/pipeline/files.py</code> <pre><code>def get_temp_file(dir=None, shared=False, suffix=\"\", mode=\"w+\", encoding=\"utf-8\"):\n    '''get a temporary file.\n\n    The file is created and the caller needs to close and delete the\n    temporary file once it is not used any more. By default, the file\n    is opened as a text file (mode ``w+``) with encoding ``utf-8``\n    instead of the default mode ``w+b`` used in\n    :class:`tempfile.NamedTemporaryFile`\n\n    If dir does not exist, it will be created.\n\n    Arguments\n    ---------\n    dir : string\n        Directory of the temporary file and if not given is set to the\n        default temporary location in the global configuration dictionary.\n    shared : bool\n        If set, the tempory file will be in a shared temporary\n        location (given by the global configuration directory).\n    suffix : string\n        Filename suffix\n\n    Returns\n    -------\n    file : File\n        A file object of the temporary file.\n\n    '''\n    if dir is None:\n        if shared:\n            dir = get_params()['shared_tmpdir']\n        else:\n            dir = get_params()['tmpdir']\n\n    if not os.path.exists(dir):\n        try:\n            os.makedirs(dir)\n        except OSError:\n            # avoid race condition when several processes try to create\n            # temporary directory.\n            pass\n        if not os.path.exists(dir):\n            raise OSError(\n                \"temporary directory {} could not be created\".format(dir))\n\n    return tempfile.NamedTemporaryFile(dir=dir, delete=False, prefix=\"ctmp\",\n                                       mode=mode,\n                                       encoding=encoding, suffix=suffix)\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.get_temp_filename","title":"<code>get_temp_filename(dir=None, shared=False, clear=True, suffix='')</code>","text":"<p>return a temporary filename.</p> <p>The file is created and the caller needs to delete the temporary file once it is not used any more (unless <code>clear</code> is set`).</p> <p>If dir does not exist, it will be created.</p>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.get_temp_filename--arguments","title":"Arguments","text":"<p>dir : string     Directory of the temporary file and if not given is set to the     default temporary location in the global configuration dictionary. shared : bool     If set, the tempory file will be in a shared temporary     location. clear : bool     If set, remove the temporary file after creation. suffix : string     Filename suffix</p>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.get_temp_filename--returns","title":"Returns","text":"<p>filename : string     Absolute pathname of temporary file.</p> Source code in <code>cgatcore/pipeline/files.py</code> <pre><code>def get_temp_filename(dir=None, shared=False, clear=True, suffix=\"\"):\n    '''return a temporary filename.\n\n    The file is created and the caller needs to delete the temporary\n    file once it is not used any more (unless `clear` is set`).\n\n    If dir does not exist, it will be created.\n\n    Arguments\n    ---------\n    dir : string\n        Directory of the temporary file and if not given is set to the\n        default temporary location in the global configuration dictionary.\n    shared : bool\n        If set, the tempory file will be in a shared temporary\n        location.\n    clear : bool\n        If set, remove the temporary file after creation.\n    suffix : string\n        Filename suffix\n\n    Returns\n    -------\n    filename : string\n        Absolute pathname of temporary file.\n\n    '''\n    tmpfile = get_temp_file(dir=dir, shared=shared, suffix=suffix)\n    tmpfile.close()\n    if clear:\n        os.unlink(tmpfile.name)\n    return tmpfile.name\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.initialize","title":"<code>initialize(argv=None, caller=None, defaults=None, optparse=True, **kwargs)</code>","text":"<p>setup the pipeline framework.</p>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.initialize--arguments","title":"Arguments","text":"<p>options: object     Container for command line arguments. args : list     List of command line arguments. defaults : dictionary     Dictionary with default values to be added to global     parameters dictionary.</p> <p>Additional keyword arguments will be passed to the :func:<code>~.parse_commandline</code> function to set command-line defaults.</p> Source code in <code>cgatcore/pipeline/control.py</code> <pre><code>def initialize(argv=None, caller=None, defaults=None, optparse=True, **kwargs):\n    \"\"\"setup the pipeline framework.\n\n    Arguments\n    ---------\n    options: object\n        Container for command line arguments.\n    args : list\n        List of command line arguments.\n    defaults : dictionary\n        Dictionary with default values to be added to global\n        parameters dictionary.\n\n    Additional keyword arguments will be passed to the\n    :func:`~.parse_commandline` function to set command-line defaults.\n\n    \"\"\"\n    if argv is None:\n        argv = sys.argv\n\n    # load default options from config files\n    if caller:\n        path = os.path.splitext(caller)[0]\n    else:\n        try:\n            path = os.path.splitext(get_caller().__file__)[0]\n        except AttributeError as ex:\n            path = \"unknown\"\n\n    parse_commandline(argv, optparse, **kwargs)\n    args = E.get_args()\n    get_parameters(\n        [os.path.join(path, \"pipeline.yml\"),\n         \"../pipeline.yml\",\n         args.config_file],\n        defaults=defaults)\n\n    logger = logging.getLogger(\"cgatcore.pipeline\")\n    logger.info(\"started in directory: {}\".format(\n        get_params().get(\"start_dir\")))\n\n    # At this point, the PARAMS dictionary has already been\n    # built. It now needs to be updated with selected command\n    # line options as these should always take precedence over\n    # configuration files.\n    update_params_with_commandline_options(get_params(), args)\n\n    logger.info(get_header())\n\n    logger.info(get_param_output(get_params()))\n\n    code_location, version = get_version()\n    logger.info(\"code location: {}\".format(code_location))\n    logger.info(\"code version: {}\".format(version))\n\n    logger.info(\"working directory is: {}\".format(\n        get_params().get(\"work_dir\")))\n    work_dir = get_params().get(\"work_dir\")\n    if not os.path.exists(work_dir):\n        E.info(\"working directory {} does not exist - creating\".format(work_dir))\n        os.makedirs(work_dir)\n    logger.info(\"changing directory to {}\".format(work_dir))\n    os.chdir(work_dir)\n\n    logger.info(\"pipeline has been initialized\")\n\n    return args\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.input_validation","title":"<code>input_validation(PARAMS, pipeline_script='')</code>","text":"<p>Inspects the PARAMS dictionary looking for problematic input values.</p> <p>So far we just check that:</p> <pre><code>* all required 3rd party tools are on the PATH\n\n* input parameters are not empty\n\n* input parameters do not contain the \"?\" character (used as a\n  placeholder in different pipelines)\n\n* if the input is a file, check whether it exists and\n  is readable\n</code></pre> Source code in <code>cgatcore/pipeline/parameters.py</code> <pre><code>def input_validation(PARAMS, pipeline_script=\"\"):\n    '''Inspects the PARAMS dictionary looking for problematic input values.\n\n    So far we just check that:\n\n        * all required 3rd party tools are on the PATH\n\n        * input parameters are not empty\n\n        * input parameters do not contain the \"?\" character (used as a\n          placeholder in different pipelines)\n\n        * if the input is a file, check whether it exists and\n          is readable\n    '''\n\n    E.info('''input Validation starting''')\n    E.info('''checking 3rd party dependencies''')\n\n    # check 3rd party dependencies\n    if len(pipeline_script) &gt; 0:\n        # this import requires the PYTHONPATH in the following order\n        # PYTHONPATH=&lt;src&gt;/CGATpipelines:&lt;src&gt;/cgat\n        import scripts.cgat_check_deps as cd\n        deps, check_path_failures = cd.checkDepedencies(pipeline_script)\n        # print info about dependencies\n        if len(deps) == 0:\n            E.info('no dependencies found')\n        else:\n            # print dictionary ordered by value\n            for k in sorted(deps, key=deps.get, reverse=True):\n                E.info('Program: {0!s} used {1} time(s)'.format(k, deps[k]))\n            n_failures = len(check_path_failures)\n            if n_failures == 0:\n                E.info('All required programs are available on your PATH')\n            else:\n                E.info('The following programs are not on your PATH')\n                for p in check_path_failures:\n                    E.info('{0!s}'.format(p))\n\n    # check PARAMS\n    num_missing = 0\n    num_questions = 0\n\n    E.info('''checking pipeline configuration''')\n\n    for key, value in sorted(PARAMS.items()):\n\n        key = str(key)\n        value = str(value)\n\n        # check for missing values\n        if value == \"\":\n            E.warn('\\n\"{}\" is empty, is that expected?'.format(key))\n            num_missing += 1\n\n        # check for a question mark in the dictironary (indicates\n        # that there is a missing input parameter)\n        if \"?\" in value:\n            E.warn('\\n\"{}\" is not defined (?), is that expected?'.format(key))\n            num_questions += 1\n\n        # validate input files listed in PARAMS\n        if (value.startswith(\"/\") or value.endswith(\".gz\") or value.endswith(\".gtf\")) and \",\" not in value:\n            if not os.access(value, os.R_OK):\n                E.warn('\\n\"{}\": \"{}\" is not readable'.format(key, value))\n\n    if num_missing or num_questions:\n        raise ValueError(\"pipeline has configuration issues\")\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.interpolate_statement","title":"<code>interpolate_statement(statement, kwargs)</code>","text":"<p>interpolate command line statement with parameters</p> <p>The skeleton of the statement should be defined in kwargs.  The method then applies string interpolation using a dictionary built from the global configuration dictionary PARAMS, but augmented by <code>kwargs</code>. The latter takes precedence.</p>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.interpolate_statement--arguments","title":"Arguments","text":"<p>statement: string     Command line statement to be interpolated. kwargs : dict     Keyword arguments that are used for parameter interpolation.</p>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.interpolate_statement--returns","title":"Returns","text":"<p>statement : string     The command line statement with interpolated parameters.</p>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.interpolate_statement--raises","title":"Raises","text":"<p>KeyError     If <code>statement</code> contains unresolved references.</p> Source code in <code>cgatcore/pipeline/execution.py</code> <pre><code>def interpolate_statement(statement, kwargs):\n    '''interpolate command line statement with parameters\n\n    The skeleton of the statement should be defined in kwargs.  The\n    method then applies string interpolation using a dictionary built\n    from the global configuration dictionary PARAMS, but augmented by\n    `kwargs`. The latter takes precedence.\n\n    Arguments\n    ---------\n    statement: string\n        Command line statement to be interpolated.\n    kwargs : dict\n        Keyword arguments that are used for parameter interpolation.\n\n    Returns\n    -------\n    statement : string\n        The command line statement with interpolated parameters.\n\n    Raises\n    ------\n    KeyError\n        If ``statement`` contains unresolved references.\n\n    '''\n\n    local_params = substitute_parameters(**kwargs)\n\n    # build the statement\n    try:\n        statement = statement % local_params\n    except KeyError as msg:\n        raise KeyError(\n            \"Error when creating command: could not \"\n            \"find %s in dictionaries\" % msg)\n    except ValueError as msg:\n        raise ValueError(\n            \"Error when creating command: %s, statement = %s\" % (\n                msg, statement))\n\n    # cleaning up of statement\n    # remove new lines and superfluous spaces and tabs\n    statement = \" \".join(re.sub(\"\\t+\", \" \", statement).split(\"\\n\")).strip()\n    if statement.endswith(\";\"):\n        statement = statement[:-1]\n\n    # mark arvados mount points in statement\n    if get_params().get(\"mount_point\", None):\n        statement = re.sub(get_params()[\"mount_point\"], \"arv=\", statement)\n\n    return statement\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.is_test","title":"<code>is_test()</code>","text":"<p>return True if the pipeline is run in a \"testing\" mode.</p> <p>This method checks if <code>-is-test</code> has been given as a command line option.</p> Source code in <code>cgatcore/pipeline/utils.py</code> <pre><code>def is_test():\n    \"\"\"return True if the pipeline is run in a \"testing\" mode.\n\n    This method checks if ``-is-test`` has been given as a\n    command line option.\n    \"\"\"\n    return \"--is-test\" in sys.argv\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.is_true","title":"<code>is_true(param, **kwargs)</code>","text":"<p>return True if param has a True value.</p> <p>A parameter is False if it is:</p> <ul> <li>not set</li> <li>0</li> <li>the empty string</li> <li>false or False</li> </ul> <p>Otherwise the value is True.</p>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.is_true--arguments","title":"Arguments","text":"<p>param : string     Parameter to be tested kwargs : dict     Dictionary of local configuration values. These will be passed     to :func:<code>substitute_parameters</code> before evaluating <code>param</code></p>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.is_true--returns","title":"Returns","text":"<p>bool</p> Source code in <code>cgatcore/pipeline/parameters.py</code> <pre><code>def is_true(param, **kwargs):\n    '''return True if param has a True value.\n\n    A parameter is False if it is:\n\n    * not set\n    * 0\n    * the empty string\n    * false or False\n\n    Otherwise the value is True.\n\n    Arguments\n    ---------\n    param : string\n        Parameter to be tested\n    kwargs : dict\n        Dictionary of local configuration values. These will be passed\n        to :func:`substitute_parameters` before evaluating `param`\n\n    Returns\n    -------\n    bool\n\n    '''\n    if kwargs:\n        p = substitute_parameters(**kwargs)\n    else:\n        p = PARAMS\n    value = p.get(param, 0)\n    return value not in (0, '', 'false', 'False')\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.join_statements","title":"<code>join_statements(statements, infile, outfile=None)</code>","text":"<p>join a chain of statements into a single statement.</p> <p>Each statement contains an @IN@ or a @OUT@ placeholder or both. These will be replaced by the names of successive temporary files.</p> <p>In the first statement, @IN@ is replaced with <code>infile</code> and, if given, the @OUT@ is replaced by outfile in the last statement.</p>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.join_statements--arguments","title":"Arguments","text":"<p>statements : list     A list of command line statements. infile : string     Filename of the first data set. outfile : string     Filename of the target data set.</p>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.join_statements--returns","title":"Returns","text":"<p>last_file : string     Filename of last file created, outfile, if given. statement : string     A command line statement built from merging the statements cleanup : string     A command line statement for cleaning up.</p> Source code in <code>cgatcore/pipeline/execution.py</code> <pre><code>def join_statements(statements, infile, outfile=None):\n    '''join a chain of statements into a single statement.\n\n    Each statement contains an @IN@ or a @OUT@ placeholder or both.\n    These will be replaced by the names of successive temporary files.\n\n    In the first statement, @IN@ is replaced with `infile` and, if given,\n    the @OUT@ is replaced by outfile in the last statement.\n\n    Arguments\n    ---------\n    statements : list\n        A list of command line statements.\n    infile : string\n        Filename of the first data set.\n    outfile : string\n        Filename of the target data set.\n\n    Returns\n    -------\n    last_file : string\n        Filename of last file created, outfile, if given.\n    statement : string\n        A command line statement built from merging the statements\n    cleanup : string\n        A command line statement for cleaning up.\n\n    '''\n\n    prefix = get_temp_filename()\n\n    pattern = \"%s_%%i\" % prefix\n\n    result = []\n    for x, statement in enumerate(statements):\n        s = statement\n        if x == 0:\n            if infile is not None:\n                s = re.sub(\"@IN@\", infile, s)\n        else:\n            s = re.sub(\"@IN@\", pattern % x, s)\n            if x &gt; 2:\n                s = re.sub(\"@IN-2@\", pattern % (x - 2), s)\n            if x &gt; 1:\n                s = re.sub(\"@IN-1@\", pattern % (x - 1), s)\n\n        s = re.sub(\"@OUT@\", pattern % (x + 1), s).strip()\n\n        if s.endswith(\";\"):\n            s = s[:-1]\n        result.append(s)\n\n    result = \"; \".join(result)\n    last_file = pattern % (x + 1)\n    if outfile:\n        result = re.sub(last_file, outfile, result)\n        last_file = outfile\n\n    assert prefix != \"\"\n    return last_file, result, \"rm -f %s*\" % prefix\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.load","title":"<code>load(infile, outfile=None, options='', collapse=False, transpose=False, tablename=None, retry=True, limit=0, shuffle=False, job_memory=None, to_cluster=False)</code>","text":"<p>import data from a tab-separated file into database.</p> <p>The table name is given by outfile without the \".load\" suffix.</p> <p>A typical load task in ruffus would look like this::</p> <pre><code>@transform(\"*.tsv.gz\", suffix(\".tsv.gz\"), \".load\")\ndef loadData(infile, outfile):\n    P.load(infile, outfile)\n</code></pre> <p>Upload is performed via the :doc:<code>csv2db</code> script.</p>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.load--arguments","title":"Arguments","text":"<p>infile : string     Filename of the input data outfile : string     Output filename. This will contain the logging information. The     table name is derived from <code>outfile</code> if <code>tablename</code> is not set. options : string     Command line options for the <code>csv2db.py</code> script. collapse : string     If set, the table will be collapsed before loading. This     transforms a data set with two columns where the first column     is the row name into a multi-column table.  The value of     collapse is the value used for missing values. transpose : string     If set, the table will be transposed before loading. The first     column in the first row will be set to the string within     transpose. retry : bool     If True, multiple attempts will be made if the data can     not be loaded at the first try, for example if a table is locked. limit : int     If set, only load the first n lines. shuffle : bool     If set, randomize lines before loading. Together with <code>limit</code>     this permits loading a sample of rows. job_memory : string     Amount of memory to allocate for job. If unset, uses the global     default. Implies to_cluster=True. to_cluster : bool     By default load jobs are not submitted to the cluster as they sometimes     become blocked. Setting this true will override this behavoir.</p> Source code in <code>cgatcore/pipeline/database.py</code> <pre><code>def load(infile,\n         outfile=None,\n         options=\"\",\n         collapse=False,\n         transpose=False,\n         tablename=None,\n         retry=True,\n         limit=0,\n         shuffle=False,\n         job_memory=None,\n         to_cluster=False):\n    \"\"\"import data from a tab-separated file into database.\n\n    The table name is given by outfile without the\n    \".load\" suffix.\n\n    A typical load task in ruffus would look like this::\n\n        @transform(\"*.tsv.gz\", suffix(\".tsv.gz\"), \".load\")\n        def loadData(infile, outfile):\n            P.load(infile, outfile)\n\n    Upload is performed via the :doc:`csv2db` script.\n\n    Arguments\n    ---------\n    infile : string\n        Filename of the input data\n    outfile : string\n        Output filename. This will contain the logging information. The\n        table name is derived from `outfile` if `tablename` is not set.\n    options : string\n        Command line options for the `csv2db.py` script.\n    collapse : string\n        If set, the table will be collapsed before loading. This\n        transforms a data set with two columns where the first column\n        is the row name into a multi-column table.  The value of\n        collapse is the value used for missing values.\n    transpose : string\n        If set, the table will be transposed before loading. The first\n        column in the first row will be set to the string within\n        transpose.\n    retry : bool\n        If True, multiple attempts will be made if the data can\n        not be loaded at the first try, for example if a table is locked.\n    limit : int\n        If set, only load the first n lines.\n    shuffle : bool\n        If set, randomize lines before loading. Together with `limit`\n        this permits loading a sample of rows.\n    job_memory : string\n        Amount of memory to allocate for job. If unset, uses the global\n        default. Implies to_cluster=True.\n    to_cluster : bool\n        By default load jobs are not submitted to the cluster as they sometimes\n        become blocked. Setting this true will override this behavoir.\n    \"\"\"\n\n    if job_memory is None:\n        job_memory = get_params()[\"cluster_memory_default\"]\n\n    if not tablename:\n        tablename = to_table(outfile)\n\n    statement = []\n\n    if infile.endswith(\".gz\"):\n        statement.append(\"zcat %(infile)s\")\n    else:\n        statement.append(\"cat %(infile)s\")\n\n    if collapse:\n        statement.append(\n            \"python -m cgatcore.table \"\n            \"--log=%(outfile)s.collapse.log \"\n            \"--collapse=%(collapse)s\")\n\n    if transpose:\n        statement.append(\n            \"python -m cgatcore.table \"\n            \"--log=%(outfile)s.transpose.log \"\n            \"--transpose \"\n            \"--set-transpose-field=%(transpose)s\")\n\n    if shuffle:\n        statement.append(\n            \"python -m cgatcore.table \"\n            \"--log=%(outfile)s.shuffle.log \"\n            \"--method=randomize-rows\")\n\n    if limit &gt; 0:\n        # use awk to filter in order to avoid a pipeline broken error from head\n        statement.append(\"awk 'NR &gt; %i {exit(0)} {print}'\" % (limit + 1))\n        # ignore errors from cat or zcat due to broken pipe\n        ignore_pipe_errors = True\n\n    statement.append(build_load_statement(tablename,\n                                          options=options,\n                                          retry=retry))\n\n    statement = \" | \".join(statement) + \" &gt; %(outfile)s\"\n\n    run(statement)\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.load_from_iterator","title":"<code>load_from_iterator(outfile, tablename, iterator, columns=None, indices=None)</code>","text":"<p>import data from an iterator into a database.</p>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.load_from_iterator--arguments","title":"Arguments","text":"<p>outfile : string     Output file name tablename : string     Table name iterator : iterator     Iterator to import data from. The iterator should     yield either list/tuples or dictionaries for each     row in the table. columns : list     Column names. If not given, the assumption is that     iterator will dictionaries and column names are derived     from that. indices : list     List of column names to add indices on.</p> Source code in <code>cgatcore/pipeline/database.py</code> <pre><code>def load_from_iterator(\n        outfile,\n        tablename,\n        iterator,\n        columns=None,\n        indices=None):\n    '''import data from an iterator into a database.\n\n    Arguments\n    ---------\n    outfile : string\n        Output file name\n    tablename : string\n        Table name\n    iterator : iterator\n        Iterator to import data from. The iterator should\n        yield either list/tuples or dictionaries for each\n        row in the table.\n    columns : list\n        Column names. If not given, the assumption is that\n        iterator will dictionaries and column names are derived\n        from that.\n    indices : list\n        List of column names to add indices on.\n    '''\n\n    tmpfile = get_temp_file(\".\")\n\n    if columns:\n        keys, values = list(zip(*list(columns.items())))\n        tmpfile.write(\"\\t\".join(values) + \"\\n\")\n\n    for row in iterator:\n        if not columns:\n            keys = list(row[0].keys())\n            values = keys\n            columns = keys\n            tmpfile.write(\"\\t\".join(values) + \"\\n\")\n\n        tmpfile.write(\"\\t\".join(str(row[x]) for x in keys) + \"\\n\")\n\n    tmpfile.close()\n\n    if indices:\n        indices = \" \".join(\"--add-index=%s\" % x for x in indices)\n    else:\n        indices = \"\"\n\n    load(tmpfile.name,\n         outfile,\n         tablename=tablename,\n         options=indices)\n\n    os.unlink(tmpfile.name)\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.main","title":"<code>main(argv=None)</code>","text":"<p>command line control function for a pipeline.</p> <p>This method defines command line options for the pipeline and updates the global configuration dictionary correspondingly.</p> <p>It then provides a command parser to execute particular tasks using the ruffus pipeline control functions. See the generated command line help for usage.</p> <p>To use it, add::</p> <pre><code>import CGAT.pipeline as P\n\nif __name__ == \"__main__\":\n    sys.exit(P.main(sys.argv))\n</code></pre> <p>to your pipeline script.</p>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.main--arguments","title":"Arguments","text":"<p>args : list     List of command line arguments.</p> Source code in <code>cgatcore/pipeline/control.py</code> <pre><code>def main(argv=None):\n    \"\"\"command line control function for a pipeline.\n\n    This method defines command line options for the pipeline and\n    updates the global configuration dictionary correspondingly.\n\n    It then provides a command parser to execute particular tasks\n    using the ruffus pipeline control functions. See the generated\n    command line help for usage.\n\n    To use it, add::\n\n        import CGAT.pipeline as P\n\n        if __name__ == \"__main__\":\n            sys.exit(P.main(sys.argv))\n\n    to your pipeline script.\n\n    Arguments\n    ---------\n    args : list\n        List of command line arguments.\n\n    \"\"\"\n\n    if argv is None:\n        argv = sys.argv\n\n    if E.get_args() is None:\n        initialize(caller=get_caller().__file__)\n\n    args = E.get_args()\n\n    run_workflow(args)\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.match_parameter","title":"<code>match_parameter(param)</code>","text":"<p>find an exact match or prefix-match in the global configuration dictionary param.</p>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.match_parameter--arguments","title":"Arguments","text":"<p>param : string     Parameter to search for.</p>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.match_parameter--returns","title":"Returns","text":"<p>name : string     The full parameter name.</p>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.match_parameter--raises","title":"Raises","text":"<p>KeyError if param can't be matched.</p> Source code in <code>cgatcore/pipeline/parameters.py</code> <pre><code>def match_parameter(param):\n    '''find an exact match or prefix-match in the global\n    configuration dictionary param.\n\n    Arguments\n    ---------\n    param : string\n        Parameter to search for.\n\n    Returns\n    -------\n    name : string\n        The full parameter name.\n\n    Raises\n    ------\n    KeyError if param can't be matched.\n\n    '''\n    if param in PARAMS:\n        return param\n\n    for key in list(PARAMS.keys()):\n        if \"%\" in key:\n            rx = re.compile(re.sub(\"%\", \".*\", key))\n            if rx.search(param):\n                return key\n\n    raise KeyError(\"parameter '%s' can not be matched in dictionary\" %\n                   param)\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.merge_and_load","title":"<code>merge_and_load(infiles, outfile, suffix=None, columns=(0, 1), regex=None, row_wise=True, retry=True, options='', prefixes=None)</code>","text":"<p>merge multiple categorical tables and load into a database.</p> <p>The tables are merged and entered row-wise, i.e, the contents of each file are a row.</p> <p>For example, the statement::</p> <pre><code>mergeAndLoad(['file1.txt', 'file2.txt'],\n             \"test_table.load\")\n</code></pre> <p>with the two files::     &gt; cat file1.txt     Category    Result     length      12     width       100</p> <pre><code>&gt; cat file2.txt\nCategory    Result\nlength      20\nwidth       50\n</code></pre> <p>will be added into table <code>test_table</code> as::     track   length   width     file1   12       100     file2   20       50</p> <p>If row-wise is set::     mergeAndLoad(['file1.txt', 'file2.txt'],                  \"test_table.load\", row_wise=True)</p> <p><code>test_table</code> will be transposed and look like this::     track    file1 file2     length   12    20     width    20    50</p>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.merge_and_load--arguments","title":"Arguments","text":"<p>infiles : list     Filenames of the input data outfile : string     Output filename. This will contain the logging information. The     table name is derived from <code>outfile</code>. suffix : string     If <code>suffix</code> is given, the suffix will be removed from the filenames. columns : list     The columns to be taken. By default, the first two columns are     taken with the first being the key. Filenames are stored in a     <code>track</code> column. Directory names are chopped off.  If     <code>columns</code> is set to None, all columns will be taken. Here,     column names will receive a prefix given by <code>prefixes</code>. If     <code>prefixes</code> is None, the filename will be added as a prefix. regex : string     If set, the full filename will be used to extract a     track name via the supplied regular expression. row_wise : bool     If set to False, each table will be a column in the resulting     table.  This is useful if histograms are being merged. retry : bool     If True, multiple attempts will be made if the data can     not be loaded at the first try, for example if a table is locked. options : string     Command line options for the <code>csv2db.py</code> script. prefixes : list     If given, the respective prefix will be added to each     column. The number of <code>prefixes</code> and <code>infiles</code> needs to be the     same.</p> Source code in <code>cgatcore/pipeline/database.py</code> <pre><code>def merge_and_load(infiles,\n                   outfile,\n                   suffix=None,\n                   columns=(0, 1),\n                   regex=None,\n                   row_wise=True,\n                   retry=True,\n                   options=\"\",\n                   prefixes=None):\n    '''merge multiple categorical tables and load into a database.\n\n    The tables are merged and entered row-wise, i.e, the contents of\n    each file are a row.\n\n    For example, the statement::\n\n        mergeAndLoad(['file1.txt', 'file2.txt'],\n                     \"test_table.load\")\n\n    with the two files::\n        &gt; cat file1.txt\n        Category    Result\n        length      12\n        width       100\n\n        &gt; cat file2.txt\n        Category    Result\n        length      20\n        width       50\n\n    will be added into table ``test_table`` as::\n        track   length   width\n        file1   12       100\n        file2   20       50\n\n    If row-wise is set::\n        mergeAndLoad(['file1.txt', 'file2.txt'],\n                     \"test_table.load\", row_wise=True)\n\n    ``test_table`` will be transposed and look like this::\n        track    file1 file2\n        length   12    20\n        width    20    50\n\n    Arguments\n    ---------\n    infiles : list\n        Filenames of the input data\n    outfile : string\n        Output filename. This will contain the logging information. The\n        table name is derived from `outfile`.\n    suffix : string\n        If `suffix` is given, the suffix will be removed from the filenames.\n    columns : list\n        The columns to be taken. By default, the first two columns are\n        taken with the first being the key. Filenames are stored in a\n        ``track`` column. Directory names are chopped off.  If\n        `columns` is set to None, all columns will be taken. Here,\n        column names will receive a prefix given by `prefixes`. If\n        `prefixes` is None, the filename will be added as a prefix.\n    regex : string\n        If set, the full filename will be used to extract a\n        track name via the supplied regular expression.\n    row_wise : bool\n        If set to False, each table will be a column in the resulting\n        table.  This is useful if histograms are being merged.\n    retry : bool\n        If True, multiple attempts will be made if the data can\n        not be loaded at the first try, for example if a table is locked.\n    options : string\n        Command line options for the `csv2db.py` script.\n    prefixes : list\n        If given, the respective prefix will be added to each\n        column. The number of `prefixes` and `infiles` needs to be the\n        same.\n    '''\n    if len(infiles) == 0:\n        raise ValueError(\"no files for merging\")\n\n    if suffix:\n        header = \",\".join([os.path.basename(snip(x, suffix)) for x in infiles])\n    elif regex:\n        header = \",\".join([\"-\".join(re.search(regex, x).groups())\n                           for x in infiles])\n    else:\n        header = \",\".join([os.path.basename(x) for x in infiles])\n\n    header_stmt = \"--header-names=%s\" % header\n\n    if columns:\n        column_filter = \"| cut -f %s\" % \",\".join(map(str,\n                                                     [x + 1 for x in columns]))\n    else:\n        column_filter = \"\"\n        if prefixes:\n            assert len(prefixes) == len(infiles)\n            header_stmt = \"--prefixes=%s\" % \",\".join(prefixes)\n        else:\n            header_stmt = \"--add-file-prefix\"\n\n    if infiles[0].endswith(\".gz\"):\n        filenames = \" \".join(\n            [\"&lt;( zcat %s %s )\" % (x, column_filter) for x in infiles])\n    else:\n        filenames = \" \".join(\n            [\"&lt;( cat %s %s )\" % (x, column_filter) for x in infiles])\n\n    if row_wise:\n        transform = \"\"\"| perl -p -e \"s/bin/track/\"\n        | python -m cgatcore.table --transpose\"\"\"\n    else:\n        transform = \"\"\n\n    load_statement = build_load_statement(\n        to_table(outfile),\n        options=\"--add-index=track \" + options,\n        retry=retry)\n\n    statement = \"\"\"python -m cgatcore.tables\n    %(header_stmt)s\n    --skip-titles\n    --missing-value=0\n    --ignore-empty\n    %(filenames)s\n    %(transform)s\n    | %(load_statement)s\n    &gt; %(outfile)s\n    \"\"\"\n    run(statement)\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.nested_update","title":"<code>nested_update(old, new)</code>","text":"<p>Update potentially nested dictionaries. If both old[x] and new[x] inherit from collections.abc.Mapping, then update old[x] with entries from new[x], otherwise set old[x] to new[x]</p> Source code in <code>cgatcore/pipeline/parameters.py</code> <pre><code>def nested_update(old, new):\n    '''Update potentially nested dictionaries. If both old[x] and new[x]\n    inherit from collections.abc.Mapping, then update old[x] with entries from\n    new[x], otherwise set old[x] to new[x]'''\n\n    for key, value in new.items():\n        if isinstance(value, Mapping) and \\\n           isinstance(old.get(key, str()), Mapping):\n            old[key].update(new[key])\n        else:\n            old[key] = new[key]\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.parse_commandline","title":"<code>parse_commandline(argv=None, optparse=True, **kwargs)</code>","text":"<p>parse command line.</p> <p>Create option parser and parse command line.</p>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.parse_commandline--arguments","title":"Arguments","text":"<p>argv : list     List of command line options to parse. If None, use sys.argv.</p> <p>**kwargs: dict     Additional arguments overwrite default option settings.</p> Source code in <code>cgatcore/pipeline/control.py</code> <pre><code>def parse_commandline(argv=None, optparse=True, **kwargs):\n    \"\"\"parse command line.\n\n    Create option parser and parse command line.\n\n    Arguments\n    ---------\n    argv : list\n        List of command line options to parse. If None, use sys.argv.\n\n    **kwargs: dict\n        Additional arguments overwrite default option settings.\n\n    \"\"\"\n    if argv is None:\n        argv = sys.argv\n\n    if optparse is True:\n\n        parser = E.OptionParser(version=\"%prog version: $Id$\",\n                                usage=USAGE)\n\n        parser.add_option(\"--pipeline-action\", dest=\"pipeline_action\",\n                          type=\"choice\",\n                          choices=(\n                              \"make\", \"show\", \"plot\", \"dump\", \"config\",\n                              \"clone\", \"check\", \"regenerate\", \"state\",\n                              \"printconfig\", \"svg\"),\n                          help=\"action to take [default=%default].\")\n\n        parser.add_option(\"--pipeline-format\", dest=\"pipeline_format\",\n                          type=\"choice\",\n                          choices=(\"dot\", \"jpg\", \"svg\", \"ps\", \"png\"),\n                          help=\"pipeline format [default=%default].\")\n\n        parser.add_option(\"-n\", \"--dry-run\", dest=\"dry_run\",\n                          action=\"store_true\",\n                          help=\"perform a dry run (do not execute any shell \"\n                          \"commands) [default=%default].\")\n\n        parser.add_option(\"-c\", \"--config-file\", dest=\"config_file\",\n                          help=\"benchmark configuration file \"\n                          \"[default=%default].\")\n\n        parser.add_option(\"-f\", \"--force-run\", dest=\"force_run\",\n                          type=\"string\",\n                          help=\"force running the pipeline even if there are \"\n                          \"up-to-date tasks. If option is 'all', all tasks \"\n                          \"will be rerun. Otherwise, only the tasks given as \"\n                          \"arguments will be rerun. \"\n                          \"[default=%default].\")\n\n        parser.add_option(\"-p\", \"--multiprocess\", dest=\"multiprocess\", type=\"int\",\n                          help=\"number of parallel processes to use on \"\n                          \"submit host \"\n                          \"(different from number of jobs to use for \"\n                          \"cluster jobs) \"\n                          \"[default=%default].\")\n\n        parser.add_option(\"-e\", \"--exceptions\", dest=\"log_exceptions\",\n                          action=\"store_true\",\n                          help=\"echo exceptions immediately as they occur \"\n                          \"[default=%default].\")\n\n        parser.add_option(\"-i\", \"--terminate\", dest=\"terminate\",\n                          action=\"store_true\",\n                          help=\"terminate immediately at the first exception \"\n                          \"[default=%default].\")\n\n        parser.add_option(\"-d\", \"--debug\", dest=\"debug\",\n                          action=\"store_true\",\n                          help=\"output debugging information on console, \"\n                          \"and not the logfile \"\n                          \"[default=%default].\")\n\n        parser.add_option(\"-s\", \"--set\", dest=\"variables_to_set\",\n                          type=\"string\", action=\"append\",\n                          help=\"explicitely set paramater values \"\n                          \"[default=%default].\")\n\n        parser.add_option(\"--input-glob\", \"--input-glob\", dest=\"input_globs\",\n                          type=\"string\", action=\"append\",\n                          help=\"glob expression for input filenames. The exact format \"\n                          \"is pipeline specific. If the pipeline expects only a single input, \"\n                          \"`--input-glob=*.bam` will be sufficient. If the pipeline expects \"\n                          \"multiple types of input, a qualifier might need to be added, for example \"\n                          \"`--input-glob=bam=*.bam` --input-glob=bed=*.bed.gz`. Giving this option \"\n                          \"overrides the default of a pipeline looking for input in the current directory \"\n                          \"or specified the config file. \"\n                          \"[default=%default].\")\n\n        parser.add_option(\"--checksums\", dest=\"ruffus_checksums_level\",\n                          type=\"int\",\n                          help=\"set the level of ruffus checksums\"\n                          \"[default=%default].\")\n\n        parser.add_option(\"-t\", \"--is-test\", dest=\"is_test\",\n                          action=\"store_true\",\n                          help=\"this is a test run\"\n                          \"[default=%default].\")\n\n        parser.add_option(\"--engine\", dest=\"engine\",\n                          choices=(\"local\", \"arvados\"),\n                          help=\"engine to use.\"\n                          \"[default=%default].\")\n\n        parser.add_option(\n            \"--always-mount\", dest=\"always_mount\",\n            action=\"store_true\",\n            help=\"force mounting of arvados keep [%default]\")\n\n        parser.add_option(\"--only-info\", dest=\"only_info\",\n                          action=\"store_true\",\n                          help=\"only update meta information, do not run \"\n                          \"[default=%default].\")\n\n        parser.add_option(\"--work-dir\", dest=\"work_dir\",\n                          type=\"string\",\n                          help=\"working directory. Will be created if it does not exist \"\n                          \"[default=%default].\")\n\n        group = E.OptionGroup(parser, \"pipeline logging configuration\")\n\n        group.add_option(\"--pipeline-logfile\", dest=\"pipeline_logfile\",\n                         type=\"string\",\n                         help=\"primary logging destination.\"\n                         \"[default=%default].\")\n\n        group.add_option(\"--shell-logfile\", dest=\"shell_logfile\",\n                         type=\"string\",\n                         help=\"filename for shell debugging information. \"\n                         \"If it is not an absolute path, \"\n                         \"the output will be written into the current working \"\n                         \"directory. If unset, no logging will be output. \"\n                         \"[default=%default].\")\n\n        parser.add_option(\"--input-validation\", dest=\"input_validation\",\n                          action=\"store_true\",\n                          help=\"perform input validation before starting \"\n                          \"[default=%default].\")\n\n        parser.add_option_group(group)\n\n        parser.set_defaults(\n            pipeline_action=None,\n            pipeline_format=\"svg\",\n            pipeline_targets=[],\n            force_run=False,\n            multiprocess=None,\n            pipeline_logfile=\"pipeline.log\",\n            shell_logfile=None,\n            dry_run=False,\n            log_exceptions=True,\n            engine=\"local\",\n            exceptions_terminate_immediately=None,\n            debug=False,\n            variables_to_set=[],\n            is_test=False,\n            ruffus_checksums_level=0,\n            config_file=\"pipeline.yml\",\n            work_dir=None,\n            always_mount=False,\n            only_info=False,\n            input_globs=[],\n            input_validation=False)\n\n        parser.set_defaults(**kwargs)\n\n        if \"callback\" in kwargs:\n            kwargs[\"callback\"](parser)\n\n        logger_callback = setup_logging\n        (options, args) = E.start(\n            parser,\n            add_cluster_options=True,\n            argv=argv,\n            logger_callback=logger_callback)\n        options.pipeline_name = argv[0]\n        if args:\n            options.pipeline_action = args[0]\n            options.pipeline_targets = args[1:]\n\n    else:\n        parser = E.ArgumentParser(description=USAGE)\n\n        parser.add_argument(\"--pipeline-action\", dest=\"pipeline_action\",\n                            type=str,\n                            choices=(\n                                \"make\", \"show\", \"plot\", \"dump\", \"config\",\n                                \"clone\", \"check\", \"regenerate\", \"state\",\n                                \"printconfig\", \"svg\"),\n                            help=\"action to take.\")\n\n        parser.add_argument(\"--pipeline-format\", dest=\"pipeline_format\",\n                            type=str,\n                            choices=(\"dot\", \"jpg\", \"svg\", \"ps\", \"png\"),\n                            help=\"pipeline format.\")\n\n        parser.add_argument(\"-n\", \"--dry-run\", dest=\"dry_run\",\n                            action=\"store_true\",\n                            help=\"perform a dry run (do not execute any shell \"\n                            \"commands).\")\n\n        parser.add_argument(\"-c\", \"--config-file\", dest=\"config_file\",\n                            help=\"benchmark configuration file \")\n\n        parser.add_argument(\"-f\", \"--force-run\", dest=\"force_run\",\n                            type=str,\n                            help=\"force running the pipeline even if there are \"\n                            \"up-to-date tasks. If option is 'all', all tasks \"\n                            \"will be rerun. Otherwise, only the tasks given as \"\n                            \"arguments will be rerun. \")\n\n        parser.add_argument(\"-p\", \"--multiprocess\", dest=\"multiprocess\", type=int,\n                            help=\"number of parallel processes to use on \"\n                            \"submit host \"\n                            \"(different from number of jobs to use for \"\n                            \"cluster jobs) \")\n\n        parser.add_argument(\"-e\", \"--exceptions\", dest=\"log_exceptions\",\n                            action=\"store_true\",\n                            help=\"echo exceptions immediately as they occur \")\n\n        parser.add_argument(\"-i\", \"--terminate\", dest=\"terminate\",\n                            action=\"store_true\",\n                            help=\"terminate immediately at the first exception\")\n\n        parser.add_argument(\"-d\", \"--debug\", dest=\"debug\",\n                            action=\"store_true\",\n                            help=\"output debugging information on console, \"\n                            \"and not the logfile \")\n\n        parser.add_argument(\"-s\", \"--set\", dest=\"variables_to_set\",\n                            type=str, action=\"append\",\n                            help=\"explicitely set paramater values \")\n\n        parser.add_argument(\"--input-glob\", \"--input-glob\", dest=\"input_globs\",\n                            type=str, action=\"append\",\n                            help=\"glob expression for input filenames. The exact format \"\n                            \"is pipeline specific. If the pipeline expects only a single input, \"\n                            \"`--input-glob=*.bam` will be sufficient. If the pipeline expects \"\n                            \"multiple types of input, a qualifier might need to be added, for example \"\n                            \"`--input-glob=bam=*.bam` --input-glob=bed=*.bed.gz`. Giving this option \"\n                            \"overrides the default of a pipeline looking for input in the current directory \"\n                            \"or specified the config file.\")\n\n        parser.add_argument(\"--checksums\", dest=\"ruffus_checksums_level\",\n                            type=int,\n                            help=\"set the level of ruffus checksums\")\n\n        parser.add_argument(\"-t\", \"--is-test\", dest=\"is_test\",\n                            action=\"store_true\",\n                            help=\"this is a test run\")\n\n        parser.add_argument(\"--engine\", dest=\"engine\",\n                            type=str,\n                            choices=(\"local\", \"arvados\"),\n                            help=\"engine to use.\")\n\n        parser.add_argument(\n            \"--always-mount\", dest=\"always_mount\",\n            action=\"store_true\",\n            help=\"force mounting of arvados keep\")\n\n        parser.add_argument(\"--only-info\", dest=\"only_info\",\n                            action=\"store_true\",\n                            help=\"only update meta information, do not run\")\n\n        parser.add_argument(\"--work-dir\", dest=\"work_dir\",\n                            type=str,\n                            help=\"working directory. Will be created if it does not exist\")\n\n        parser.add_argument(\"--cleanup-on-fail\", action=\"store_true\", default=True,\n                            help=\"Enable cleanup of jobs on pipeline failure.\")\n\n        group = parser.add_argument_group(\"pipeline logging configuration\")\n\n        group.add_argument(\"--pipeline-logfile\", dest=\"pipeline_logfile\",\n                           type=str,\n                           help=\"primary logging destination.\")\n\n        group.add_argument(\"--shell-logfile\", dest=\"shell_logfile\",\n                           type=str,\n                           help=\"filename for shell debugging information. \"\n                           \"If it is not an absolute path, \"\n                           \"the output will be written into the current working \"\n                           \"directory. If unset, no logging will be output.\")\n\n        group.add_argument(\"--input-validation\", dest=\"input_validation\",\n                           action=\"store_true\",\n                           help=\"perform input validation before starting\")\n\n        parser.set_defaults(\n            pipeline_action=None,\n            pipeline_format=\"svg\",\n            pipeline_targets=[],\n            force_run=False,\n            multiprocess=None,\n            pipeline_logfile=\"pipeline.log\",\n            shell_logfile=None,\n            dry_run=False,\n            log_exceptions=True,\n            engine=\"local\",\n            exceptions_terminate_immediately=None,\n            debug=False,\n            variables_to_set=[],\n            is_test=False,\n            ruffus_checksums_level=0,\n            config_file=\"pipeline.yml\",\n            work_dir=None,\n            always_mount=False,\n            only_info=False,\n            input_globs=[],\n            input_validation=False)\n\n        parser.set_defaults(**kwargs)\n\n        if \"callback\" in kwargs:\n            kwargs[\"callback\"](parser)\n\n        logger_callback = setup_logging\n        args, unknown = E.start(\n            parser,\n            add_cluster_options=True,\n            argv=argv,\n            logger_callback=logger_callback,\n            unknowns=True)\n\n        args.pipeline_name = argv[0]\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.peek_parameters","title":"<code>peek_parameters(workingdir, pipeline, on_error_raise=None, prefix=None, update_interface=False, restrict_interface=False)</code>","text":"<p>peek configuration parameters from external pipeline.</p> <p>As the paramater dictionary is built at runtime, this method executes the pipeline in workingdir, dumping its configuration values and reading them into a dictionary.</p> <p>If either <code>pipeline</code> or <code>workingdir</code> are not found, an error is raised. This behaviour can be changed by setting <code>on_error_raise</code> to False. In that case, an empty dictionary is returned.</p>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.peek_parameters--arguments","title":"Arguments","text":"<p>workingdir : string    Working directory. This is the directory that the pipeline    was executed in. pipeline : string    Name of the pipeline script. The pipeline is assumed to live    in the same directory as the current pipeline. on_error_raise : Bool    If set to a boolean, an error will be raised (or not) if there    is an error during parameter peeking, for example if    <code>workingdir</code> can not be found. If <code>on_error_raise</code> is None, it    will be set to the default, which is to raise an exception    unless the calling script is imported or the option    <code>--is-test</code> has been passed at the command line. prefix : string    Add a prefix to all parameters. This is useful if the paramaters    are added to the configuration dictionary of the calling pipeline. update_interface : bool    If True, this method will prefix any options in the    <code>[interface]</code> section with <code>workingdir</code>. This allows    transparent access to files in the external pipeline. restrict_interface : bool    If  True, only interface parameters will be imported.</p>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.peek_parameters--returns","title":"Returns","text":"<p>config : dict     Dictionary of configuration values.</p> Source code in <code>cgatcore/pipeline/control.py</code> <pre><code>def peek_parameters(workingdir,\n                    pipeline,\n                    on_error_raise=None,\n                    prefix=None,\n                    update_interface=False,\n                    restrict_interface=False):\n    '''peek configuration parameters from external pipeline.\n\n    As the paramater dictionary is built at runtime, this method\n    executes the pipeline in workingdir, dumping its configuration\n    values and reading them into a dictionary.\n\n    If either `pipeline` or `workingdir` are not found, an error is\n    raised. This behaviour can be changed by setting `on_error_raise`\n    to False. In that case, an empty dictionary is returned.\n\n    Arguments\n    ---------\n    workingdir : string\n       Working directory. This is the directory that the pipeline\n       was executed in.\n    pipeline : string\n       Name of the pipeline script. The pipeline is assumed to live\n       in the same directory as the current pipeline.\n    on_error_raise : Bool\n       If set to a boolean, an error will be raised (or not) if there\n       is an error during parameter peeking, for example if\n       `workingdir` can not be found. If `on_error_raise` is None, it\n       will be set to the default, which is to raise an exception\n       unless the calling script is imported or the option\n       ``--is-test`` has been passed at the command line.\n    prefix : string\n       Add a prefix to all parameters. This is useful if the paramaters\n       are added to the configuration dictionary of the calling pipeline.\n    update_interface : bool\n       If True, this method will prefix any options in the\n       ``[interface]`` section with `workingdir`. This allows\n       transparent access to files in the external pipeline.\n    restrict_interface : bool\n       If  True, only interface parameters will be imported.\n\n    Returns\n    -------\n    config : dict\n        Dictionary of configuration values.\n\n    '''\n    caller_locals = get_caller_locals()\n\n    # check if we should raise errors\n    if on_error_raise is None:\n        on_error_raise = not is_test() and \\\n            \"__name__\" in caller_locals and \\\n            caller_locals[\"__name__\"] == \"__main__\"\n\n    # patch - if --help or -h in command line arguments,\n    # do not peek as there might be no config file.\n    if \"--help\" in sys.argv or \"-h\" in sys.argv:\n        return {}\n\n    if workingdir == \"\":\n        workingdir = os.path.abspath(\".\")\n\n    # patch for the \"config\" target - use default\n    # pipeline directory if directory is not specified\n    # working dir is set to \"?!\"\n    if (\"config\" in sys.argv or \"check\" in sys.argv or \"clone\" in sys.argv and workingdir == \"?!\"):\n        workingdir = os.path.join(get_params()[\"pipelinedir\"],\n                                  \"pipeline_\" + pipeline)\n\n    if not os.path.exists(workingdir):\n        if on_error_raise:\n            raise ValueError(\n                \"can't find working dir %s\" % workingdir)\n        else:\n            return {}\n\n    statement = \"cgatflow {} dump -v 0\".format(pipeline)\n\n    os.environ.update(\n        {'BASH_ENV': os.path.join(os.environ['HOME'], '.bashrc')})\n    process = subprocess.Popen(statement,\n                               cwd=workingdir,\n                               shell=True,\n                               stdin=subprocess.PIPE,\n                               stdout=subprocess.PIPE,\n                               stderr=subprocess.PIPE,\n                               env=os.environ.copy())\n\n    # process.stdin.close()\n    stdout, stderr = process.communicate()\n    if process.returncode != 0:\n        raise OSError(\n            (\"Child was terminated by signal %i: \\n\"\n             \"Statement: %s\\n\"\n             \"The stderr was: \\n%s\\n\"\n             \"Stdout: %s\") %\n            (-process.returncode, statement, stderr, stdout))\n\n    # subprocess only accepts encoding argument in py &gt;= 3.6 so\n    # decode here.\n    stdout = stdout.decode(\"utf-8\").splitlines()\n    # remove any log messages\n    stdout = [x for x in stdout if x.startswith(\"{\")]\n    if len(stdout) &gt; 1:\n        raise ValueError(\"received multiple configurations\")\n    dump = json.loads(stdout[0])\n\n    # update interface\n    if update_interface:\n        for key, value in list(dump.items()):\n            if key.startswith(\"interface\"):\n                if isinstance(value, str):\n                    dump[key] = os.path.join(workingdir, value)\n                elif isinstance(value, Mapping):\n                    for kkey, vvalue in list(value.items()):\n                        value[key] = os.path.join(workingdir, vvalue)\n\n    # keep only interface if so required\n    if restrict_interface:\n        dump = dict([(k, v) for k, v in dump.items()\n                     if k.startswith(\"interface\")])\n\n    # prefix all parameters\n    if prefix is not None:\n        dump = dict([(\"%s%s\" % (prefix, x), y) for x, y in list(dump.items())])\n\n    return dump\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.print_config_files","title":"<code>print_config_files()</code>","text":"<p>Print the list of .ini files used to configure the pipeline along with their associated priorities. Priority 1 is the highest.</p> Source code in <code>cgatcore/pipeline/control.py</code> <pre><code>def print_config_files():\n    '''\n        Print the list of .ini files used to configure the pipeline\n        along with their associated priorities.\n        Priority 1 is the highest.\n    '''\n\n    filenames = get_params()['pipeline_yml']\n    print(\"\\n List of .yml files used to configure the pipeline\")\n    s = len(filenames)\n    if s == 0:\n        print(\" No yml files passed!\")\n    elif s &gt;= 1:\n        print(\" %-11s: %s \" % (\"Priority\", \"File\"))\n        for f in filenames:\n            if s == 1:\n                print(\" (highest) %s: %s\\n\" % (s, f))\n            else:\n                print(\" %-11s: %s \" % (s, f))\n            s -= 1\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.run","title":"<code>run(statement, **kwargs)</code>","text":"<p>run a command line statement.</p> <p>This function runs a single or multiple statements either locally or on the cluster using drmaa. How a statement is executed or how it is modified depends on the context.</p> <p>The context is provided by keyword arguments provided as named function arguments ('kwargs') but also from defaults (see below). The following keyword arguments are recognized:</p> <p>job_memory     memory to use for the job per thread. Memory specification should be in a     format that is accepted by the job scheduler. Note that memory     is per thread. If you have 6 threads and the total memory is     6Gb, use 1G as job_memory. job_total_memory     total memory to use for a job. This will be divided by the number of     threads. job_threads     number of threads to request for the job. job_options     options to the job scheduler. job_condaenv     conda environment to use for the job. job_array     if set, run statement as an array job. Job_array should be     tuple with start, end, and increment.</p> <p>In addition, any additional variables will be used to interpolate the command line string using python's '%' string interpolation operator.</p> <p>The context is build in a hierarchical manner with successive operations overwriting previous values.</p> <ol> <li>Global variables    The context is initialized    with system-wide defaults stored in the global PARAMS    singleton.</li> <li>Context of caller    The context of the calling function is examined    and any local variables defined in this context are added.</li> <li>kwargs    Any options given explicitely as options to the run() method    are added.</li> <li>params    If the context of the calling function contains a params    variable, its contents are added to the context. This permits    setting variables in configuration files in TaskLibrary    functions.</li> </ol> <p>By default, a job is sent to the cluster, unless:</p> <pre><code>* ``to_cluster`` is present and set to None.\n\n* ``without_cluster`` is True.\n\n* ``--local`` has been specified on the command line\n  and the option ``without_cluster`` has been set as\n  a result.\n\n* no libdrmaa is present\n\n* the global session is not initialized (GLOBAL_SESSION is\n  None)\n</code></pre> <p>Troubleshooting:</p> <ol> <li> <p>DRMAA creates sessions and their is a limited number       of sessions available. If there are two many or sessions       become not available after failed jobs, use <code>qconf -secl</code>       to list sessions and <code>qconf -kec #</code> to delete sessions.</p> </li> <li> <p>Memory: 1G of free memory can be requested using the job_memory       variable: <code>job_memory = \"1G\"</code>       If there are error messages like \"no available queue\", then the       problem could be that a particular complex attribute has       not been defined (the code should be <code>hc</code> for <code>host:complex</code>       and not <code>hl</code> for <code>host:local</code>. Note that qrsh/qsub directly       still works.</p> </li> </ol> <p>The job will be executed within PARAMS[\"work_dir\"], unless PARAMS[\"work_dir\"] is not local. In that case, the job will be executed in a shared temporary directory.</p>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.run--arguments","title":"Arguments","text":"<p>statement : string or list of strings     A command line statement or a list of command line statements     to be executed. kwargs : dictionary     Context for job. The context is used to interpolate the command     line statement.</p> Source code in <code>cgatcore/pipeline/execution.py</code> <pre><code>def run(statement, **kwargs):\n    \"\"\"run a command line statement.\n\n    This function runs a single or multiple statements either locally\n    or on the cluster using drmaa. How a statement is executed or how\n    it is modified depends on the context.\n\n    The context is provided by keyword arguments provided as named\n    function arguments ('kwargs') but also from defaults (see\n    below). The following keyword arguments are recognized:\n\n    job_memory\n        memory to use for the job per thread. Memory specification should be in a\n        format that is accepted by the job scheduler. Note that memory\n        is per thread. If you have 6 threads and the total memory is\n        6Gb, use 1G as job_memory.\n    job_total_memory\n        total memory to use for a job. This will be divided by the number of\n        threads.\n    job_threads\n        number of threads to request for the job.\n    job_options\n        options to the job scheduler.\n    job_condaenv\n        conda environment to use for the job.\n    job_array\n        if set, run statement as an array job. Job_array should be\n        tuple with start, end, and increment.\n\n    In addition, any additional variables will be used to interpolate\n    the command line string using python's '%' string interpolation\n    operator.\n\n    The context is build in a hierarchical manner with successive\n    operations overwriting previous values.\n\n    1. Global variables\n       The context is initialized\n       with system-wide defaults stored in the global PARAMS\n       singleton.\n    2. Context of caller\n       The context of the calling function is examined\n       and any local variables defined in this context are added.\n    3. kwargs\n       Any options given explicitely as options to the run() method\n       are added.\n    4. params\n       If the context of the calling function contains a params\n       variable, its contents are added to the context. This permits\n       setting variables in configuration files in TaskLibrary\n       functions.\n\n    By default, a job is sent to the cluster, unless:\n\n        * ``to_cluster`` is present and set to None.\n\n        * ``without_cluster`` is True.\n\n        * ``--local`` has been specified on the command line\n          and the option ``without_cluster`` has been set as\n          a result.\n\n        * no libdrmaa is present\n\n        * the global session is not initialized (GLOBAL_SESSION is\n          None)\n\n    Troubleshooting:\n\n       1. DRMAA creates sessions and their is a limited number\n          of sessions available. If there are two many or sessions\n          become not available after failed jobs, use ``qconf -secl``\n          to list sessions and ``qconf -kec #`` to delete sessions.\n\n       2. Memory: 1G of free memory can be requested using the job_memory\n          variable: ``job_memory = \"1G\"``\n          If there are error messages like \"no available queue\", then the\n          problem could be that a particular complex attribute has\n          not been defined (the code should be ``hc`` for ``host:complex``\n          and not ``hl`` for ``host:local``. Note that qrsh/qsub directly\n          still works.\n\n    The job will be executed within PARAMS[\"work_dir\"], unless\n    PARAMS[\"work_dir\"] is not local. In that case, the job will\n    be executed in a shared temporary directory.\n\n    Arguments\n    ---------\n    statement : string or list of strings\n        A command line statement or a list of command line statements\n        to be executed.\n    kwargs : dictionary\n        Context for job. The context is used to interpolate the command\n        line statement.\n\n    \"\"\"\n    logger = get_logger()\n\n    # Combine options using priority\n    options = dict(list(get_params().items()))\n    caller_options = get_caller_locals()\n    options.update(list(caller_options.items()))\n\n    if \"self\" in options:\n        del options[\"self\"]\n    options.update(list(kwargs.items()))\n\n    # Inject params named tuple from TaskLibrary functions into option\n    # dict. This allows overriding options set in the code with options set\n    # in a .yml file\n    if \"params\" in options:\n        try:\n            options.update(options[\"params\"]._asdict())\n        except AttributeError:\n            pass\n\n    # Insert parameters supplied through simplified interface such\n    # as job_memory, job_options, job_queue\n    options['cluster']['options'] = options.get(\n        'job_options', options['cluster']['options'])\n    options['cluster']['queue'] = options.get(\n        'job_queue', options['cluster']['queue'])\n    options['without_cluster'] = options.get('without_cluster')\n\n    # SGE compatible job_name\n    name_substrate = str(options.get(\"outfile\", \"cgatcore\"))\n    if os.path.basename(name_substrate).startswith(\"result\"):\n        name_substrate = os.path.basename(os.path.dirname(name_substrate))\n    else:\n        name_substrate = os.path.basename(name_substrate)\n\n    options[\"job_name\"] = re.sub(\"[:]\", \"_\", name_substrate)\n    try:\n        calling_module = get_caller().__name__\n    except AttributeError:\n        calling_module = \"unknown\"\n\n    options[\"task_name\"] = calling_module + \".\" + get_calling_function()\n\n    # Build statements using parameter interpolation\n    if isinstance(statement, list):\n        statement_list = [interpolate_statement(stmt, options) for stmt in statement]\n    else:\n        statement_list = [interpolate_statement(statement, options)]\n\n    if len(statement_list) == 0:\n        logger.warn(\"No statements found - no execution\")\n        return []\n\n    if options.get(\"dryrun\", False):\n        for statement in statement_list:\n            logger.info(\"Dry-run: {}\".format(statement))\n        return []\n\n    # Use get_executor to get the appropriate executor\n    executor = get_executor(options)  # Updated to use get_executor\n\n    # Execute statement list within the context of the executor\n    with executor as e:\n        benchmark_data = e.run(statement_list)\n\n    # Log benchmark data\n    for data in benchmark_data:\n        logger.info(json.dumps(data))\n\n    BenchmarkData = collections.namedtuple('BenchmarkData', sorted(benchmark_data[0]))\n    return [BenchmarkData(**d) for d in benchmark_data]\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.run_pickled","title":"<code>run_pickled(params)</code>","text":"<p>run a function whose arguments have been pickled.</p> <p>expects that params is [module_name, function_name, arguments_file]</p> Source code in <code>cgatcore/pipeline/execution.py</code> <pre><code>def run_pickled(params):\n    ''' run a function whose arguments have been pickled.\n\n    expects that params is [module_name, function_name, arguments_file] '''\n\n    module_name, func_name, args_file = params\n    location = os.path.dirname(module_name)\n    if location != \"\":\n        sys.path.append(location)\n\n    module_base_name = os.path.basename(module_name)\n    logger = get_logger()\n    logger.info(\"importing module '%s' \" % module_base_name)\n    logger.debug(\"sys.path is: %s\" % sys.path)\n\n    module = importlib.import_module(module_base_name)\n    try:\n        function = getattr(module, func_name)\n    except AttributeError as msg:\n        raise AttributeError(msg.message\n                             + \"unknown function, available functions are: %s\" %\n                             \",\".join([x for x in dir(module)\n                                       if not x.startswith(\"_\")]))\n\n    args, kwargs = pickle.load(open(args_file, \"rb\"))\n    logger.info(\"arguments = %s\" % str(args))\n    logger.info(\"keyword arguments = %s\" % str(kwargs))\n\n    function(*args, **kwargs)\n\n    os.unlink(args_file)\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.shellquote","title":"<code>shellquote(statement)</code>","text":"<p>shell quote a string to be used as a function argument.</p> <p>from http://stackoverflow.com/questions/967443/ python-module-to-shellquote-unshellquote</p> Source code in <code>cgatcore/pipeline/execution.py</code> <pre><code>def shellquote(statement):\n    '''shell quote a string to be used as a function argument.\n\n    from http://stackoverflow.com/questions/967443/\n    python-module-to-shellquote-unshellquote\n    '''\n    _quote_pos = re.compile('(?=[^-0-9a-zA-Z_./\\n])')\n\n    if statement:\n        return _quote_pos.sub('\\\\\\\\', statement).replace('\\n', \"'\\n'\")\n    else:\n        return \"''\"\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.snip","title":"<code>snip(filename, extension=None, alt_extension=None, strip_path=False)</code>","text":"<p>return prefix of <code>filename</code>, that is the part without the extension.</p> <p>If <code>extension</code> is given, make sure that filename has the extension (or <code>alt_extension</code>). Both extension or alt_extension can be list of extensions.</p> <p>If <code>strip_path</code> is set to true, the path is stripped from the file name.</p> Source code in <code>cgatcore/iotools.py</code> <pre><code>def snip(filename, extension=None, alt_extension=None,\n         strip_path=False):\n    '''return prefix of `filename`, that is the part without the\n    extension.\n\n    If `extension` is given, make sure that filename has the\n    extension (or `alt_extension`). Both extension or alt_extension\n    can be list of extensions.\n\n    If `strip_path` is set to true, the path is stripped from the file\n    name.\n\n    '''\n    if extension is None:\n        extension = []\n    elif isinstance(extension, str):\n        extension = [extension]\n\n    if alt_extension is None:\n        alt_extension = []\n    elif isinstance(alt_extension, str):\n        alt_extension = [alt_extension]\n\n    if extension:\n        for ext in extension + alt_extension:\n            if filename.endswith(ext):\n                root = filename[:-len(ext)]\n                break\n        else:\n            raise ValueError(\"'%s' expected to end in '%s'\" %\n                             (filename, \",\".join(\n                                 extension + alt_extension)))\n    else:\n        root, ext = os.path.splitext(filename)\n\n    if strip_path:\n        snipped = os.path.basename(root)\n    else:\n        snipped = root\n\n    return snipped\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.start_session","title":"<code>start_session()</code>","text":"<p>start and initialize the global DRMAA session.</p> Source code in <code>cgatcore/pipeline/execution.py</code> <pre><code>def start_session():\n    \"\"\"start and initialize the global DRMAA session.\"\"\"\n    global GLOBAL_SESSION\n\n    if HAS_DRMAA and GLOBAL_SESSION is None:\n        GLOBAL_SESSION = drmaa.Session()\n        try:\n            GLOBAL_SESSION.initialize()\n        except drmaa.errors.InternalException as ex:\n            get_logger().warn(\"could not initialize global drmaa session: {}\".format(\n                ex))\n            GLOBAL_SESSION = None\n        return GLOBAL_SESSION\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.submit","title":"<code>submit(module, function, args=None, infiles=None, outfiles=None, to_cluster=True, logfile=None, job_options='', job_threads=1, job_memory=False)</code>","text":"<p>submit a python function as a job to the cluster.</p> <p>This method runs the script :file:<code>run_function</code> using the :func:<code>run</code> method in this module thus providing the same control options as for command line tools.</p>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.submit--arguments","title":"Arguments","text":"<p>module : string     Module name that contains the function. If <code>module</code> is     not part of the PYTHONPATH, an absolute path can be given. function : string     Name of function to execute infiles : string or list     Filenames of input data outfiles : string or list     Filenames of output data logfile : filename     Logfile to provide to the <code>--log</code> option job_options : string     String for generic job options for the queuing system job_threads : int     Number of slots (threads/cores/CPU) to use for the task job_memory : string     Amount of memory to reserve for the job.</p> Source code in <code>cgatcore/pipeline/execution.py</code> <pre><code>def submit(module,\n           function,\n           args=None,\n           infiles=None,\n           outfiles=None,\n           to_cluster=True,\n           logfile=None,\n           job_options=\"\",\n           job_threads=1,\n           job_memory=False):\n    '''submit a python *function* as a job to the cluster.\n\n    This method runs the script :file:`run_function` using the\n    :func:`run` method in this module thus providing the same\n    control options as for command line tools.\n\n    Arguments\n    ---------\n    module : string\n        Module name that contains the function. If `module` is\n        not part of the PYTHONPATH, an absolute path can be given.\n    function : string\n        Name of function to execute\n    infiles : string or list\n        Filenames of input data\n    outfiles : string or list\n        Filenames of output data\n    logfile : filename\n        Logfile to provide to the ``--log`` option\n    job_options : string\n        String for generic job options for the queuing system\n    job_threads : int\n        Number of slots (threads/cores/CPU) to use for the task\n    job_memory : string\n        Amount of memory to reserve for the job.\n\n    '''\n\n    if not job_memory:\n        job_memory = get_params().get(\"cluster_memory_default\", \"2G\")\n\n    if type(infiles) in (list, tuple):\n        infiles = \" \".join([\"--input=%s\" % x for x in infiles])\n    else:\n        infiles = \"--input=%s\" % infiles\n\n    if type(outfiles) in (list, tuple):\n        outfiles = \" \".join([\"--output-section=%s\" % x for x in outfiles])\n    else:\n        outfiles = \"--output-section=%s\" % outfiles\n\n    if logfile:\n        logfile = \"--log=%s\" % logfile\n    else:\n        logfile = \"\"\n\n    if args:\n        args = \"--args=%s\" % \",\".join(args)\n    else:\n        args = \"\"\n\n    statement = (\n        \"python -m cgatcore.pipeline.run_function \"\n        \"--module=%(module)s \"\n        \"--function=%(function)s \"\n        \"%(logfile)s \"\n        \"%(infiles)s \"\n        \"%(outfiles)s \"\n        \"%(args)s\")\n    run(statement)\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.substitute_parameters","title":"<code>substitute_parameters(**kwargs)</code>","text":"<p>return a parameter dictionary.</p> <p>This method builds a dictionary of parameter values to apply for a specific task. The dictionary is built in the following order:</p> <ol> <li>take values from the global dictionary (:py:data:<code>PARAMS</code>)</li> <li>substitute values appearing in <code>kwargs</code>.</li> <li>Apply task specific configuration values by looking for the    presence of <code>outfile</code> in kwargs.</li> </ol> <p>The substition of task specific values works by looking for any parameter values starting with the value of <code>outfile</code>.  The suffix of the parameter value will then be substituted.</p> <p>For example::</p> <pre><code>PARAMS = {\"tophat_threads\": 4,\n          \"tophat_cutoff\": 0.5,\n          \"sample1.bam.gz_tophat_threads\" : 6}\noutfile = \"sample1.bam.gz\"\nprint(substitute_parameters(**locals()))\n{\"tophat_cutoff\": 0.5, \"tophat_threads\": 6}\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.substitute_parameters--returns","title":"Returns","text":"<p>params : dict     Dictionary with parameter values.</p> Source code in <code>cgatcore/pipeline/parameters.py</code> <pre><code>def substitute_parameters(**kwargs):\n    '''return a parameter dictionary.\n\n    This method builds a dictionary of parameter values to\n    apply for a specific task. The dictionary is built in\n    the following order:\n\n    1. take values from the global dictionary (:py:data:`PARAMS`)\n    2. substitute values appearing in `kwargs`.\n    3. Apply task specific configuration values by looking for the\n       presence of ``outfile`` in kwargs.\n\n    The substition of task specific values works by looking for any\n    parameter values starting with the value of ``outfile``.  The\n    suffix of the parameter value will then be substituted.\n\n    For example::\n\n        PARAMS = {\"tophat_threads\": 4,\n                  \"tophat_cutoff\": 0.5,\n                  \"sample1.bam.gz_tophat_threads\" : 6}\n        outfile = \"sample1.bam.gz\"\n        print(substitute_parameters(**locals()))\n        {\"tophat_cutoff\": 0.5, \"tophat_threads\": 6}\n\n    Returns\n    -------\n    params : dict\n        Dictionary with parameter values.\n\n    '''\n\n    # build parameter dictionary\n    # note the order of addition to make sure that kwargs takes precedence\n    local_params = dict(list(PARAMS.items()) + list(kwargs.items()))\n\n    if \"outfile\" in local_params:\n        # replace specific parameters with task (outfile) specific parameters\n        outfile = local_params[\"outfile\"]\n        keys = list(local_params.keys())\n        for k in keys:\n            if k.startswith(outfile):\n                p = k[len(outfile) + 1:]\n                if p not in local_params:\n                    # do not raise error, argument might be a prefix\n                    continue\n                get_logger.debug(\"substituting task specific parameter \"\n                                 \"for %s: %s = %s\" %\n                                 (outfile, p, local_params[k]))\n                local_params[p] = local_params[k]\n\n    return local_params\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.tablequote","title":"<code>tablequote(track)</code>","text":"<p>quote a track name such that is suitable as a table name.</p> Source code in <code>cgatcore/pipeline/database.py</code> <pre><code>def tablequote(track):\n    '''quote a track name such that is suitable as a table name.'''\n    return re.sub(r\"[-(),\\[\\].]\", \"_\", track)\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.to_table","title":"<code>to_table(outfile)</code>","text":"<p>convert a filename from a load statement into a table name.</p> <p>This method checks if the filename ends with \".load\". The suffix is then removed and the filename quoted so that it is suitable as a table name.</p>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.to_table--arguments","title":"Arguments","text":"<p>outfile : string     A filename ending in \".load\".</p>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.to_table--returns","title":"Returns","text":"<p>tablename : string</p> Source code in <code>cgatcore/pipeline/database.py</code> <pre><code>def to_table(outfile):\n    '''convert a filename from a load statement into a table name.\n\n    This method checks if the filename ends with \".load\". The suffix\n    is then removed and the filename quoted so that it is suitable\n    as a table name.\n\n    Arguments\n    ---------\n    outfile : string\n        A filename ending in \".load\".\n\n    Returns\n    -------\n    tablename : string\n\n    '''\n    assert outfile.endswith(\".load\")\n    name = os.path.basename(outfile[:-len(\".load\")])\n    return tablequote(name)\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.touch_file","title":"<code>touch_file(filename, mode=438, times=None, dir_fd=None, ref=None, **kwargs)</code>","text":"<p>update/create a sentinel file.</p> <p>modified from: https://stackoverflow.com/questions/1158076/implement-touch-using-python</p> <p>Compressed files (ending in .gz) are created as empty 'gzip' files, i.e., with a header.</p> Source code in <code>cgatcore/iotools.py</code> <pre><code>def touch_file(filename, mode=0o666, times=None, dir_fd=None, ref=None, **kwargs):\n    '''update/create a sentinel file.\n\n    modified from: https://stackoverflow.com/questions/1158076/implement-touch-using-python\n\n    Compressed files (ending in .gz) are created as empty 'gzip'\n    files, i.e., with a header.\n\n    '''\n    flags = os.O_CREAT | os.O_APPEND\n    existed = os.path.exists(filename)\n\n    if filename.endswith(\".gz\") and not existed:\n        # this will automatically add a gzip header\n        with gzip.GzipFile(filename, \"w\") as fhandle:\n            pass\n\n    if ref:\n        stattime = os.stat(ref)\n        times = (stattime.st_atime, stattime.st_mtime)\n\n    with os.fdopen(os.open(\n            filename, flags=flags, mode=mode, dir_fd=dir_fd)) as fhandle:\n        os.utime(\n            fhandle.fileno() if os.utime in os.supports_fd else filename,\n            dir_fd=None if os.supports_fd else dir_fd,\n            **kwargs)\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.update_params_with_commandline_options","title":"<code>update_params_with_commandline_options(params, args)</code>","text":"<p>add and update selected parameters in the parameter dictionary with command line args.</p> Source code in <code>cgatcore/pipeline/control.py</code> <pre><code>def update_params_with_commandline_options(params, args):\n    \"\"\"add and update selected parameters in the parameter\n    dictionary with command line args.\n    \"\"\"\n\n    params[\"pipeline_name\"] = args.pipeline_name\n    params[\"dryrun\"] = args.dry_run\n\n    # translate cluster options into dict\n    for key in params[\"cluster\"].keys():\n        arg_key = \"cluster_{}\".format(key)\n        if hasattr(args, arg_key):\n            val = getattr(args, arg_key)\n            if val is not None:\n                params[\"cluster\"][key] = val\n\n    if args.without_cluster:\n        params[\"without_cluster\"] = True\n\n    params[\"shell_logfile\"] = args.shell_logfile\n\n    params[\"ruffus_checksums_level\"] = args.ruffus_checksums_level\n    # always create an \"input\" section\n    params[\"input_globs\"] = {}\n    for variable in args.input_globs:\n        if \"=\" in variable:\n            variable, value = variable.split(\"=\")\n            params[\"input_globs\"][variable.strip()] = value.strip()\n        else:\n            params[\"input_globs\"][\"default\"] = variable.strip()\n\n    for variables in args.variables_to_set:\n        variable, value = variables.split(\"=\")\n        value = iotools.str2val(value.strip())\n        # enter old style\n        params[variable.strip()] = value\n        # enter new style\n        parts = variable.split(\"_\")\n        for x in range(1, len(parts)):\n            prefix = \"_\".join(parts[:x])\n            if prefix in params:\n                suffix = \"_\".join(parts[x:])\n                params[prefix][suffix] = value\n\n    if args.work_dir:\n        params[\"work_dir\"] = os.path.abspath(args.work_dir)\n    else:\n        params[\"work_dir\"] = params[\"start_dir\"]\n</code></pre>"},{"location":"function_doc/pipeline/#cgatcore.pipeline.write_config_files","title":"<code>write_config_files(pipeline_path, general_path)</code>","text":"<p>create default configuration files in <code>path</code>.</p> Source code in <code>cgatcore/pipeline/control.py</code> <pre><code>def write_config_files(pipeline_path, general_path):\n    '''create default configuration files in `path`.\n    '''\n\n    paths = [pipeline_path, general_path]\n    config_files = ['pipeline.yml']\n\n    for dest in config_files:\n        if os.path.exists(dest):\n            E.warn(\"file `%s` already exists - skipped\" % dest)\n            continue\n\n        for path in paths:\n            src = os.path.join(path, dest)\n            if os.path.exists(src):\n                shutil.copyfile(src, dest)\n                E.info(\"created new configuration file `%s` \" % dest)\n                break\n        else:\n            raise ValueError(\n                \"default config file `%s` not found in %s\" %\n                (config_files, paths))\n</code></pre>"},{"location":"getting_started/examples/","title":"Running a pipeline","text":"<p>This section provides a tutorial-like introduction to running CGAT pipelines. For an example of how to build simple computational pipelines, refer to cgat-showcase. To see how <code>cgatcore</code> is used to build more complex computational pipelines, refer to the cgat-flow repository.</p>"},{"location":"getting_started/examples/#introduction","title":"Introduction","text":"<p>A pipeline takes input data and performs a series of automated steps to produce output data. Each pipeline is usually coupled with a report (such as MultiQC or Rmarkdown) to summarise and visualise the results.</p> <p>It helps if you are familiar with:</p> <ul> <li>The Unix command line to run and debug the pipeline</li> <li>Python to understand what happens in the pipeline</li> <li>Ruffus to understand the pipeline code</li> <li>SGE (or any other workload manager) to monitor jobs</li> <li>Git to keep code up-to-date</li> </ul>"},{"location":"getting_started/examples/#setting-up-a-pipeline","title":"Setting up a pipeline","text":"<p>Step 1: Install <code>cgat-showcase</code> (a toy example of a <code>cgatcore</code> pipeline).</p> <p>Ensure your computing environment is appropriate and follow <code>cgat-showcase</code> installation instructions (see Installation instructions).</p> <p>Step 2: Clone the repository</p> <p>To inspect the code and layout, clone the repository:</p> <pre><code>git clone https://github.com/cgat-developers/cgat-showcase.git\n</code></pre> <p>When inspecting the repository:</p> <ul> <li>The source directory will contain the pipeline master script named <code>cgatshowcase/pipeline_&lt;name&gt;.py</code>.</li> <li>The default configuration files are in <code>cgatshowcase/pipeline&lt;Name&gt;/</code>.</li> <li>Associated module files are typically named <code>cgatshowcase/Module&lt;Name&gt;.py</code> and contain code required to run pipeline tasks.</li> </ul> <p>Step 3: Create a working directory</p> <p>To run a pipeline, create a working directory and navigate to it:</p> <pre><code>mkdir version1\ncd version1/\n</code></pre> <p>The pipeline will execute and generate files in this directory.</p> <p>To use test data for <code>cgat-showcase</code>, download it with:</p> <pre><code>wget https://www.cgat.org/downloads/public/showcase/showcase_test_data.tar.gz\ntar -zxvf showcase_test_data.tar.gz\ncd showcase_test_data\n</code></pre> <p>Step 4: Configure the cluster</p> <p>Running pipelines on a cluster requires configuring DRMAA API settings for <code>cgatcore</code>. The default cluster engine is SGE, but SLURM and Torque/PBSpro are also supported. To use a non-SGE cluster, create a <code>.cgat.yml</code> file in your home directory and configure the parameters (see the cluster configuration documentation).</p> <p>Step 5: Generate a configuration file</p> <p>Our pipelines are written with minimal hard-coded options. To run a pipeline, generate an initial configuration file:</p> <pre><code>cgatshowcase &lt;name&gt; config\n</code></pre> <p>For example, to run the <code>transdiffexprs</code> pipeline, run:</p> <pre><code>cgatshowcase transdiffexprs config\n</code></pre> <p>This will create a new <code>pipeline.yml</code> file. You must edit this file as the default values are unlikely to suit your data. The configuration file format is simple and well-documented. For more information, see the ConfigParser documentation.</p> <p>Step 6: Add input files</p> <p>The required input is specific to each pipeline. Check the pipeline documentation to determine which files are needed and where to place them. Typically, input files are linked into the working directory and follow pipeline-specific naming conventions.</p> <p>Step 7: Check dependencies</p> <p>Check if all external dependencies, such as tools and R packages, are satisfied by running:</p> <pre><code>cgatshowcase &lt;name&gt; check\n</code></pre>"},{"location":"getting_started/examples/#running-a-pipeline_1","title":"Running a pipeline","text":"<p>Pipelines are controlled by a Python script named <code>pipeline_&lt;name&gt;.py</code> in the source directory. Command line usage information is available by running:</p> <pre><code>cgatshowcase &lt;name&gt; --help\n</code></pre> <p>Alternatively, call the Python script directly:</p> <pre><code>python /path/to/code/cgatshowcase/pipeline_&lt;name&gt;.py --help\n</code></pre> <p>The basic syntax for <code>pipeline_&lt;name&gt;.py</code> is:</p> <pre><code>cgatshowcase &lt;name&gt; [workflow options] [workflow arguments]\n</code></pre> <p>For example, to run the <code>readqc</code> pipeline:</p> <pre><code>cgatshowcase readqc make full\n</code></pre>"},{"location":"getting_started/examples/#workflow-options","title":"Workflow options","text":"<ul> <li>make <code>&lt;task&gt;</code>: Run all tasks required to build <code>&lt;task&gt;</code>.</li> <li>show <code>&lt;task&gt;</code>: Show tasks required to build <code>&lt;task&gt;</code> without executing them.</li> <li>plot <code>&lt;task&gt;</code>: Plot an image of workflow (requires Inkscape) of pipeline state for <code>&lt;task&gt;</code>.</li> <li>touch <code>&lt;task&gt;</code>: Touch files without running <code>&lt;task&gt;</code> or prerequisites, setting timestamps so files appear up-to-date.</li> <li>config: Write a new configuration file (<code>pipeline.ini</code>) with default values (won't overwrite existing files).</li> <li>clone <code>&lt;srcdir&gt;</code>: Clone a pipeline from <code>&lt;srcdir&gt;</code> into the current directory.</li> </ul> <p>To run a long pipeline appropriately:</p> <pre><code>nice -19 nohup cgatshowcase &lt;name&gt; make full -v5 -c1\n</code></pre> <p>This command will keep the pipeline running if you close the terminal.</p>"},{"location":"getting_started/examples/#fastq-naming-convention","title":"Fastq naming convention","text":"<p>Most of our pipelines assume input FASTQ files follow this naming convention:</p> <pre><code>sample1-condition.fastq.1.gz\nsample1-condition.fastq.2.gz\n</code></pre> <p>This convention ensures regular expressions do not need to account for the read within the name, making it more explicit.</p>"},{"location":"getting_started/examples/#additional-pipeline-options","title":"Additional pipeline options","text":"<p>Running the pipeline with <code>--help</code> will show additional workflow arguments that modify the pipeline's behaviour:</p> <ul> <li>--no-cluster: Run the pipeline locally.</li> <li>--input-validation: Check <code>pipeline.ini</code> file for missing values before starting.</li> <li>--debug: Add debugging information to the console (not the logfile).</li> <li>--dry-run: Perform a dry run (do not execute shell commands).</li> <li>--exceptions: Echo exceptions immediately as they occur.</li> <li>-c --checksums: Set the level of Ruffus checksums.</li> </ul>"},{"location":"getting_started/examples/#building-pipeline-reports","title":"Building pipeline reports","text":"<p>We associate some form of reporting with our pipelines to display summary information as nicely formatted HTML pages. Currently, CGAT supports three types of reports:</p> <ul> <li>MultiQC: For general alignment and tool reporting.</li> <li>R Markdown: For bespoke reporting.</li> <li>Jupyter Notebook: For bespoke reporting.</li> </ul> <p>Refer to the specific pipeline documentation at the beginning of the script to determine which type of reporting is implemented.</p> <p>Reports are generated using the following command once a workflow has completed:</p> <pre><code>cgatshowcase &lt;name&gt; make build_report\n</code></pre>"},{"location":"getting_started/examples/#multiqc-report","title":"MultiQC report","text":"<p>MultiQC is a Python framework for automating reporting, implemented in most workflows to generate QC stats for commonly used tools.</p>"},{"location":"getting_started/examples/#r-markdown","title":"R Markdown","text":"<p>R Markdown report generation is useful for creating custom reports. This is implemented in the <code>bamstats</code> workflow.</p>"},{"location":"getting_started/examples/#jupyter-notebook","title":"Jupyter Notebook","text":"<p>Jupyter Notebook is another approach we use for bespoke reports, also implemented in the <code>bamstats</code> workflow.</p>"},{"location":"getting_started/examples/#troubleshooting","title":"Troubleshooting","text":"<p>Many things can go wrong while running the pipeline:</p> <ul> <li>Bad input format: The pipeline does not perform sanity checks on input formats, which may lead to missing or incorrect results.</li> <li>Pipeline disruptions: Issues with the cluster, file system, or terminal may cause the pipeline to abort.</li> <li>Bugs: Changes in program versions or inputs can cause unexpected issues.</li> </ul> <p>If the pipeline aborts, read the log files and error messages (e.g., <code>nohup.out</code>) to locate the error. Attempt to fix the error, remove the output files from the step in which the error occurred, and restart the pipeline.</p> <p>Note: Look out for upstream errors. For example, if a geneset filtering by specific contigs does not match, the geneset may be empty, causing errors in downstream steps. To resolve this, fix the initial error and delete the files from the geneset-building step, not just the step that threw the error.</p>"},{"location":"getting_started/examples/#common-pipeline-errors","title":"Common pipeline errors","text":"<p>One common error is:</p> <pre><code>GLOBAL_SESSION = drmaa.Session()\nNameError: name 'drmaa' is not defined\n</code></pre> <p>This occurs because you are not connected to the cluster. Alternatively, run the pipeline in local mode by adding <code>--no-cluster</code> as a command line option.</p>"},{"location":"getting_started/examples/#updating-to-the-latest-code-version","title":"Updating to the latest code version","text":"<p>To get the latest bug fixes, navigate to the source directory and run:</p> <pre><code>git pull\n</code></pre> <p>This command retrieves the latest changes from the master repository and updates your local version.</p>"},{"location":"getting_started/examples/#using-qsub-commands","title":"Using qsub commands","text":"<p>We recommend using <code>cgat-core</code> to perform job submissions, as this is handled automatically. However, if you wish to use <code>qsub</code> manually, you can do so. Since the statements passed to <code>P.run()</code> are essentially command-line scripts, you can write the <code>qsub</code> commands as needed. For example:</p> <pre><code>statement = \"qsub [commands] echo 'This is where you would put commands you want ran' \"\nP.run(statement)\n</code></pre> <p>When running the pipeline, make sure to specify <code>--no-cluster</code> as a command line option.</p>"},{"location":"getting_started/examples/#troubleshooting_1","title":"Troubleshooting","text":"<ul> <li>Common Issues: If you encounter errors during pipeline execution, ensure that all dependencies are installed and paths are correctly set.</li> <li>Logs: Check the log files generated during the pipeline run for detailed error messages.</li> <li>Support: For further assistance, refer to the CGAT-core documentation or raise an issue on our GitHub repository.</li> </ul>"},{"location":"getting_started/examples/#cgat-core-examples","title":"CGAT-core Examples","text":"<p>This guide provides practical examples of CGAT-core pipelines for various use cases, from basic file processing to complex genomics workflows.</p>"},{"location":"getting_started/examples/#quick-start-examples","title":"Quick Start Examples","text":""},{"location":"getting_started/examples/#hello-world-pipeline","title":"Hello World Pipeline","text":"<pre><code>\"\"\"hello_world.py - Simple CGAT pipeline example\n\nThis pipeline demonstrates the basic structure of a CGAT pipeline:\n1. Task definition\n2. Pipeline flow\n3. Command execution\n\"\"\"\n\nfrom ruffus import *\nfrom cgatcore import pipeline as P\nimport sys\n\n# ------------------------------------------------------------------------\n# Tasks\n# ------------------------------------------------------------------------\n@originate(\"hello.txt\")\ndef create_file(outfile):\n    \"\"\"Create a simple text file.\"\"\"\n    statement = \"\"\"echo \"Hello, CGAT!\" &gt; %(outfile)s\"\"\"\n    P.run(statement)\n\n@transform(create_file, suffix(\".txt\"), \".upper.txt\")\ndef convert_to_upper(infile, outfile):\n    \"\"\"Convert text to uppercase.\"\"\"\n    statement = \"\"\"cat %(infile)s | tr '[:lower:]' '[:upper:]' &gt; %(outfile)s\"\"\"\n    P.run(statement)\n\n# ------------------------------------------------------------------------\n# Pipeline Running\n# ------------------------------------------------------------------------\nif __name__ == \"__main__\":\n    sys.exit(P.main(sys.argv))\n</code></pre>"},{"location":"getting_started/examples/#configuration-example","title":"Configuration Example","text":"<pre><code># pipeline.yml\npipeline:\n    name: hello_world\n    author: Your Name\n\n# Cluster configuration\ncluster:\n    queue_manager: slurm\n    queue: main\n    memory_resource: mem\n    memory_default: 1G\n</code></pre>"},{"location":"getting_started/examples/#real-world-examples","title":"Real-World Examples","text":""},{"location":"getting_started/examples/#genomics-pipeline","title":"1. Genomics Pipeline","text":"<p>This example demonstrates a typical RNA-seq analysis pipeline:</p> <pre><code>\"\"\"rnaseq_pipeline.py - RNA-seq analysis pipeline\n\nFeatures:\n- FastQ quality control\n- Read alignment\n- Expression quantification\n- Differential expression analysis\n\"\"\"\n\nfrom ruffus import *\nfrom cgatcore import pipeline as P\nimport logging as L\nimport sys\nimport os\n\n# ------------------------------------------------------------------------\n# Configuration\n# ------------------------------------------------------------------------\nP.get_parameters([\n    \"%s/pipeline.yml\" % os.path.splitext(__file__)[0],\n    \"pipeline.yml\"])\n\n# ------------------------------------------------------------------------\n# Tasks\n# ------------------------------------------------------------------------\n@transform(\"*.fastq.gz\", suffix(\".fastq.gz\"), \".fastqc.done\")\ndef run_fastqc(infile, outfile):\n    \"\"\"Quality control of sequencing reads.\"\"\"\n    job_threads = 1\n    job_memory = \"2G\"\n\n    statement = \"\"\"\n    fastqc --outdir=fastqc %(infile)s &amp;&amp;\n    touch %(outfile)s\n    \"\"\"\n    P.run(statement)\n\n@transform(\"*.fastq.gz\", suffix(\".fastq.gz\"), \".bam\")\ndef align_reads(infile, outfile):\n    \"\"\"Align reads to reference genome.\"\"\"\n    job_threads = 8\n    job_memory = \"32G\"\n\n    statement = \"\"\"\n    STAR \n        --runThreadN %(job_threads)s\n        --genomeDir %(genome_dir)s\n        --readFilesIn %(infile)s\n        --readFilesCommand zcat\n        --outFileNamePrefix %(outfile)s\n        --outSAMtype BAM SortedByCoordinate\n    \"\"\"\n    P.run(statement)\n\n@transform(align_reads, suffix(\".bam\"), \".counts.tsv\")\ndef count_features(infile, outfile):\n    \"\"\"Count reads in genomic features.\"\"\"\n    job_threads = 4\n    job_memory = \"8G\"\n\n    statement = \"\"\"\n    featureCounts\n        -T %(job_threads)s\n        -a %(annotations)s\n        -o %(outfile)s\n        %(infile)s\n    \"\"\"\n    P.run(statement)\n\n@merge(count_features, \"deseq2_results\")\ndef run_deseq2(infiles, outfile):\n    \"\"\"Differential expression analysis.\"\"\"\n    job_memory = \"16G\"\n\n    statement = \"\"\"\n    Rscript scripts/run_deseq2.R\n        --counts=%(infiles)s\n        --design=%(design_file)s\n        --outdir=%(outfile)s\n    \"\"\"\n    P.run(statement)\n\nif __name__ == \"__main__\":\n    sys.exit(P.main(sys.argv))\n</code></pre>"},{"location":"getting_started/examples/#data-processing-pipeline","title":"2. Data Processing Pipeline","text":"<p>Example of a data processing pipeline with S3 integration:</p> <pre><code>\"\"\"data_pipeline.py - Data processing with S3 integration\n\nFeatures:\n- S3 input/output\n- Parallel processing\n- Error handling\n- Resource management\n\"\"\"\n\nfrom ruffus import *\nfrom cgatcore import pipeline as P\nimport logging as L\nimport sys\nimport os\n\n# ------------------------------------------------------------------------\n# Configuration\n# ------------------------------------------------------------------------\nP.get_parameters([\n    \"%s/pipeline.yml\" % os.path.splitext(__file__)[0],\n    \"pipeline.yml\"])\n\n# Configure S3\nP.configure_s3()\n\n# ------------------------------------------------------------------------\n# Tasks\n# ------------------------------------------------------------------------\n@P.s3_transform(\"s3://bucket/data/*.csv\", \n                suffix(\".csv\"), \n                \".processed.csv\")\ndef process_data(infile, outfile):\n    \"\"\"Process CSV files from S3.\"\"\"\n    job_memory = \"4G\"\n\n    statement = \"\"\"\n    python scripts/process_data.py\n        --input=%(infile)s\n        --output=%(outfile)s\n        --config=%(processing_config)s\n    \"\"\"\n    try:\n        P.run(statement)\n    except P.PipelineError as e:\n        L.error(\"Processing failed: %s\" % e)\n        # Cleanup temporary files\n        cleanup_temp_files()\n        raise\n    finally:\n        # Always clean up\n        P.cleanup_tmpdir()\n\n@P.s3_merge(process_data, \n           \"s3://bucket/results/report.html\")\ndef create_report(infiles, outfile):\n    \"\"\"Generate analysis report.\"\"\"\n    job_memory = \"8G\"\n\n    statement = \"\"\"\n    python scripts/create_report.py\n        --input=%(infiles)s\n        --output=%(outfile)s\n        --template=%(report_template)s\n    \"\"\"\n    P.run(statement)\n\nif __name__ == \"__main__\":\n    sys.exit(P.main(sys.argv))\n</code></pre>"},{"location":"getting_started/examples/#image-processing-pipeline","title":"3. Image Processing Pipeline","text":"<p>Example of an image processing pipeline:</p> <pre><code>\"\"\"image_pipeline.py - Image processing pipeline\n\nFeatures:\n- Batch image processing\n- Feature extraction\n- Analysis reporting\n\"\"\"\n\nfrom ruffus import *\nfrom cgatcore import pipeline as P\nimport sys\nimport os\n\n# ------------------------------------------------------------------------\n# Configuration\n# ------------------------------------------------------------------------\nP.get_parameters([\n    \"%s/pipeline.yml\" % os.path.splitext(__file__)[0],\n    \"pipeline.yml\"])\n\n# ------------------------------------------------------------------------\n# Tasks\n# ------------------------------------------------------------------------\n@transform(\"*.png\", suffix(\".png\"), \".processed.png\")\ndef preprocess_images(infile, outfile):\n    \"\"\"Image preprocessing.\"\"\"\n    statement = \"\"\"\n    python scripts/preprocess.py\n        --input=%(infile)s\n        --output=%(outfile)s\n        --params=%(preprocessing_params)s\n    \"\"\"\n    P.run(statement)\n\n@transform(preprocess_images, \n          suffix(\".processed.png\"), \n          \".features.json\")\ndef extract_features(infile, outfile):\n    \"\"\"Feature extraction.\"\"\"\n    statement = \"\"\"\n    python scripts/extract_features.py\n        --input=%(infile)s\n        --output=%(outfile)s\n        --model=%(feature_model)s\n    \"\"\"\n    P.run(statement)\n\n@merge(extract_features, \"analysis_report.html\")\ndef analyze_results(infiles, outfile):\n    \"\"\"Generate analysis report.\"\"\"\n    statement = \"\"\"\n    python scripts/analyze.py\n        --input=%(infiles)s\n        --output=%(outfile)s\n        --config=%(analysis_config)s\n    \"\"\"\n    P.run(statement)\n\nif __name__ == \"__main__\":\n    sys.exit(P.main(sys.argv))\n</code></pre>"},{"location":"getting_started/examples/#best-practices","title":"Best Practices","text":""},{"location":"getting_started/examples/#resource-management","title":"1. Resource Management","text":"<pre><code>@transform(\"*.bam\", suffix(\".bam\"), \".sorted.bam\")\ndef sort_bam(infile, outfile):\n    \"\"\"Example of proper resource management.\"\"\"\n    # Calculate memory based on input size\n    infile_size = os.path.getsize(infile)\n    job_memory = \"%dG\" % max(4, infile_size // (1024**3) + 2)\n\n    # Set threads based on system\n    job_threads = min(4, os.cpu_count())\n\n    # Use temporary directory\n    tmpdir = P.get_temp_dir()\n\n    statement = \"\"\"\n    samtools sort \n        -@ %(job_threads)s \n        -m %(job_memory)s \n        -T %(tmpdir)s/sort \n        %(infile)s &gt; %(outfile)s\n    \"\"\"\n    P.run(statement)\n</code></pre>"},{"location":"getting_started/examples/#error-handling","title":"2. Error Handling","text":"<pre><code>@transform(\"*.txt\", suffix(\".txt\"), \".processed\")\ndef robust_processing(infile, outfile):\n    \"\"\"Example of proper error handling.\"\"\"\n    try:\n        statement = \"\"\"\n        process_data %(infile)s &gt; %(outfile)s\n        \"\"\"\n        P.run(statement)\n    except P.PipelineError as e:\n        L.error(\"Processing failed: %s\" % e)\n        # Cleanup temporary files\n        cleanup_temp_files()\n        raise\n    finally:\n        # Always clean up\n        P.cleanup_tmpdir()\n</code></pre>"},{"location":"getting_started/examples/#configuration-management","title":"3. Configuration Management","text":"<pre><code># pipeline.yml - Example configuration\n\n# Pipeline metadata\npipeline:\n    name: example_pipeline\n    version: 1.0.0\n    author: Your Name\n\n# Input/Output\nio:\n    input_dir: /path/to/input\n    output_dir: /path/to/output\n    temp_dir: /tmp/pipeline\n\n# Processing parameters\nprocessing:\n    threads: 4\n    memory: 8G\n    chunk_size: 1000\n\n# Cluster configuration\ncluster:\n    queue_manager: slurm\n    queue: main\n    memory_resource: mem\n    parallel_environment: smp\n    max_jobs: 100\n\n# S3 configuration (if needed)\ns3:\n    bucket: my-pipeline-bucket\n    region: us-west-2\n    transfer:\n        multipart_threshold: 8388608\n        max_concurrency: 10\n</code></pre>"},{"location":"getting_started/examples/#running-the-examples","title":"Running the Examples","text":"<ol> <li> <p>Setup Configuration <pre><code># Copy and edit pipeline configuration\ncp pipeline.yml.example pipeline.yml\n</code></pre></p> </li> <li> <p>Run Pipeline <pre><code># Show pipeline tasks\npython pipeline.py show full\n\n# Run specific task\npython pipeline.py make task_name\n\n# Run entire pipeline\npython pipeline.py make full\n</code></pre></p> </li> <li> <p>Cluster Execution <pre><code># Run on cluster\npython pipeline.py make full --cluster-queue=main\n</code></pre></p> </li> </ol> <p>For more information, see: - Pipeline Overview - Cluster Configuration - S3 Integration</p>"},{"location":"getting_started/installation/","title":"Installation","text":"<p>The following sections describe how to install the <code>cgatcore</code> framework.</p>"},{"location":"getting_started/installation/#conda-installation","title":"Conda installation","text":"<p>The preferred method of installation is using Conda. If you do not have Conda installed, you can install it using Miniconda or Anaconda.</p> <p><code>cgatcore</code> is installed via the Bioconda channel, and the recipe can be found on GitHub. To install <code>cgatcore</code>, run the following command:</p> <pre><code>conda install -c conda-forge -c bioconda cgatcore\n</code></pre>"},{"location":"getting_started/installation/#prerequisites","title":"Prerequisites","text":"<p>Before installing <code>cgatcore</code>, ensure that you have the following prerequisites:</p> <ul> <li>Operating System: Linux or macOS</li> <li>Python: Version 3.6 or higher</li> <li>Conda: Recommended for dependency management</li> </ul>"},{"location":"getting_started/installation/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Conda Issues: If you encounter issues with Conda, ensure that the Bioconda and Conda-Forge channels are added and prioritized correctly.</li> <li>Pip Dependencies: When using pip, manually install any missing dependencies listed in the error messages.</li> <li>Script Errors: If the installation script fails, check the script's output for error messages and ensure all prerequisites are met.</li> </ul>"},{"location":"getting_started/installation/#verification","title":"Verification","text":"<p>After installation, verify the installation by running:</p> <pre><code>python\n</code></pre> <pre><code>import cgatcore\nprint(cgatcore.__version__)\n</code></pre> <p>This should display the installed version of <code>cgatcore</code>.</p>"},{"location":"getting_started/installation/#pip-installation","title":"Pip installation","text":"<p>We recommend installation through Conda because it manages dependencies automatically. However, <code>cgatcore</code> is generally lightweight and can also be installed using the <code>pip</code> package manager. Note that you may need to manually install other dependencies as needed:</p> <pre><code>pip install cgatcore\n</code></pre>"},{"location":"getting_started/installation/#automated-installation","title":"Automated installation","text":"<p>The preferred method to install <code>cgatcore</code> is using Conda. However, we have also created a Bash installation script, which uses Conda under the hood.</p> <p>Here are the steps:</p> <pre><code># Download the installation script:\ncurl -O https://raw.githubusercontent.com/cgat-developers/cgat-core/master/install.sh\n\n# See help:\nbash install.sh\n\n# Install the development version (recommended, as there is no production version yet):\nbash install.sh --devel [--location &lt;/full/path/to/folder/without/trailing/slash&gt;]\n\n# To download the code in Git format instead of the default zip format, use:\n--git # for an HTTPS clone\n--git-ssh # for an SSH clone (you need to be a cgat-developer contributor on GitHub to do this)\n\n# Enable the Conda environment as instructed by the installation script\n# Note: you might want to automate this by adding the following instructions to your .bashrc\nsource &lt;/full/path/to/folder/without/trailing/slash&gt;/conda-install/etc/profile.d/conda.sh\nconda activate base\nconda activate cgat-c\n</code></pre> <p>The installation script will place everything under the specified location. The aim of the script is to provide a portable installation that does not interfere with existing software environments. As a result, you will have a dedicated Conda environment that can be activated as needed to work with <code>cgatcore</code>.</p>"},{"location":"getting_started/installation/#manual-installation","title":"Manual installation","text":"<p>To obtain the latest code, check it out from the public Git repository and activate it:</p> <pre><code>git clone https://github.com/cgat-developers/cgat-core.git\ncd cgat-core\npython setup.py develop\n</code></pre> <p>To update to the latest version, simply pull the latest changes:</p> <pre><code>git pull\n</code></pre>"},{"location":"getting_started/installation/#installing-additional-software","title":"Installing additional software","text":"<p>When building your own workflows, we recommend using Conda to install software into your environment where possible. This ensures compatibility and ease of installation.</p> <p>To search for and install a package using Conda:</p> <pre><code>conda search &lt;package&gt;\nconda install &lt;package&gt;\n</code></pre>"},{"location":"getting_started/installation/#accessing-libdrmaa","title":"Accessing the libdrmaa shared library","text":"<p>You may also need access to the <code>libdrmaa.so.1.0</code> C library, which can often be installed as part of the <code>libdrmaa-dev</code> package on most Unix systems. Once installed, you may need to specify the location of the DRMAA library if it is not in a default library path. Set the <code>DRMAA_LIBRARY_PATH</code> environment variable to point to the library location.</p> <p>To set this variable permanently, add the following line to your <code>.bashrc</code> file (adjusting the path as necessary):</p> <pre><code>export DRMAA_LIBRARY_PATH=/usr/lib/libdrmaa.so.1.0\n</code></pre> <p>Conda documentation</p>"},{"location":"getting_started/run_parameters/","title":"Cluster configuration","text":"<p>Currently, cgatcore supports the following workload managers: SGE, SLURM and Torque. The default cluster options are set for SunGrid Engine (SGE). If you are using a different workload manager, you need to configure your cluster settings accordingly by creating a <code>.cgat.yml</code> file in your home directory.</p> <p>This configuration file allows you to override the default settings. To view the hardcoded parameters for cgatcore, refer to the parameters.py file.</p> <p>For an example of configuring a PBSPro workload manager, see the provided config example.</p> <p>The <code>.cgat.yml</code> file in your home directory will take precedence over the default cgatcore settings. For instance, adding the following configuration to <code>.cgat.yml</code> will implement cluster settings for SLURM:</p> <pre><code>memory_resource: mem\n\noptions: --time=00:10:00 --cpus-per-task=8 --mem=1G\n\nqueue_manager: slurm\n\nqueue: NONE\n\nparallel_environment: \"dedicated\"\n</code></pre> <p>This setup specifies memory resource allocation (<code>mem</code>), runtime limits (<code>walltime</code>), selection of CPU and memory resources, and the use of the PBSPro queue manager, among other settings. Make sure to adjust the parameters according to your cluster environment to optimise the workload manager for your pipeline runs.</p>"},{"location":"getting_started/run_parameters/#default-parameters","title":"Default Parameters","text":"<p>The following are some of the default parameters in <code>cgatcore</code> that can be overridden in your <code>.cgat.yml</code> file:</p> <ul> <li>memory_resource: Defines the memory resource name (e.g., <code>mem</code> for PBSPro).</li> <li>options: Specifies additional options for job submission (e.g., <code>-l walltime=00:10:00</code>).</li> <li>queue_manager: The queue manager to be used (e.g., <code>pbspro</code>, <code>slurm</code>).</li> <li>queue: The default queue for job submission.</li> <li>parallel_environment: Specifies the parallel environment settings.</li> </ul>"},{"location":"getting_started/run_parameters/#additional-parameters","title":"Additional Parameters","text":"<p>The following additional parameters can also be configured in your <code>.cgat.yml</code> file:</p> <ul> <li>cluster_queue: Specifies the cluster queue to use (default: <code>all.q</code>).</li> <li>cluster_priority: Sets the priority of jobs in the cluster queue (default: <code>-10</code>).</li> <li>cluster_num_jobs: Limits the number of jobs to submit to the cluster queue (default: <code>100</code>).</li> <li>cluster_memory_resource: Name of the consumable resource to request memory (default: <code>mem_free</code>).</li> <li>cluster_memory_default: Default amount of memory allocated for each job (default: <code>4G</code>).</li> <li>cluster_memory_ulimit: Ensures requested memory is not exceeded via ulimit (default: <code>False</code>).</li> <li>cluster_options: General cluster options for job submission.</li> <li>cluster_parallel_environment: Parallel environment for multi-threaded jobs (default: <code>dedicated</code>).</li> <li>cluster_queue_manager: Specifies the cluster queue manager (default: <code>sge</code>).</li> <li>cluster_tmpdir: Directory specification for temporary files on cluster nodes. If set to <code>False</code>, the general <code>tmpdir</code> parameter is used.</li> </ul> <p>These parameters allow you to customize the cluster environment to better suit your pipeline's needs.</p>"},{"location":"getting_started/run_parameters/#example-configurations","title":"Example Configurations","text":""},{"location":"getting_started/run_parameters/#slurm-configuration","title":"SLURM Configuration","text":"<pre><code>memory_resource: mem\n\noptions: --time=00:10:00 --cpus-per-task=8 --mem=1G\n\nqueue_manager: slurm\n\nqueue: NONE\n\nparallel_environment: \"dedicated\"\n</code></pre>"},{"location":"getting_started/run_parameters/#torque-configuration","title":"Torque Configuration","text":"<pre><code>memory_resource: mem\n\noptions: -l walltime=00:10:00 -l nodes=1:ppn=8\n\nqueue_manager: torque\n\nqueue: NONE\n\nparallel_environment: \"dedicated\"\n</code></pre> <p>These configurations specify memory allocation, runtime limits, and other settings specific to each workload manager. Adjust these parameters to suit your cluster environment.</p>"},{"location":"getting_started/tutorial/","title":"Running a pipeline - Tutorial","text":"<p>Before beginning this tutorial, ensure that <code>cgat-core</code> is installed correctly. Refer to the installation instructions for guidance.</p> <p>As a tutorial example of how to run a CGAT workflow, we will use the <code>cgat-showcase</code> pipeline. You will also need to install <code>cgat-showcase</code> (see the instructions).</p> <p>The aim of this pipeline is to perform pseudoalignment using <code>kallisto</code>. The pipeline can be run locally or distributed across a cluster. This tutorial will explain the steps required to run the pipeline. Further documentation on <code>cgat-showcase</code> can be found here.</p> <p>The <code>cgat-showcase</code> pipeline highlights some of the functionality of <code>cgat-core</code>. Additionally, more advanced workflows for next-generation sequencing analysis are available in the cgat-flow repository.</p>"},{"location":"getting_started/tutorial/#tutorial-start","title":"Tutorial start","text":""},{"location":"getting_started/tutorial/#download-data","title":"Step 1: Download the tutorial data","text":"<p>Create a new directory, navigate to it, and download the test data:</p> <pre><code>mkdir showcase\ncd showcase\nwget https://www.cgat.org/downloads/public/showcase/showcase_test_data.tar.gz\ntar -zxvf showcase_test_data.tar.gz\n</code></pre>"},{"location":"getting_started/tutorial/#generate-config","title":"Step 2: Generate a configuration YAML file","text":"<p>Navigate to the test data directory and generate a configuration file for the pipeline:</p> <pre><code>cd showcase_test_data\ncgatshowcase transdiffexpres config\n</code></pre> <p>Alternatively, you can call the workflow file directly:</p> <pre><code>python /path/to/file/pipeline_transdiffexpres.py config\n</code></pre> <p>This will generate a <code>pipeline.yml</code> file containing configuration parameters that can be used to modify the pipeline output. For this tutorial, you do not need to modify the parameters to run the pipeline. In the Modify Config section below, you will find details on how to adjust the config file to change the pipeline's output.</p>"},{"location":"getting_started/tutorial/#run-pipeline","title":"Step 3: Run the pipeline","text":"<p>To run the pipeline, execute the following command in the directory containing the <code>pipeline.yml</code> file:</p> <pre><code>cgatshowcase transdiffexpres make full -v5 --no-cluster\n</code></pre> <p>The <code>--no-cluster</code> flag will run the pipeline locally if you do not have access to a cluster. If you have access to a cluster, you can remove the <code>--no-cluster</code> option, and the pipeline will distribute the jobs across the cluster.</p> <p>Note: There are many command line options available to run the pipeline. To see the available options, run:</p> <pre><code>cgatshowcase --help\n</code></pre> <p>This will start the pipeline execution. Monitor the output for any errors or warnings.</p>"},{"location":"getting_started/tutorial/#review-results","title":"Step 4: Review Results","text":"<p>Once the pipeline completes, review the output files generated in the <code>showcase_test_data</code> directory. These files contain the results of the pseudoalignment.</p>"},{"location":"getting_started/tutorial/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Common Issues: If you encounter errors during execution, ensure that all dependencies are installed and paths are correctly set.</li> <li>Logs: Check the log files generated during the pipeline run for detailed error messages.</li> <li>Support: For further assistance, refer to the CGAT-core documentation or raise an issue on our GitHub repository.</li> </ul>"},{"location":"getting_started/tutorial/#generate-report","title":"Step 5: Generate a report","text":"<p>The final step is to generate a report to display the output of the pipeline. We recommend using <code>MultiQC</code> for generating reports from commonly used bioinformatics tools (such as mappers and pseudoaligners) and <code>Rmarkdown</code> for generating custom reports.</p> <p>To generate these reports, run the following command:</p> <pre><code>cgatshowcase transdiffexprs make build_report -v 5 --no-cluster\n</code></pre> <p>This will generate a <code>MultiQC</code> report in the folder <code>MultiQC_report.dir/</code> and an <code>Rmarkdown</code> report in <code>R_report.dir/</code>.</p>"},{"location":"getting_started/tutorial/#core-concepts","title":"Core Concepts","text":""},{"location":"getting_started/tutorial/#pipeline-structure","title":"Pipeline Structure","text":"<p>A CGAT pipeline typically consists of: 1. Tasks: Individual processing steps 2. Dependencies: Relationships between tasks 3. Configuration: Pipeline settings 4. Execution: Running the pipeline</p>"},{"location":"getting_started/tutorial/#task-types","title":"Task Types","text":"<ol> <li> <p>@transform: One-to-one file transformation <pre><code>@transform(\"*.bam\", suffix(\".bam\"), \".sorted.bam\")\ndef sort_bam(infile, outfile):\n    pass\n</code></pre></p> </li> <li> <p>@merge: Many-to-one operation <pre><code>@merge(\"*.counts\", \"final_counts.txt\")\ndef merge_counts(infiles, outfile):\n    pass\n</code></pre></p> </li> <li> <p>@split: One-to-many operation <pre><code>@split(\"input.txt\", \"chunk_*.txt\")\ndef split_file(infile, outfiles):\n    pass\n</code></pre></p> </li> </ol>"},{"location":"getting_started/tutorial/#resource-management","title":"Resource Management","text":"<p>Control resource allocation: <pre><code>@transform(\"*.bam\", suffix(\".bam\"), \".sorted.bam\")\ndef sort_bam(infile, outfile):\n    job_memory = \"8G\"\n    job_threads = 4\n    statement = \"\"\"\n    samtools sort -@ %(job_threads)s -m %(job_memory)s \n    %(infile)s &gt; %(outfile)s\n    \"\"\"\n    P.run(statement)\n</code></pre></p>"},{"location":"getting_started/tutorial/#error-handling","title":"Error Handling","text":"<p>Implement robust error handling: <pre><code>try:\n    P.run(statement)\nexcept P.PipelineError as e:\n    L.error(\"Task failed: %s\" % e)\n    raise\n</code></pre></p>"},{"location":"getting_started/tutorial/#advanced-topics","title":"Advanced Topics","text":""},{"location":"getting_started/tutorial/#pipeline-parameters","title":"1. Pipeline Parameters","text":"<p>Access configuration parameters: <pre><code># Get parameter with default\nthreads = PARAMS.get(\"threads\", 1)\n\n# Required parameter\ninput_dir = PARAMS[\"input_dir\"]\n</code></pre></p>"},{"location":"getting_started/tutorial/#logging","title":"2. Logging","text":"<p>Use the logging system: <pre><code># Log information\nL.info(\"Processing %s\" % infile)\n\n# Log warnings\nL.warning(\"Low memory condition\")\n\n# Log errors\nL.error(\"Task failed: %s\" % e)\n</code></pre></p>"},{"location":"getting_started/tutorial/#temporary-files","title":"3. Temporary Files","text":"<p>Manage temporary files: <pre><code>@transform(\"*.bam\", suffix(\".bam\"), \".sorted.bam\")\ndef sort_bam(infile, outfile):\n    # Get temp directory\n    tmpdir = P.get_temp_dir()\n\n    statement = \"\"\"\n    samtools sort -T %(tmpdir)s/sort \n    %(infile)s &gt; %(outfile)s\n    \"\"\"\n    P.run(statement)\n</code></pre></p>"},{"location":"getting_started/tutorial/#best-practices","title":"Best Practices","text":""},{"location":"getting_started/tutorial/#code-organization","title":"Code Organization","text":""},{"location":"getting_started/tutorial/#1-task-structure","title":"1. Task Structure","text":"<ul> <li>Use meaningful task names</li> <li>Group related tasks together</li> <li>Keep tasks focused and single-purpose</li> <li>Document task dependencies clearly</li> </ul>"},{"location":"getting_started/tutorial/#2-file-management","title":"2. File Management","text":"<ul> <li>Use consistent file naming patterns</li> <li>Organize output directories logically</li> <li>Clean up temporary files</li> <li>Handle file paths safely</li> </ul>"},{"location":"getting_started/tutorial/#3-documentation","title":"3. Documentation","text":"<ul> <li>Add docstrings to all tasks</li> <li>Document configuration parameters</li> <li>Include usage examples</li> <li>Maintain a clear README</li> </ul>"},{"location":"getting_started/tutorial/#resource-management-best-practices","title":"Resource Management","text":""},{"location":"getting_started/tutorial/#1-memory-usage","title":"1. Memory Usage","text":"<ul> <li>Set appropriate memory limits</li> <li>Scale memory with input size</li> <li>Monitor memory consumption</li> <li>Handle memory errors gracefully</li> </ul> <pre><code>@transform(\"*.bam\", suffix(\".bam\"), \".sorted.bam\")\ndef sort_bam(infile, outfile):\n    \"\"\"Sort BAM file with memory scaling.\"\"\"\n    # Scale memory based on input size\n    infile_size = os.path.getsize(infile)\n    job_memory = \"%dG\" % max(4, infile_size // (1024**3) + 2)\n\n    statement = \"\"\"\n    samtools sort -m %(job_memory)s %(infile)s &gt; %(outfile)s\n    \"\"\"\n    P.run(statement)\n</code></pre>"},{"location":"getting_started/tutorial/#2-cpu-allocation","title":"2. CPU Allocation","text":"<ul> <li>Set appropriate thread counts</li> <li>Consider cluster limitations</li> <li>Scale threads with task needs</li> <li>Monitor CPU usage</li> </ul> <pre><code>@transform(\"*.fa\", suffix(\".fa\"), \".indexed\")\ndef index_genome(infile, outfile):\n    \"\"\"Index genome with appropriate thread count.\"\"\"\n    # Set threads based on system\n    job_threads = min(4, os.cpu_count())\n\n    statement = \"\"\"\n    bwa index -t %(job_threads)s %(infile)s\n    \"\"\"\n    P.run(statement)\n</code></pre>"},{"location":"getting_started/tutorial/#3-temporary-files","title":"3. Temporary Files","text":"<ul> <li>Use proper temporary directories</li> <li>Clean up after task completion</li> <li>Handle cleanup in error cases</li> <li>Monitor disk usage</li> </ul> <pre><code>@transform(\"*.bam\", suffix(\".bam\"), \".sorted.bam\")\ndef sort_with_temp(infile, outfile):\n    \"\"\"Sort using managed temporary directory.\"\"\"\n    tmpdir = P.get_temp_dir()\n    try:\n        statement = \"\"\"\n        samtools sort -T %(tmpdir)s/sort %(infile)s &gt; %(outfile)s\n        \"\"\"\n        P.run(statement)\n    finally:\n        P.cleanup_tmpdir()\n</code></pre>"},{"location":"getting_started/tutorial/#error-handling-best-practices","title":"Error Handling","text":""},{"location":"getting_started/tutorial/#1-task-failures","title":"1. Task Failures","text":"<ul> <li>Implement proper error checking</li> <li>Log informative error messages</li> <li>Clean up on failure</li> <li>Provide recovery options</li> </ul> <pre><code>@transform(\"*.txt\", suffix(\".txt\"), \".processed\")\ndef process_with_errors(infile, outfile):\n    \"\"\"Process files with error handling.\"\"\"\n    try:\n        statement = \"\"\"\n        process_data %(infile)s &gt; %(outfile)s\n        \"\"\"\n        P.run(statement)\n    except P.PipelineError as e:\n        L.error(\"Processing failed: %s\" % e)\n        # Cleanup and handle error\n        cleanup_and_notify()\n        raise\n</code></pre>"},{"location":"getting_started/tutorial/#2-input-validation","title":"2. Input Validation","text":"<ul> <li>Check input file existence</li> <li>Validate input formats</li> <li>Verify parameter values</li> <li>Handle missing data</li> </ul> <pre><code>@transform(\"*.bam\", suffix(\".bam\"), \".stats\")\ndef calculate_stats(infile, outfile):\n    \"\"\"Calculate statistics with input validation.\"\"\"\n    # Check input file\n    if not os.path.exists(infile):\n        raise ValueError(\"Input file not found: %s\" % infile)\n\n    # Verify file format\n    if not P.is_valid_bam(infile):\n        raise ValueError(\"Invalid BAM file: %s\" % infile)\n\n    statement = \"\"\"\n    samtools stats %(infile)s &gt; %(outfile)s\n    \"\"\"\n    P.run(statement)\n</code></pre>"},{"location":"getting_started/tutorial/#3-logging","title":"3. Logging","text":"<ul> <li>Use appropriate log levels</li> <li>Include relevant context</li> <li>Log progress and milestones</li> <li>Maintain log rotation</li> </ul> <pre><code>@transform(\"*.data\", suffix(\".data\"), \".processed\")\ndef process_with_logging(infile, outfile):\n    \"\"\"Process with comprehensive logging.\"\"\"\n    L.info(\"Starting processing of %s\" % infile)\n\n    try:\n        statement = \"\"\"\n        process_data %(infile)s &gt; %(outfile)s\n        \"\"\"\n        P.run(statement)\n        L.info(\"Successfully processed %s\" % infile)\n    except Exception as e:\n        L.error(\"Failed to process %s: %s\" % (infile, e))\n        raise\n</code></pre>"},{"location":"getting_started/tutorial/#pipeline-configuration","title":"Pipeline Configuration","text":""},{"location":"getting_started/tutorial/#1-parameter-management","title":"1. Parameter Management","text":"<ul> <li>Use configuration files</li> <li>Set sensible defaults</li> <li>Document parameters</li> <li>Validate parameter values</li> </ul> <pre><code># pipeline.yml\npipeline:\n    name: example_pipeline\n    version: 1.0.0\n\n# Resource configuration\ncluster:\n    memory_default: 4G\n    threads_default: 1\n    queue: main\n\n# Processing parameters\nparams:\n    min_quality: 20\n    max_threads: 4\n    chunk_size: 1000\n</code></pre>"},{"location":"getting_started/tutorial/#2-environment-setup","title":"2. Environment Setup","text":"<ul> <li>Use virtual environments</li> <li>Document dependencies</li> <li>Version control configuration</li> <li>Handle platform differences</li> </ul> <pre><code># Create virtual environment\npython -m venv pipeline-env\n\n# Install dependencies\npip install -r requirements.txt\n\n# Set environment variables\nexport PIPELINE_CONFIG=/path/to/pipeline.yml\n</code></pre>"},{"location":"getting_started/tutorial/#3-testing","title":"3. Testing","text":"<ul> <li>Write unit tests</li> <li>Test with sample data</li> <li>Verify outputs</li> <li>Monitor performance</li> </ul> <pre><code>def test_pipeline():\n    \"\"\"Test pipeline with sample data.\"\"\"\n    # Run pipeline\n    statement = \"\"\"\n    python pipeline.py make all --local\n    \"\"\"\n    P.run(statement)\n\n    # Verify outputs\n    assert os.path.exists(\"expected_output.txt\")\n    assert check_output_validity(\"expected_output.txt\")\n</code></pre>"},{"location":"getting_started/tutorial/#troubleshooting-best-practices","title":"Troubleshooting","text":"<p>If you encounter issues:</p> <ol> <li>Check Logs</li> <li>Review pipeline logs</li> <li>Check cluster logs</li> <li>Examine error messages</li> <li> <p>Monitor resource usage</p> </li> <li> <p>Common Issues</p> </li> <li>Memory allocation errors</li> <li>File permission problems</li> <li>Cluster queue issues</li> <li> <p>Software version conflicts</p> </li> <li> <p>Getting Help</p> </li> <li>Check documentation</li> <li>Search issue tracker</li> <li>Ask on forums</li> <li>Contact support team</li> </ol> <p>For more detailed information, see: - Pipeline Overview - Cluster Configuration - Error Handling</p>"},{"location":"getting_started/tutorial/#next-steps","title":"Next Steps","text":"<ul> <li>Review the Examples section</li> <li>Learn about Cluster Configuration</li> <li>Explore Cloud Integration</li> </ul> <p>For more advanced topics, see the Pipeline Modules documentation.</p>"},{"location":"getting_started/tutorial/#conclusion","title":"Conclusion","text":"<p>This completes the tutorial for running the <code>transdiffexprs</code> pipeline for <code>cgat-showcase</code>. We hope you find it as useful as we do for writing workflows in Python.</p>"},{"location":"pipeline_modules/cluster/","title":"Cluster Module","text":"<p><code>cluster.py</code> - Cluster utility functions for cgatcore pipelines</p> <p>==============================================================</p> <p>This module abstracts the DRMAA native specification and provides convenience functions for running DRMAA jobs. It currently supports SGE, SLURM, Torque, and PBSPro cluster environments, enabling users to submit and manage cluster jobs easily within cgatcore pipelines.</p>"},{"location":"pipeline_modules/cluster/#reference","title":"Reference","text":"<p>The following documentation details the cluster management utilities provided by the <code>cluster.py</code> module.</p>"},{"location":"pipeline_modules/cluster/#import-statements","title":"Import Statements","text":"<pre><code>import re\nimport math\nimport collections\nimport os\nimport stat\nimport time\nimport datetime\nimport logging\nimport gevent\nimport cgatcore.experiment as E\ntry:\n    import drmaa\nexcept (ImportError, RuntimeError, OSError):\n    pass\n</code></pre>"},{"location":"pipeline_modules/cluster/#key-classes-and-functions","title":"Key Classes and Functions","text":""},{"location":"pipeline_modules/cluster/#get_logger","title":"<code>get_logger()</code>","text":"<p>Returns the logger for the CGAT-core pipeline, which is used to handle logging within the cluster management utilities.</p> <pre><code>def get_logger():\n    return logging.getLogger(\"cgatcore.pipeline\")\n</code></pre>"},{"location":"pipeline_modules/cluster/#drmaacluster-class","title":"<code>DRMAACluster</code> Class","text":"<p>This class provides core functionality for managing DRMAA cluster jobs, abstracting cluster specifications for SGE, SLURM, Torque, and PBSPro.</p>"},{"location":"pipeline_modules/cluster/#__init__self-session-ignore_errorsfalse","title":"<code>__init__(self, session, ignore_errors=False)</code>","text":"<p>Initialises a DRMAA cluster instance.</p> <p>Arguments: - <code>session</code> (drmaa.Session): DRMAA session for interacting with the cluster. - <code>ignore_errors</code> (bool, optional): If <code>True</code>, job errors are ignored, allowing the pipeline to continue.</p>"},{"location":"pipeline_modules/cluster/#setup_drmaa_job_template","title":"<code>setup_drmaa_job_template(...)</code>","text":"<p>Sets up a DRMAA job template. Supported environments include SGE, SLURM, Torque, and PBSPro.</p> <p>Arguments: - <code>drmaa_session</code>: The DRMAA session object. - <code>job_name</code> (string): Name of the job. - <code>job_memory</code> (string): Memory requirements for the job. - <code>job_threads</code> (int): Number of threads to allocate for the job. - <code>working_directory</code> (string): Working directory for the job.</p> <p>Raises: - <code>ValueError</code>: If job memory is not specified.</p>"},{"location":"pipeline_modules/cluster/#collect_single_job_from_cluster","title":"<code>collect_single_job_from_cluster(...)</code>","text":"<p>Collects a single job running on the cluster, waiting for its completion and returning stdout, stderr, and resource usage.</p> <p>Arguments: - <code>job_id</code> (string): The job ID. - <code>statement</code> (string): Command executed by the job. - <code>stdout_path</code> (string): Path to the stdout file. - <code>stderr_path</code> (string): Path to the stderr file. - <code>job_path</code> (string): Path to the job file.</p>"},{"location":"pipeline_modules/cluster/#get_drmaa_job_stdout_stderr","title":"<code>get_drmaa_job_stdout_stderr(...)</code>","text":"<p>Fetches stdout and stderr for a DRMAA job, allowing for some lag.</p> <p>Arguments: - <code>stdout_path</code> (string): Path to the stdout file. - <code>stderr_path</code> (string): Path to the stderr file. - <code>tries</code> (int, optional): Number of attempts to retrieve the files. - <code>encoding</code> (string, optional): Encoding for reading files.</p> <p>Returns: - <code>tuple</code>: stdout and stderr as lists of strings.</p>"},{"location":"pipeline_modules/cluster/#set_drmaa_job_pathsjob_template-job_path","title":"<code>set_drmaa_job_paths(job_template, job_path)</code>","text":"<p>Adds the job path, stdout path, and stderr path to the job template.</p> <p>Arguments: - <code>job_template</code>: DRMAA job template object. - <code>job_path</code> (string): Path to the job script.</p>"},{"location":"pipeline_modules/cluster/#cluster-specific-classes","title":"Cluster-Specific Classes","text":"<p>The following classes inherit from <code>DRMAACluster</code> and implement cluster-specific logic for each cluster type.</p>"},{"location":"pipeline_modules/cluster/#sgecluster","title":"<code>SGECluster</code>","text":"<p>Handles SGE-specific cluster job setup.</p> <ul> <li><code>get_native_specification(...)</code>: Returns native specification parameters for SGE jobs.</li> </ul>"},{"location":"pipeline_modules/cluster/#slurmcluster","title":"<code>SlurmCluster</code>","text":"<p>Handles SLURM-specific cluster job setup.</p> <ul> <li><code>get_native_specification(...)</code>: Returns native specification parameters for SLURM jobs.</li> <li><code>parse_accounting_data(...)</code>: Parses SLURM accounting data to retrieve resource usage information.</li> </ul>"},{"location":"pipeline_modules/cluster/#torquecluster","title":"<code>TorqueCluster</code>","text":"<p>Handles Torque-specific cluster job setup.</p> <ul> <li><code>get_native_specification(...)</code>: Returns native specification parameters for Torque jobs.</li> </ul>"},{"location":"pipeline_modules/cluster/#pbsprocluster","title":"<code>PBSProCluster</code>","text":"<p>Handles PBSPro-specific cluster job setup.</p> <ul> <li><code>get_native_specification(...)</code>: Returns native specification parameters for PBSPro jobs.</li> <li><code>update_template(jt)</code>: Updates the DRMAA job template environment.</li> </ul>"},{"location":"pipeline_modules/cluster/#get_queue_managerqueue_manager-args-kwargs","title":"<code>get_queue_manager(queue_manager, *args, **kwargs)</code>","text":"<p>Returns a cluster instance based on the specified queue manager type.</p> <p>Arguments: - <code>queue_manager</code> (string): Type of queue manager (<code>sge</code>, <code>slurm</code>, <code>torque</code>, <code>pbspro</code>). - <code>*args, **kwargs</code>: Additional arguments passed to the cluster class initialiser.</p> <p>Raises: - <code>ValueError</code>: If the queue manager type is not supported.</p> <pre><code>def get_queue_manager(queue_manager, *args, **kwargs):\n    qm = queue_manager.lower()\n    if qm == \"sge\":\n        return SGECluster(*args, **kwargs)\n    elif qm == \"slurm\":\n        return SlurmCluster(*args, **kwargs)\n    elif qm == \"torque\":\n        return TorqueCluster(*args, **kwargs)\n    elif qm == \"pbspro\":\n        return PBSProCluster(*args, **kwargs)\n    else:\n        raise ValueError(\"Queue manager {} not supported\".format(queue_manager))\n</code></pre>"},{"location":"pipeline_modules/cluster/#cluster-configuration","title":"Cluster Configuration","text":"<p>CGAT-core provides robust support for executing pipelines on various cluster platforms, including SLURM, SGE, and PBS/Torque.</p>"},{"location":"pipeline_modules/cluster/#supported-platforms","title":"Supported Platforms","text":"<ol> <li>SLURM Workload Manager</li> <li>Modern, scalable cluster manager</li> <li>Extensive resource control</li> <li> <p>Fair-share scheduling</p> </li> <li> <p>Sun Grid Engine (SGE)</p> </li> <li>Traditional cluster system</li> <li>Wide deployment base</li> <li> <p>Flexible job control</p> </li> <li> <p>PBS/Torque</p> </li> <li>Professional batch system</li> <li>Advanced scheduling</li> <li>Resource management</li> </ol>"},{"location":"pipeline_modules/cluster/#configuration","title":"Configuration","text":""},{"location":"pipeline_modules/cluster/#basic-setup","title":"Basic Setup","text":"<p>Create <code>.cgat.yml</code> in your home directory:</p> <pre><code>cluster:\n    # Queue manager type (slurm, sge, pbspro, torque)\n    queue_manager: slurm\n\n    # Default queue\n    queue: main\n\n    # Memory resource identifier\n    memory_resource: mem\n\n    # Default memory per job\n    memory_default: 4G\n\n    # Parallel environment\n    parallel_environment: dedicated\n\n    # Maximum concurrent jobs\n    max_jobs: 100\n\n    # Job priority\n    priority: 0\n</code></pre>"},{"location":"pipeline_modules/cluster/#platform-specific-configuration","title":"Platform-Specific Configuration","text":""},{"location":"pipeline_modules/cluster/#slurm-configuration","title":"SLURM Configuration","text":"<pre><code>cluster:\n    queue_manager: slurm\n    options: --time=00:10:00 --cpus-per-task=8 --mem=1G\n    queue: main\n    memory_resource: mem\n    parallel_environment: dedicated\n</code></pre>"},{"location":"pipeline_modules/cluster/#sge-configuration","title":"SGE Configuration","text":"<pre><code>cluster:\n    queue_manager: sge\n    options: -l h_rt=00:10:00\n    queue: all.q\n    memory_resource: h_vmem\n    parallel_environment: smp\n</code></pre>"},{"location":"pipeline_modules/cluster/#pbstorque-configuration","title":"PBS/Torque Configuration","text":"<pre><code>cluster:\n    queue_manager: torque\n    options: -l walltime=00:10:00 -l nodes=1:ppn=8\n    queue: batch\n    memory_resource: mem\n    parallel_environment: dedicated\n</code></pre>"},{"location":"pipeline_modules/cluster/#resource-management","title":"Resource Management","text":""},{"location":"pipeline_modules/cluster/#memory-allocation","title":"Memory Allocation","text":"<pre><code>@transform(\"*.bam\", suffix(\".bam\"), \".sorted.bam\")\ndef sort_bam(infile, outfile):\n    \"\"\"Sort BAM file with specific memory requirements.\"\"\"\n    job_memory = \"8G\"\n    job_threads = 4\n\n    statement = \"\"\"\n    samtools sort \n        -@ %(job_threads)s \n        -m %(job_memory)s \n        %(infile)s &gt; %(outfile)s\n    \"\"\"\n    P.run(statement)\n</code></pre>"},{"location":"pipeline_modules/cluster/#cpu-allocation","title":"CPU Allocation","text":"<pre><code>@transform(\"*.fa\", suffix(\".fa\"), \".indexed\")\ndef index_genome(infile, outfile):\n    \"\"\"Index genome using multiple cores.\"\"\"\n    job_threads = 8\n\n    statement = \"\"\"\n    bwa index \n        -t %(job_threads)s \n        %(infile)s\n    \"\"\"\n    P.run(statement)\n</code></pre>"},{"location":"pipeline_modules/cluster/#temporary-directory","title":"Temporary Directory","text":"<pre><code>@transform(\"*.bam\", suffix(\".bam\"), \".sorted.bam\")\ndef sort_with_temp(infile, outfile):\n    \"\"\"Sort using specific temporary directory.\"\"\"\n    tmpdir = P.get_temp_dir()\n\n    statement = \"\"\"\n    samtools sort \n        -T %(tmpdir)s/sort \n        %(infile)s &gt; %(outfile)s\n    \"\"\"\n    P.run(statement)\n</code></pre>"},{"location":"pipeline_modules/cluster/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"pipeline_modules/cluster/#job-dependencies","title":"Job Dependencies","text":"<pre><code>@follows(previous_task)\n@transform(\"*.txt\", suffix(\".txt\"), \".processed\")\ndef dependent_task(infile, outfile):\n    \"\"\"Task that depends on previous_task completion.\"\"\"\n    P.run(\"process_file %(infile)s &gt; %(outfile)s\")\n</code></pre>"},{"location":"pipeline_modules/cluster/#resource-scaling","title":"Resource Scaling","text":"<pre><code>@transform(\"*.bam\", suffix(\".bam\"), \".stats\")\ndef calculate_stats(infile, outfile):\n    \"\"\"Scale resources based on input size.\"\"\"\n    infile_size = os.path.getsize(infile)\n    job_memory = \"%dG\" % max(4, infile_size // (1024**3) + 2)\n    job_threads = min(4, os.cpu_count())\n\n    statement = \"\"\"\n    samtools stats \n        -@ %(job_threads)s \n        %(infile)s &gt; %(outfile)s\n    \"\"\"\n    P.run(statement)\n</code></pre>"},{"location":"pipeline_modules/cluster/#queue-selection","title":"Queue Selection","text":"<pre><code>@transform(\"*.big\", suffix(\".big\"), \".processed\")\ndef process_big_file(infile, outfile):\n    \"\"\"Use specific queue for large jobs.\"\"\"\n    job_queue = \"bigmem\"\n    job_memory = \"64G\"\n\n    statement = \"\"\"\n    process_large_file %(infile)s &gt; %(outfile)s\n    \"\"\"\n    P.run(statement)\n</code></pre>"},{"location":"pipeline_modules/cluster/#best-practices","title":"Best Practices","text":""},{"location":"pipeline_modules/cluster/#1-resource-specification","title":"1. Resource Specification","text":"<ul> <li>Always specify memory requirements</li> <li>Set appropriate number of threads</li> <li>Use queue-specific options wisely</li> <li>Consider input file sizes</li> </ul>"},{"location":"pipeline_modules/cluster/#2-error-handling","title":"2. Error Handling","text":"<pre><code>try:\n    P.run(statement)\nexcept P.PipelineError as e:\n    L.error(\"Cluster job failed: %s\" % e)\n    # Cleanup and resubmit if needed\n    cleanup_and_resubmit()\n</code></pre>"},{"location":"pipeline_modules/cluster/#3-performance-optimization","title":"3. Performance Optimization","text":"<ul> <li>Group small tasks</li> <li>Use appropriate chunk sizes</li> <li>Monitor resource usage</li> <li>Clean up temporary files</li> </ul>"},{"location":"pipeline_modules/cluster/#4-monitoring","title":"4. Monitoring","text":"<pre><code># Log resource usage\n@transform(\"*.bam\", suffix(\".bam\"), \".sorted.bam\")\ndef monitored_sort(infile, outfile):\n    \"\"\"Monitor resource usage during sort.\"\"\"\n    job_memory = \"8G\"\n    job_threads = 4\n\n    statement = \"\"\"\n    { time samtools sort %(infile)s &gt; %(outfile)s ; } 2&gt; %(outfile)s.metrics\n    \"\"\"\n    P.run(statement)\n</code></pre>"},{"location":"pipeline_modules/cluster/#troubleshooting","title":"Troubleshooting","text":""},{"location":"pipeline_modules/cluster/#common-issues","title":"Common Issues","text":"<ol> <li>Job Failures</li> <li>Check error logs</li> <li>Verify resource requirements</li> <li> <p>Monitor cluster status</p> </li> <li> <p>Resource Exhaustion</p> </li> <li>Adjust memory limits</li> <li>Check disk space</li> <li> <p>Monitor CPU usage</p> </li> <li> <p>Queue Issues</p> </li> <li>Verify queue availability</li> <li>Check user limits</li> <li>Monitor queue status</li> </ol>"},{"location":"pipeline_modules/cluster/#debugging-tips","title":"Debugging Tips","text":"<ol> <li> <p>Enable Detailed Logging <pre><code>import logging\nlogging.basicConfig(level=logging.DEBUG)\n</code></pre></p> </li> <li> <p>Test Jobs Locally <pre><code>python pipeline.py make task --local\n</code></pre></p> </li> <li> <p>Monitor Resource Usage <pre><code>python pipeline.py make task --cluster-queue=main --debug\n</code></pre></p> </li> </ol>"},{"location":"pipeline_modules/cluster/#security-considerations","title":"Security Considerations","text":"<ol> <li>Access Control</li> <li>Use appropriate permissions</li> <li>Implement job quotas</li> <li> <p>Monitor user activity</p> </li> <li> <p>Data Protection</p> </li> <li>Secure temporary files</li> <li>Clean up job artifacts</li> <li>Protect sensitive data</li> </ol> <p>For more information, see: - SLURM Documentation - SGE Documentation - PBS Documentation</p>"},{"location":"pipeline_modules/cluster/#notes","title":"Notes","text":"<ul> <li>This module provides a unified interface for running cluster jobs across different cluster managers, allowing the user to switch between cluster types without rewriting job submission scripts.</li> <li>The module includes timeout settings for managing gevent event loops (<code>GEVENT_TIMEOUT_SACCT</code> and <code>GEVENT_TIMEOUT_WAIT</code>) to ensure that jobs are properly monitored without excessive waiting.</li> <li>The <code>JobInfo</code> named tuple is used to encapsulate job information, including job ID and resource usage.</li> </ul>"},{"location":"pipeline_modules/cluster/#supported-clusters","title":"Supported Clusters","text":"<ul> <li>SGE (Sun Grid Engine)</li> <li>SLURM (Simple Linux Utility for Resource Management)</li> <li>Torque</li> <li>PBSPro</li> </ul> <p>Each cluster type requires specific configurations and resource definitions, which are managed through the appropriate cluster class.</p>"},{"location":"pipeline_modules/cluster/#usage-example","title":"Usage Example","text":"<p>To use a specific cluster type, you would first initialise the relevant cluster class or use the <code>get_queue_manager()</code> function to automatically return an instance:</p> <pre><code>from cluster import get_queue_manager\n\nqueue_manager = \"slurm\"\ncluster = get_queue_manager(queue_manager, session=drmaa.Session(), ignore_errors=True)\n</code></pre> <p>Once the cluster is initialised, you can use its methods to create job templates, submit jobs, and manage their execution.</p> <p>As you continue to expand the functionality of CGAT-core, ensure that this module is updated with new cluster types, resource mappings, and relevant updates for managing cluster jobs effectively.</p>"},{"location":"pipeline_modules/control/","title":"Pipeline Control Module","text":"<p>The Control module in CGAT-core is responsible for managing the overall execution flow of the pipeline. It provides functions and classes for running the pipeline, handling command-line arguments, and controlling the pipeline's behaviour.</p>"},{"location":"pipeline_modules/control/#key-functions","title":"Key Functions","text":""},{"location":"pipeline_modules/control/#run_pipeline","title":"<code>run_pipeline()</code>","text":"<p>This function is the main entry point for executing a pipeline. It sets up the pipeline, processes command-line arguments, and runs the specified tasks.</p> <pre><code>from cgatcore import pipeline as P\n\ndef my_pipeline():\n    # Define your pipeline tasks here\n    pass\n\nif __name__ == \"__main__\":\n    P.run_pipeline(pipeline_func=my_pipeline)\n</code></pre>"},{"location":"pipeline_modules/control/#get_parameters","title":"<code>get_parameters()</code>","text":"<p>Retrieves pipeline parameters from configuration files and command-line arguments.</p> <pre><code>PARAMS = P.get_parameters(\"pipeline.yml\")\n</code></pre>"},{"location":"pipeline_modules/control/#pipeline-class","title":"<code>Pipeline</code> Class","text":"<p>The <code>Pipeline</code> class is the core class for managing pipeline execution. It provides methods for adding tasks, running the pipeline, and handling dependencies.</p> <pre><code>pipeline = P.Pipeline()\npipeline.add_task(my_task)\npipeline.run()\n</code></pre>"},{"location":"pipeline_modules/control/#command-line-interface","title":"Command-line Interface","text":"<p>The Control module provides a command-line interface for running pipelines. Common options include:</p> <ul> <li><code>--pipeline-action</code>: Specify the action to perform (e.g., <code>show</code>, <code>plot</code>, <code>run</code>)</li> <li><code>--local</code>: Run the pipeline locally instead of on a cluster</li> <li><code>--multiprocess</code>: Specify the number of processes to use for local execution</li> </ul>"},{"location":"pipeline_modules/control/#example-usage","title":"Example usage:","text":"<pre><code>python my_pipeline.py --pipeline-action run --local\n</code></pre> <p>For more detailed information on pipeline control and execution, refer to the Pipeline Execution documentation.</p>"},{"location":"pipeline_modules/control/#next-steps","title":"Next Steps","text":"<p>These new pages provide more comprehensive documentation for the CGAT-core pipeline modules and S3 integration. You should create similar pages for the other modules (Database, Files, Cluster, Execution, Utils, Parameters) and S3-related topics (S3 Decorators, Configuring S3).</p> <p>Remember to include code examples, explanations of key concepts, and links to other relevant parts of the documentation. As you continue to develop and expand the CGAT-core functionality, make sure to update the documentation accordingly.</p>"},{"location":"pipeline_modules/database/","title":"Database Module","text":"<p><code>database.py</code> - Database upload for cgatcore pipelines</p> <p>=========================================================</p>"},{"location":"pipeline_modules/database/#reference","title":"Reference","text":"<p>This module contains functions to facilitate data upload into a database using CGAT-core. It is particularly useful for integrating with cgatcore pipelines for automating and managing complex workflows.</p>"},{"location":"pipeline_modules/database/#import-statements","title":"Import Statements","text":"<pre><code>import re\nimport os\nimport sqlite3\nimport sqlalchemy\nfrom cgatcore import database as database\nimport cgatcore.experiment as E\nfrom cgatcore.iotools import snip, touch_file\nfrom cgatcore.pipeline.files import get_temp_file\nfrom cgatcore.pipeline.execution import run\nfrom cgatcore.pipeline.parameters import get_params\n</code></pre>"},{"location":"pipeline_modules/database/#key-functions","title":"Key Functions","text":""},{"location":"pipeline_modules/database/#tablequotetrack","title":"<code>tablequote(track)</code>","text":"<p>Quotes a track name to make it suitable as a table name.</p> <pre><code>def tablequote(track):\n    '''quote a track name such that it is suitable as a table name.'''\n    return re.sub(r\"[-(),\\[\\].]\", \"_\", track)\n</code></pre>"},{"location":"pipeline_modules/database/#to_tableoutfile","title":"<code>to_table(outfile)</code>","text":"<p>Converts a filename from a load statement into a table name. Checks if the filename ends with <code>.load</code>, removes the suffix, and then quotes it.</p> <p>Arguments: - <code>outfile</code> (string): Filename ending in <code>.load</code>.</p> <p>Returns: - <code>tablename</code> (string): A suitable table name derived from the file.</p> <pre><code>def to_table(outfile):\n    '''convert a filename from a load statement into a table name.'''\n    assert outfile.endswith(\".load\")\n    name = os.path.basename(outfile[:-len(\".load\")])\n    return tablequote(name)\n</code></pre>"},{"location":"pipeline_modules/database/#build_load_statementtablename-retrytrue-options","title":"<code>build_load_statement(tablename, retry=True, options=\"\")</code>","text":"<p>Builds a command line statement to upload data to the database via the <code>csv2db</code> script.</p> <p>Arguments: - <code>tablename</code> (string): Tablename for upload. - <code>retry</code> (bool): If <code>True</code>, add the <code>--retry</code> option to <code>csv2db.py</code>. - <code>options</code> (string): Command line options to be passed on to <code>csv2db.py</code>.</p> <p>Returns: - <code>string</code>: A command line statement for uploading data.</p> <pre><code>def build_load_statement(tablename, retry=True, options=\"\"):\n    opts = []\n    if retry:\n        opts.append(\" --retry \")\n    params = get_params()\n    opts.append(\"--database-url={}\".format(params[\"database\"][\"url\"]))\n    db_options = \" \".join(opts)\n    load_statement = (\n        \"python -m cgatcore.csv2db {db_options} {options} --table={tablename}\".format(**locals()))\n    return load_statement\n</code></pre>"},{"location":"pipeline_modules/database/#load","title":"<code>load(...)</code>","text":"<p>Imports data from a tab-separated file into the database.</p> <p>Arguments: - <code>infile</code> (string): Filename of the input data. - <code>outfile</code> (string): Output filename containing logging information. - Various additional arguments to control the loading behaviour.</p> <p>Typical usage within Ruffus:</p> <pre><code>@transform(\"*.tsv.gz\", suffix(\".tsv.gz\"), \".load\")\ndef loadData(infile, outfile):\n    P.load(infile, outfile)\n</code></pre>"},{"location":"pipeline_modules/database/#concatenate_and_load","title":"<code>concatenate_and_load(...)</code>","text":"<p>Concatenates multiple tab-separated files and uploads the result to the database.</p> <p>Arguments: - <code>infiles</code> (list): List of input filenames. - <code>outfile</code> (string): Output filename. - Various additional arguments for concatenation and loading.</p> <p>Typical usage within Ruffus:</p> <pre><code>@merge(\"*.tsv.gz\", \".load\")\ndef loadData(infiles, outfile):\n    P.concatenate_and_load(infiles, outfile)\n</code></pre>"},{"location":"pipeline_modules/database/#merge_and_load","title":"<code>merge_and_load(...)</code>","text":"<p>Merges multiple categorical tables and loads them into the database.</p> <p>Arguments: - <code>infiles</code> (list): List of input files. - <code>outfile</code> (string): Output filename. - Various additional arguments to control the merging and loading behaviour.</p>"},{"location":"pipeline_modules/database/#connect","title":"<code>connect()</code>","text":"<p>Connects to the SQLite database used in the pipeline. Currently only implemented for SQLite databases.</p> <p>Returns: - <code>dbh</code>: A database handle.</p>"},{"location":"pipeline_modules/database/#create_view","title":"<code>create_view(...)</code>","text":"<p>Creates a database view for a list of tables by performing a join across them.</p> <p>Arguments: - <code>dbhandle</code>: Database handle. - <code>tables</code>: List of tuples containing table names and fields to join. - <code>tablename</code> (string): Name of the view or table to be created. - <code>view_type</code> (string): Type of view, either <code>VIEW</code> or <code>TABLE</code>.</p>"},{"location":"pipeline_modules/database/#get_database_name","title":"<code>get_database_name()</code>","text":"<p>Returns the database name associated with the pipeline. Implemented for backwards compatibility.</p>"},{"location":"pipeline_modules/database/#utility-functions","title":"Utility Functions","text":"<p>These functions assist in interacting with a database in various ways:</p> <ul> <li><code>load_from_iterator(...)</code>: Imports data from an iterator into a database.</li> <li><code>apsw_connect(dbname, modname=\"tsv\")</code>: Attempts to connect to an APSW database, creating a virtual table from a TSV file.</li> </ul>"},{"location":"pipeline_modules/database/#database-utility-functions","title":"Database Utility Functions","text":"<p><code>database.py</code> - Database utility functions</p> <p>===========================================</p> <p>This module contains convenience functions to work with a relational database.</p>"},{"location":"pipeline_modules/database/#executewaitdbhandle-statement-regex_errorlocked-retries-1-wait5","title":"<code>executewait(dbhandle, statement, regex_error=\"locked\", retries=-1, wait=5)</code>","text":"<p>Repeatedly executes an SQL statement until it succeeds.</p> <p>Arguments: - <code>dbhandle</code>: A DB-API conform database handle. - <code>statement</code>: SQL statement to execute. - <code>regex_error</code>: Regex to match error messages to ignore.</p> <p>Returns: - <code>Cursor</code>: A cursor object for further database operations.</p>"},{"location":"pipeline_modules/database/#getcolumnnamesdbhandle-table","title":"<code>getColumnNames(dbhandle, table)</code>","text":"<p>Returns the column names of a table from the database.</p>"},{"location":"pipeline_modules/database/#gettablesdbhandle","title":"<code>getTables(dbhandle)</code>","text":"<p>Gets a list of tables in an SQLite database.</p>"},{"location":"pipeline_modules/database/#totsvdbhandle-outfile-statement-remove_nonetrue","title":"<code>toTSV(dbhandle, outfile, statement, remove_none=True)</code>","text":"<p>Executes a statement and saves the result as a TSV file to disk.</p>"},{"location":"pipeline_modules/database/#database-interaction-functions","title":"Database Interaction Functions","text":"<ul> <li><code>connect(dbhandle=None, attach=None, url=None)</code>: Attempts to connect to a database, returning a database handle.</li> <li><code>execute(queries, dbhandle=None, attach=False)</code>: Executes one or more SQL statements against a database.</li> <li><code>fetch(query, dbhandle=None, attach=False)</code>: Fetches all query results and returns them.</li> <li><code>fetch_with_names(query, dbhandle=None, attach=False)</code>: Fetches query results and returns them as an array of row arrays, including field names.</li> <li><code>fetch_DataFrame(query, dbhandle=None, attach=False)</code>: Fetches query results and returns them as a pandas DataFrame.</li> <li><code>write_DataFrame(dataframe, tablename, dbhandle=None, index=False, if_exists='replace')</code>: Writes a pandas DataFrame to an SQLite database.</li> </ul>"},{"location":"pipeline_modules/database/#virtual-table-creation-with-apsw","title":"Virtual Table Creation with APSW","text":"<ul> <li><code>apsw_connect(dbname=None, modname=\"tsv\")</code>: Connects to an APSW database and creates a virtual table from a TSV file.</li> <li><code>_VirtualTable</code> and <code>_Table</code> classes: Defines the structure and methods to support virtual tables in APSW.</li> </ul>"},{"location":"pipeline_modules/database/#notes-and-recommendations","title":"Notes and Recommendations","text":"<ul> <li>As you continue to expand and develop the CGAT-core functionality, ensure to update the database module documentation accordingly.</li> <li>This module heavily utilises <code>csv2db</code> to facilitate data upload and management.</li> <li>Always consider potential SQL locking issues and use retry mechanisms where applicable.</li> </ul>"},{"location":"pipeline_modules/execution/","title":"Execution Module","text":"<p><code>execution.py</code> - Job control for cgatcore pipelines</p> <p>=========================================================</p> <p>This module manages the job execution for cgatcore pipelines, particularly using DRMAA sessions for cluster job management. It provides functionality to start and close DRMAA sessions, execute shell commands, manage job submission, and handle different cluster environments. Supported cluster types include SGE, SLURM, Torque, and Kubernetes.</p>"},{"location":"pipeline_modules/execution/#reference","title":"Reference","text":"<p>The following documentation details the execution management utilities provided by the <code>execution.py</code> module.</p>"},{"location":"pipeline_modules/execution/#import-statements","title":"Import Statements","text":"<pre><code>import collections\nimport importlib\nimport os\nimport pickle\nimport re\nimport json\nimport stat\nimport socket\nimport logging\nimport subprocess\nimport sys\nimport time\nimport math\nimport shutil\nimport gevent\nimport signal\nimport cgatcore.experiment as E\nimport cgatcore.iotools as iotools\nfrom cgatcore.pipeline.utils import get_caller_locals, get_caller, get_calling_function\nfrom cgatcore.pipeline.files import get_temp_filename, get_temp_dir\nfrom cgatcore.pipeline.parameters import substitute_parameters, get_params\nfrom cgatcore.pipeline.cluster import get_queue_manager, JobInfo\nfrom cgatcore.pipeline.executors import SGEExecutor, SlurmExecutor, TorqueExecutor, LocalExecutor\ntry:\n    from cgatcore.pipeline.kubernetes import KubernetesExecutor\nexcept ImportError:\n    KubernetesExecutor = None  # Fallback if Kubernetes is not available\n</code></pre>"},{"location":"pipeline_modules/execution/#key-functions","title":"Key Functions","text":""},{"location":"pipeline_modules/execution/#start_session","title":"<code>start_session()</code>","text":"<p>Starts and initializes the global DRMAA session.</p> <pre><code>def start_session():\n    \"\"\"Start and initialize the global DRMAA session.\"\"\"\n    global GLOBAL_SESSION\n\n    if HAS_DRMAA and GLOBAL_SESSION is None:\n        GLOBAL_SESSION = drmaa.Session()\n        try:\n            GLOBAL_SESSION.initialize()\n        except drmaa.errors.InternalException as ex:\n            get_logger().warn(\"could not initialize global drmaa session: {}\".format(ex))\n            GLOBAL_SESSION = None\n        return GLOBAL_SESSION\n</code></pre>"},{"location":"pipeline_modules/execution/#close_session","title":"<code>close_session()</code>","text":"<p>Closes the global DRMAA session.</p> <pre><code>def close_session():\n    \"\"\"Close the global DRMAA session.\"\"\"\n    global GLOBAL_SESSION\n\n    if GLOBAL_SESSION is not None:\n        GLOBAL_SESSION.exit()\n        GLOBAL_SESSION = None\n</code></pre>"},{"location":"pipeline_modules/execution/#get_executoroptionsnone","title":"<code>get_executor(options=None)</code>","text":"<p>Returns an executor instance based on the specified queue manager in the options.</p> <p>Arguments: - <code>options</code> (dict): Dictionary containing execution options, including <code>\"cluster_queue_manager\"</code>.</p> <p>Returns: - Executor instance appropriate for the specified queue manager.</p> <p>This function decides which executor to use depending on the queue manager specified in the options, defaulting to the local executor if no cluster is specified or the cluster is not supported.</p> <pre><code>def get_executor(options=None):\n    if options is None:\n        options = get_params()\n\n    if options.get(\"testing\", False):\n        return LocalExecutor(**options)\n\n    if not options.get(\"to_cluster\", True):\n        return LocalExecutor(**options)\n\n    queue_manager = options.get(\"cluster_queue_manager\", None)\n\n    if queue_manager == \"kubernetes\" and KubernetesExecutor is not None:\n        return KubernetesExecutor(**options)\n    elif queue_manager == \"sge\" and shutil.which(\"qsub\") is not None:\n        return SGEExecutor(**options)\n    elif queue_manager == \"slurm\" and shutil.which(\"sbatch\") is not None:\n        return SlurmExecutor(**options)\n    elif queue_manager == \"torque\" and shutil.which(\"qsub\") is not None:\n        return TorqueExecutor(**options)\n    else:\n        return LocalExecutor(**options)\n</code></pre>"},{"location":"pipeline_modules/execution/#executestatement-kwargs","title":"<code>execute(statement, **kwargs)</code>","text":"<p>Executes a command line statement locally.</p> <p>Arguments: - <code>statement</code> (string): Command line statement to be run.</p> <p>Returns: - <code>stdout</code> (string): Data sent to standard output by the command. - <code>stderr</code> (string): Data sent to standard error by the command.</p> <pre><code>def execute(statement, **kwargs):\n    if not kwargs:\n        kwargs = get_caller_locals()\n\n    kwargs = dict(list(get_params().items()) + list(kwargs.items()))\n\n    logger = get_logger()\n    logger.info(\"running %s\" % (statement % kwargs))\n\n    if \"cwd\" not in kwargs:\n        cwd = get_params()[\"work_dir\"]\n    else:\n        cwd = kwargs[\"cwd\"]\n\n    statement = \" \".join(re.sub(\"\\t+\", \" \", statement).split(\"\\n\")).strip()\n    if statement.endswith(\";\"):\n        statement = statement[:-1]\n\n    os.environ.update({'BASH_ENV': os.path.join(os.environ['HOME'], '.bashrc')})\n    process = subprocess.Popen(statement % kwargs,\n                               cwd=cwd,\n                               shell=True,\n                               stdin=sys.stdin,\n                               stdout=sys.stdout,\n                               stderr=sys.stderr,\n                               env=os.environ.copy(),\n                               executable=\"/bin/bash\")\n\n    stdout, stderr = process.communicate()\n\n    if process.returncode != 0:\n        raise OSError(\n            \"Child was terminated by signal %i: \\n\"\n            \"The stderr was: \\n%s\\n%s\\n\" %\n            (-process.returncode, stderr, statement))\n\n    return stdout, stderr\n</code></pre>"},{"location":"pipeline_modules/execution/#runstatement-kwargs","title":"<code>run(statement, **kwargs)</code>","text":"<p>Runs a command line statement, either locally or on a cluster using DRMAA.</p> <p>Arguments: - <code>statement</code> (string or list of strings): Command line statement or list of statements to execute. - <code>kwargs</code> (dict): Additional options for job execution.</p> <p>This function runs the given statement(s) by selecting the appropriate executor. It handles different types of job submission (local or cluster-based) based on the provided arguments and global configuration.</p> <pre><code>def run(statement, **kwargs):\n    \"\"\"\n    Run a command line statement.\n    \"\"\"\n    logger = get_logger()\n\n    options = dict(list(get_params().items()))\n    caller_options = get_caller_locals()\n    options.update(list(caller_options.items()))\n\n    if \"self\" in options:\n        del options[\"self\"]\n    options.update(list(kwargs.items()))\n\n    if \"params\" in options:\n        try:\n            options.update(options[\"params\"]._asdict())\n        except AttributeError:\n            pass\n\n    options['cluster']['options'] = options.get(\n        'job_options', options['cluster']['options'])\n    options['cluster']['queue'] = options.get(\n        'job_queue', options['cluster']['queue'])\n    options['without_cluster'] = options.get('without_cluster')\n\n    name_substrate = str(options.get(\"outfile\", \"cgatcore\"))\n    if os.path.basename(name_substrate).startswith(\"result\"):\n        name_substrate = os.path.basename(os.path.dirname(name_substrate))\n    else:\n        name_substrate = os.path.basename(name_substrate)\n\n    options[\"job_name\"] = re.sub(\"[:\"]\", \"_\", name_substrate)\n    try:\n        calling_module = get_caller().__name__\n    except AttributeError:\n        calling_module = \"unknown\"\n\n    options[\"task_name\"] = calling_module + \".\" + get_calling_function()\n\n    if isinstance(statement, list):\n        statement_list = [interpolate_statement(stmt, options) for stmt in statement]\n    else:\n        statement_list = [interpolate_statement(statement, options)]\n\n    if options.get(\"dryrun\", False):\n        for statement in statement_list:\n            logger.info(\"Dry-run: {}\".format(statement))\n        return []\n\n    executor = get_executor(options)\n\n    with executor as e:\n        benchmark_data = e.run(statement_list)\n\n    for data in benchmark_data:\n        logger.info(json.dumps(data))\n\n    BenchmarkData = collections.namedtuple('BenchmarkData', sorted(benchmark_data[0]))\n    return [BenchmarkData(**d) for d in benchmark_data]\n</code></pre>"},{"location":"pipeline_modules/execution/#notes","title":"Notes","text":"<ul> <li>This module is responsible for the orchestration and execution of jobs either locally or on different cluster environments (e.g., SGE, Slurm, Torque).</li> <li>The global DRMAA session is managed to interface with cluster schedulers, and the <code>GLOBAL_SESSION</code> variable maintains the session state.</li> <li>Executors are used to control where and how the jobs are executed, allowing for local and cluster-based execution. Depending on the queue manager specified, different executor classes such as <code>LocalExecutor</code>, <code>SGEExecutor</code>, <code>SlurmExecutor</code>, <code>TorqueExecutor</code>, or <code>KubernetesExecutor</code> are instantiated.</li> </ul>"},{"location":"pipeline_modules/execution/#supported-cluster-types","title":"Supported Cluster Types","text":"<ul> <li>SGE (Sun Grid Engine)</li> <li>SLURM (Simple Linux Utility for Resource Management)</li> <li>Torque</li> <li>Kubernetes</li> </ul>"},{"location":"pipeline_modules/execution/#usage-example","title":"Usage Example","text":"<p>To use the <code>run()</code> function to execute a command either locally or on a cluster:</p> <pre><code>from execution import run\n\nstatement = \"echo 'Hello, world!'\"\nrun(statement)\n</code></pre> <p>The execution environment is determined by the configuration parameters, and the job is run on the</p>"},{"location":"pipeline_modules/executors/","title":"Executors for job scheduling","text":""},{"location":"pipeline_modules/executors/#overview","title":"Overview","text":"<p>This documentation describes several executor classes for job scheduling in computational pipelines. Each of these classes inherits from the <code>BaseExecutor</code> and is responsible for submitting jobs to a different type of cluster system or local machine. The following executors are available:</p> <ul> <li><code>SGEExecutor</code>: Submits jobs to an SGE (Sun Grid Engine) cluster.</li> <li><code>SlurmExecutor</code>: Submits jobs to a Slurm cluster.</li> <li><code>TorqueExecutor</code>: Submits jobs to a Torque cluster.</li> <li><code>LocalExecutor</code>: Executes jobs locally.</li> <li><code>KubernetesExecutor</code>: Submits jobs to a Kubernetes cluster.</li> </ul> <p>Each executor has specific methods and logging functionality that enable it to handle job submission, monitoring, and error management effectively.</p>"},{"location":"pipeline_modules/executors/#sgeexecutor","title":"<code>SGEExecutor</code>","text":"<p>The <code>SGEExecutor</code> is responsible for running jobs on an SGE cluster. It extends the <code>BaseExecutor</code> class.</p>"},{"location":"pipeline_modules/executors/#methods","title":"Methods","text":""},{"location":"pipeline_modules/executors/#__init__self-kwargs","title":"<code>__init__(self, **kwargs)</code>","text":"<p>Initialises the <code>SGEExecutor</code> and sets up a logger for the instance.</p>"},{"location":"pipeline_modules/executors/#runself-statement_list","title":"<code>run(self, statement_list)</code>","text":"<p>Runs the provided list of statements using SGE.</p> <ul> <li>Arguments:</li> <li> <p><code>statement_list</code>: A list of shell command statements to be executed.</p> </li> <li> <p>Workflow:</p> </li> <li>Builds an SGE job submission command for each statement.</li> <li>Uses <code>subprocess.run()</code> to submit jobs using the <code>qsub</code> command.</li> <li>Handles job submission errors, logs relevant information, and monitors job completion.</li> </ul>"},{"location":"pipeline_modules/executors/#build_job_scriptself-statement","title":"<code>build_job_script(self, statement)</code>","text":"<p>Builds a job script for SGE based on the provided statement.</p> <ul> <li>Overrides: This method is an override of the <code>BaseExecutor.build_job_script()</code>.</li> </ul>"},{"location":"pipeline_modules/executors/#slurmexecutor","title":"<code>SlurmExecutor</code>","text":"<p>The <code>SlurmExecutor</code> is responsible for running jobs on a Slurm cluster. It also extends the <code>BaseExecutor</code> class.</p>"},{"location":"pipeline_modules/executors/#methods_1","title":"Methods","text":""},{"location":"pipeline_modules/executors/#__init__self-kwargs_1","title":"<code>__init__(self, **kwargs)</code>","text":"<p>Initialises the <code>SlurmExecutor</code> and sets up a logger for the instance.</p>"},{"location":"pipeline_modules/executors/#runself-statement_list_1","title":"<code>run(self, statement_list)</code>","text":"<p>Runs the provided list of statements using Slurm.</p> <ul> <li>Arguments:</li> <li> <p><code>statement_list</code>: A list of shell command statements to be executed.</p> </li> <li> <p>Workflow:</p> </li> <li>Builds a Slurm job submission command for each statement using <code>sbatch</code>.</li> <li>Uses <code>subprocess.run()</code> to submit jobs to the Slurm scheduler.</li> <li>Monitors the job submission status, logs relevant information, and handles any errors.</li> </ul>"},{"location":"pipeline_modules/executors/#build_job_scriptself-statement_1","title":"<code>build_job_script(self, statement)</code>","text":"<p>Builds a job script for submission on Slurm.</p> <ul> <li>Overrides: This method is an override of the <code>BaseExecutor.build_job_script()</code>.</li> </ul>"},{"location":"pipeline_modules/executors/#torqueexecutor","title":"<code>TorqueExecutor</code>","text":"<p>The <code>TorqueExecutor</code> class runs jobs on a Torque cluster, using <code>qsub</code> for job submissions.</p>"},{"location":"pipeline_modules/executors/#methods_2","title":"Methods","text":""},{"location":"pipeline_modules/executors/#__init__self-kwargs_2","title":"<code>__init__(self, **kwargs)</code>","text":"<p>Initialises the <code>TorqueExecutor</code> and sets up a logger for the instance.</p>"},{"location":"pipeline_modules/executors/#runself-statement_list_2","title":"<code>run(self, statement_list)</code>","text":"<p>Runs the provided list of statements using Torque.</p> <ul> <li>Arguments:</li> <li> <p><code>statement_list</code>: A list of shell command statements to be executed.</p> </li> <li> <p>Workflow:</p> </li> <li>Builds a job script and submits it using the <code>qsub</code> command.</li> <li>Uses <code>subprocess.run()</code> to handle the submission and logs all related information.</li> <li>Handles job submission errors and monitors job completion.</li> </ul>"},{"location":"pipeline_modules/executors/#build_job_scriptself-statement_2","title":"<code>build_job_script(self, statement)</code>","text":"<p>Builds a job script for submission on a Torque cluster.</p> <ul> <li>Overrides: This method is an override of the <code>BaseExecutor.build_job_script()</code>.</li> </ul>"},{"location":"pipeline_modules/executors/#localexecutor","title":"<code>LocalExecutor</code>","text":"<p>The <code>LocalExecutor</code> runs jobs on the local machine without the need for cluster scheduling. This is useful for development, testing, or when the jobs are small enough to run locally.</p>"},{"location":"pipeline_modules/executors/#methods_3","title":"Methods","text":""},{"location":"pipeline_modules/executors/#__init__self-kwargs_3","title":"<code>__init__(self, **kwargs)</code>","text":"<p>Initialises the <code>LocalExecutor</code> and sets up a logger for the instance.</p>"},{"location":"pipeline_modules/executors/#runself-statement_list_3","title":"<code>run(self, statement_list)</code>","text":"<p>Runs the provided list of statements locally.</p> <ul> <li>Arguments:</li> <li> <p><code>statement_list</code>: A list of shell command statements to be executed.</p> </li> <li> <p>Workflow:</p> </li> <li>Builds the job script and runs it locally using <code>subprocess.Popen()</code>.</li> <li>Monitors the output and logs the job status.</li> <li>Handles any runtime errors by logging them and raising exceptions as needed.</li> </ul>"},{"location":"pipeline_modules/executors/#build_job_scriptself-statement_3","title":"<code>build_job_script(self, statement)</code>","text":"<p>Builds a job script for local execution.</p> <ul> <li>Overrides: This method is an override of the <code>BaseExecutor.build_job_script()</code>.</li> </ul>"},{"location":"pipeline_modules/executors/#kubernetesexecutor","title":"<code>KubernetesExecutor</code>","text":"<p>The <code>KubernetesExecutor</code> is used for running jobs on a Kubernetes cluster.</p>"},{"location":"pipeline_modules/executors/#methods_4","title":"Methods","text":""},{"location":"pipeline_modules/executors/#__init__self-kwargs_4","title":"<code>__init__(self, **kwargs)</code>","text":"<p>Initialises the <code>KubernetesExecutor</code>.</p> <ul> <li>Workflow:</li> <li>Loads the Kubernetes configuration and sets up both Core and Batch API clients for job management.</li> <li>Logs information about successful or failed configuration loads.</li> </ul>"},{"location":"pipeline_modules/executors/#runself-statement-job_path-job_condaenv","title":"<code>run(self, statement, job_path, job_condaenv)</code>","text":"<p>Runs a job using Kubernetes.</p> <ul> <li>Arguments:</li> <li><code>statement</code>: The shell command to be executed within a Kubernetes job.</li> <li><code>job_path</code>: Path for the job files.</li> <li> <p><code>job_condaenv</code>: Conda environment to be used within the job container.</p> </li> <li> <p>Workflow:</p> </li> <li>Defines the Kubernetes job specification, including container image, command, and job parameters.</li> <li>Submits the job using <code>create_namespaced_job()</code> and waits for its completion.</li> <li>Collects job logs and benchmark data for analysis.</li> <li>Cleans up the Kubernetes job once it is complete.</li> </ul>"},{"location":"pipeline_modules/executors/#_wait_for_job_completionself-job_name","title":"<code>_wait_for_job_completion(self, job_name)</code>","text":"<p>Waits for the Kubernetes job to complete.</p> <ul> <li>Arguments:</li> <li> <p><code>job_name</code>: The name of the job.</p> </li> <li> <p>Workflow:</p> </li> <li>Repeatedly queries the job status using <code>read_namespaced_job_status()</code> until it succeeds or fails.</li> </ul>"},{"location":"pipeline_modules/executors/#_get_pod_logsself-job_name","title":"<code>_get_pod_logs(self, job_name)</code>","text":"<p>Retrieves the logs of the pod associated with the specified Kubernetes job.</p> <ul> <li>Arguments:</li> <li><code>job_name</code>: The name of the job.</li> </ul>"},{"location":"pipeline_modules/executors/#_cleanup_jobself-job_name","title":"<code>_cleanup_job(self, job_name)</code>","text":"<p>Deletes the Kubernetes job and its associated pods.</p> <ul> <li>Arguments:</li> <li><code>job_name</code>: The name of the job to be deleted.</li> </ul>"},{"location":"pipeline_modules/executors/#collect_benchmark_dataself-job_name-resource_usage_file","title":"<code>collect_benchmark_data(self, job_name, resource_usage_file)</code>","text":"<p>Collects benchmark data such as CPU and memory usage from the job's pod(s).</p> <ul> <li>Arguments:</li> <li><code>job_name</code>: Name of the job for which benchmark data is being collected.</li> <li><code>resource_usage_file</code>: Path to a file where resource usage data will be saved.</li> </ul>"},{"location":"pipeline_modules/executors/#collect_metric_dataself-process-start_time-end_time-time_data_file","title":"<code>collect_metric_data(self, process, start_time, end_time, time_data_file)</code>","text":"<p>Collects and saves metric data related to job duration.</p> <ul> <li>Arguments:</li> <li><code>process</code>: The name of the process.</li> <li><code>start_time</code>: Timestamp when the job started.</li> <li><code>end_time</code>: Timestamp when the job ended.</li> <li><code>time_data_file</code>: Path to a file where timing data will be saved.</li> </ul>"},{"location":"pipeline_modules/executors/#logging-and-error-handling","title":"Logging and Error Handling","text":"<p>All executor classes use the Python <code>logging</code> module to log different stages of job submission, execution, and monitoring. Logging levels like <code>INFO</code>, <code>ERROR</code>, and <code>WARNING</code> are used to provide information on job progress and errors. Executors also make use of exception handling to raise <code>RuntimeError</code> when job submission or execution fails.</p>"},{"location":"pipeline_modules/executors/#notes","title":"Notes","text":"<ul> <li>The job script generation is handled by the <code>build_job_script()</code> function, which is customised per executor but is based on the implementation from <code>BaseExecutor</code>.</li> <li>Job monitoring and benchmark data collection are placeholder implementations in some of the executors. Users should consider implementing job-specific monitoring and resource management tailored to their requirements.</li> </ul>"},{"location":"pipeline_modules/executors/#summary","title":"Summary","text":"<p>The executor classes provide a modular way to submit jobs to different cluster systems or run them locally. Each executor manages the nuances of the corresponding cluster scheduler, allowing seamless integration with cgatcore pipelines. They provide functionalities such as job submission, logging, monitoring, and benchmarking, ensuring a streamlined and customisable workflow for distributed computing environments.</p>"},{"location":"pipeline_modules/farm/","title":"farm.py - Documentation","text":"<p>This document provides an overview of the <code>farm.py</code> script, which is used to split a data stream into independent chunks for parallel processing on a cluster. This is particularly useful for large-scale computational tasks where dividing the workload into smaller, independent parts can significantly reduce processing time.</p>"},{"location":"pipeline_modules/farm/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Purpose</li> <li>Usage</li> <li>Documentation</li> <li>Input and Output Handling</li> <li>Chunking Methods</li> <li>Error Handling</li> <li>Examples</li> <li>Classes and Functions</li> <li>Chunk Iterator Functions</li> <li>Mapper Classes</li> <li>ResultBuilder Classes</li> <li>build_command Function</li> <li>hasFinished Function</li> <li>get_option_parser Function</li> <li>main Function</li> </ul>"},{"location":"pipeline_modules/farm/#purpose","title":"Purpose","text":"<p>The <code>farm.py</code> script is designed to process a data stream in parallel on a cluster by splitting it into smaller, independent chunks. This approach is suitable for \"embarrassingly parallel\" jobs, where the computations for each chunk can be executed independently without requiring communication between them.</p> <p>The script reads data from <code>stdin</code>, splits the data, executes user-specified commands on each chunk, and writes the output to <code>stdout</code>. The results are returned in the same order as they are submitted.</p>"},{"location":"pipeline_modules/farm/#usage","title":"Usage","text":"<p>The script can be run from the command line. Below are two basic examples:</p> <ul> <li> <p>Split the input data by the first column and execute a Perl command on each split:   <pre><code>cat go.txt | farm.py --split-at-column=1 perl -p -e \"s/GO/gaga/\"\n</code></pre></p> </li> <li> <p>Split a fasta file at each sequence entry and compute an approximate sequence length:   <pre><code>cat genome.fasta | farm.py --split-at-regex=\"^&gt;(\\S+)\" \"wc -c\"\n</code></pre></p> </li> </ul> <p>Run <code>python farm.py --help</code> to get detailed command-line options.</p>"},{"location":"pipeline_modules/farm/#documentation","title":"Documentation","text":""},{"location":"pipeline_modules/farm/#input-and-output-handling","title":"Input and Output Handling","text":"<p>The input to <code>farm.py</code> is provided via <code>stdin</code> and processed in parallel. The output is written to <code>stdout</code> with results combined in the same order they were processed. The script ensures that duplicate headers are avoided and can also handle jobs that output multiple files.</p>"},{"location":"pipeline_modules/farm/#chunking-methods","title":"Chunking Methods","text":"<p>The script provides multiple ways to split (or \"chunk\") the input data: - Split at lines: Divide the input data by the number of lines. - Split by column: Split based on the unique values in a specified column. - Split using regex: Use a regular expression to define how to split data. - Group using regex: Group entries together if they match a regular expression.</p>"},{"location":"pipeline_modules/farm/#error-handling","title":"Error Handling","text":"<p>If an error occurs during the execution of a job, the error messages are printed, and the temporary directory used for processing is not deleted, allowing manual recovery. The script also implements a retry mechanism for failed jobs and can log errors into separate files for analysis.</p>"},{"location":"pipeline_modules/farm/#examples","title":"Examples","text":"<ol> <li> <p>Basic Example: Split the file \"go\" at the first column and replace <code>GO</code> with <code>gaga</code> in each chunk:    <pre><code>cat go | farm.py --split-at-column=1 perl -p -e \"s/GO/gaga/\"\n</code></pre></p> </li> <li> <p>FASTA File Processing: Split a fasta file at each sequence and calculate length:    <pre><code>cat genome.fasta | farm.py --split-at-regex=\"^&gt;(\\S+)\" \"wc -c\"\n</code></pre></p> </li> <li> <p>Chunk by Sequence Count: Split a fasta file at every 10 sequences:    <pre><code>cat genome.fasta | farm.py --split-at-regex=\"^&gt;(\\S+)\" --chunk-size=10 \"wc -c\"\n</code></pre></p> </li> </ol>"},{"location":"pipeline_modules/farm/#classes-and-functions","title":"Classes and Functions","text":""},{"location":"pipeline_modules/farm/#chunk-iterator-functions","title":"Chunk Iterator Functions","text":"<p>The script includes various functions to handle chunking of the input data:</p> <ul> <li><code>chunk_iterator_lines</code>: Splits input data by a specific number of lines.</li> <li><code>chunk_iterator_column</code>: Splits input based on values in a specified column.</li> <li><code>chunk_iterator_regex_group</code>: Groups input lines based on a regex match.</li> <li><code>chunk_iterator_regex_split</code>: Splits input whenever a regex matches.</li> </ul> <p>These functions yield filenames containing the chunks, which are then processed independently.</p>"},{"location":"pipeline_modules/farm/#mapper-classes","title":"Mapper Classes","text":"<p>Mappers are used to rename or manage output IDs: - <code>MapperGlobal</code>: Maps IDs globally with a given pattern. - <code>MapperLocal</code>: Maps IDs locally, associating a unique ID to each key within a specific file. - <code>MapperEmpty</code>: Passes through the original ID without modification.</p>"},{"location":"pipeline_modules/farm/#resultbuilder-classes","title":"ResultBuilder Classes","text":"<p>The <code>ResultBuilder</code> classes handle the output from processed chunks:</p> <ul> <li><code>ResultBuilder</code>: Merges results from table-formatted output.</li> <li><code>ResultBuilderFasta</code>: Handles results from fasta-formatted output.</li> <li><code>ResultBuilderBinary</code>: Concatenates binary output files.</li> <li><code>ResultBuilderCopies</code>: Creates indexed copies of output files.</li> <li><code>ResultBuilderLog</code>: Aggregates log files from multiple jobs.</li> </ul>"},{"location":"pipeline_modules/farm/#build_command-function","title":"build_command Function","text":"<p><pre><code>def build_command(data):\n    # Function code...\n</code></pre> This function constructs the shell command to execute each chunk, including logging and managing temporary directories. It replaces placeholders (e.g., <code>%STDIN%</code> and <code>%DIR%</code>) with appropriate values.</p>"},{"location":"pipeline_modules/farm/#hasfinished-function","title":"hasFinished Function","text":"<p><pre><code>def hasFinished(retcode, filename, output_tag, logfile):\n    # Function code...\n</code></pre> The <code>hasFinished()</code> function checks if a run has finished successfully by inspecting the return code and looking for a completion tag in the log file.</p>"},{"location":"pipeline_modules/farm/#get_option_parser-function","title":"get_option_parser Function","text":"<p><pre><code>def get_option_parser():\n    # Function code...\n</code></pre> The <code>get_option_parser()</code> function sets up and returns an argument parser with various command-line options for specifying how the input should be split, how output should be handled, and other behaviours (e.g., memory requirements, logging).</p>"},{"location":"pipeline_modules/farm/#main-function","title":"main Function","text":"<p><pre><code>def main(argv=None):\n    # Function code...\n</code></pre> The <code>main()</code> function is the entry point of the script. It parses the command-line arguments, prepares the input data for processing, builds commands for each chunk, runs these commands (using a specified method: multiprocessing, threading, etc.), and finally collects and processes the results.</p> <ul> <li>Key Steps in Main:</li> <li>Argument Parsing: Uses <code>get_option_parser()</code> to parse command-line options.</li> <li>Chunking Input: Chooses the appropriate chunking method and splits the input accordingly.</li> <li>Job Execution: Executes each chunk using the specified method (e.g., <code>multiprocessing</code>, <code>threads</code>, or <code>drmaa</code> for cluster management).</li> <li>Result Collection: Collects and combines the results using <code>ResultBuilder</code> classes.</li> <li>Error Handling: Logs any failed jobs and, if all jobs succeed, cleans up the temporary directories.</li> </ul>"},{"location":"pipeline_modules/farm/#conclusion","title":"Conclusion","text":"<p>The <code>farm.py</code> script is a powerful utility for dividing data streams into smaller tasks, running them in parallel on a cluster, and collating the output. It is well-suited for \"embarrassingly parallel\" tasks, such as processing large tabular datasets or fasta files, and integrates seamlessly with cluster environments for distributed computation.</p> <p>With flexible options for chunking data, managing output, and handling errors, this script is a useful tool for bioinformatics pipelines and other data-intensive workflows.</p>"},{"location":"pipeline_modules/files/","title":"Files Module","text":"<p><code>files.py</code> - Working with files in cgatcore pipelines</p> <p>====================================================</p>"},{"location":"pipeline_modules/files/#reference","title":"Reference","text":"<p>This module provides a collection of functions to facilitate file handling within cgatcore pipelines, including generating temporary files, temporary directories, and checking for required executables or scripts.</p>"},{"location":"pipeline_modules/files/#import-statements","title":"Import Statements","text":"<pre><code>import os\nimport tempfile\n\nimport cgatcore.iotools as iotools\nimport cgatcore.experiment as E\nfrom cgatcore.pipeline.parameters import get_params\n</code></pre>"},{"location":"pipeline_modules/files/#key-functions","title":"Key Functions","text":""},{"location":"pipeline_modules/files/#get_temp_filedirnone-sharedfalse-suffix-modew-encodingutf-8","title":"<code>get_temp_file(dir=None, shared=False, suffix=\"\", mode=\"w+\", encoding=\"utf-8\")</code>","text":"<p>Gets a temporary file. The file is created and opened in text mode by default (mode <code>w+</code>), with UTF-8 encoding.</p> <p>Arguments: - <code>dir</code> (string, optional): Directory of the temporary file. If not provided, defaults to the temporary directory defined in the global configuration. - <code>shared</code> (bool, optional): If set to <code>True</code>, creates the file in a shared temporary location. - <code>suffix</code> (string, optional): Suffix for the filename. - <code>mode</code> (string, optional): File mode (e.g., <code>w+</code> for reading and writing). Defaults to <code>w+</code>. - <code>encoding</code> (string, optional): Encoding for the file. Defaults to <code>utf-8</code>.</p> <p>Returns: - <code>file</code> (File object): A temporary file object that the caller must close and delete once it's no longer needed.</p>"},{"location":"pipeline_modules/files/#get_temp_filenamedirnone-sharedfalse-cleartrue-suffix","title":"<code>get_temp_filename(dir=None, shared=False, clear=True, suffix=\"\")</code>","text":"<p>Returns a temporary filename. The file is created and then optionally deleted if <code>clear</code> is set to <code>True</code>.</p> <p>Arguments: - <code>dir</code> (string, optional): Directory for the temporary file. Defaults to the configuration's temporary directory. - <code>shared</code> (bool, optional): If set to <code>True</code>, places the file in a shared temporary location. - <code>clear</code> (bool, optional): If set to <code>True</code>, deletes the file after creation. Defaults to <code>True</code>. - <code>suffix</code> (string, optional): Suffix for the filename.</p> <p>Returns: - <code>filename</code> (string): Absolute path to the temporary file.</p>"},{"location":"pipeline_modules/files/#get_temp_dirdirnone-sharedfalse-clearfalse","title":"<code>get_temp_dir(dir=None, shared=False, clear=False)</code>","text":"<p>Creates and returns a temporary directory.</p> <p>Arguments: - <code>dir</code> (string, optional): Directory for the temporary directory. Defaults to the configuration's temporary directory. - <code>shared</code> (bool, optional): If set to <code>True</code>, places the directory in a shared temporary location. - <code>clear</code> (bool, optional): If set to <code>True</code>, removes the directory after creation.</p> <p>Returns: - <code>filename</code> (string): Absolute path to the temporary directory.</p>"},{"location":"pipeline_modules/files/#check_executablesfilenames","title":"<code>check_executables(filenames)</code>","text":"<p>Checks for the presence of required executables in the system's PATH.</p> <p>Arguments: - <code>filenames</code> (list of strings): List of executables to check for.</p> <p>Raises: - <code>ValueError</code>: If any executable is missing.</p> <pre><code>def check_executables(filenames):\n    missing = []\n    for filename in filenames:\n        if not iotools.which(filename):\n            missing.append(filename)\n    if missing:\n        raise ValueError(\"missing executables: %s\" % \",\".join(missing))\n</code></pre>"},{"location":"pipeline_modules/files/#check_scriptsfilenames","title":"<code>check_scripts(filenames)</code>","text":"<p>Checks for the presence of specified scripts in the filesystem.</p> <p>Arguments: - <code>filenames</code> (list of strings): List of script filenames to check for.</p> <p>Raises: - <code>ValueError</code>: If any script is missing.</p> <pre><code>def check_scripts(filenames):\n    missing = []\n    for filename in filenames:\n        if not os.path.exists(filename):\n            missing.append(filename)\n    if missing:\n        raise ValueError(\"missing scripts: %s\" % \",\".join(missing))\n</code></pre>"},{"location":"pipeline_modules/files/#usage","title":"Usage","text":"<p>These functions are useful for managing temporary files and ensuring all necessary executables or scripts are available when running Ruffus pipelines. They help maintain clean temporary storage and facilitate proper error checking to prevent missing dependencies.</p>"},{"location":"pipeline_modules/files/#notes","title":"Notes","text":"<ul> <li>Temporary files and directories are created in either a shared or a default temporary location, based on the provided arguments.</li> <li>The <code>get_temp_file</code> function provides a safe way to generate temporary files, with customisable directory, mode, and encoding options.</li> <li>Always handle temporary files and directories appropriately, ensuring they are deleted after use to avoid cluttering the filesystem.</li> </ul> <p>As you continue to expand the functionality of CGAT-core, make sure to keep this module up to date with new helper functions or improvements.</p>"},{"location":"pipeline_modules/overview/","title":"Pipeline Modules Overview","text":"<p>CGAT-core provides a comprehensive set of modules for building and executing computational pipelines. This document provides an overview of the core modules and their functionality.</p>"},{"location":"pipeline_modules/overview/#core-modules","title":"Core Modules","text":""},{"location":"pipeline_modules/overview/#1-pipeline-control-controlpy","title":"1. Pipeline Control (<code>control.py</code>)","text":"<ul> <li>Pipeline initialization and configuration</li> <li>Command-line interface</li> <li>Parameter management</li> <li>Logging setup</li> </ul> <pre><code>from cgatcore import pipeline as P\n\n# Initialize pipeline\nP.initialize(argv)\n\n# Get parameters\nPARAMS = P.get_parameters()\n\n# Setup logging\nL = P.get_logger()\n</code></pre>"},{"location":"pipeline_modules/overview/#2-task-execution-executionpy","title":"2. Task Execution (<code>execution.py</code>)","text":"<ul> <li>Job submission and monitoring</li> <li>Resource management</li> <li>Error handling</li> <li>Cleanup procedures</li> </ul> <pre><code># Run command\nP.run(\"samtools sort input.bam\")\n\n# Submit Python function\nP.submit(module=\"my_module\",\n         function=\"process_data\",\n         infiles=\"input.txt\",\n         outfiles=\"output.txt\")\n</code></pre>"},{"location":"pipeline_modules/overview/#3-cluster-integration-clusterpy","title":"3. Cluster Integration (<code>cluster.py</code>)","text":"<ul> <li>Cluster job management</li> <li>Resource allocation</li> <li>Queue selection</li> <li>Job monitoring</li> </ul> <pre><code># Configure cluster\nP.setup_cluster()\n\n# Submit cluster job\nstatement = \"\"\"samtools sort input.bam\"\"\"\njob_threads = 4\njob_memory = \"8G\"\nP.run(statement)\n</code></pre>"},{"location":"pipeline_modules/overview/#4-file-management-filespy","title":"4. File Management (<code>files.py</code>)","text":"<ul> <li>File path handling</li> <li>Temporary file management</li> <li>File type detection</li> <li>Pattern matching</li> </ul> <pre><code># Get temporary directory\ntmpdir = P.get_temp_dir()\n\n# Clean up temporary files\nP.cleanup_tmpdir()\n</code></pre>"},{"location":"pipeline_modules/overview/#advanced-features","title":"Advanced Features","text":""},{"location":"pipeline_modules/overview/#1-parameter-management-parameterspy","title":"1. Parameter Management (<code>parameters.py</code>)","text":"<ul> <li>Configuration file parsing</li> <li>Parameter validation</li> <li>Default value handling</li> <li>Environment integration</li> </ul> <pre><code># Load parameters\nPARAMS = P.get_parameters([\n    \"pipeline.yml\",\n    \"cluster.yml\"\n])\n\n# Access parameters\ninput_dir = PARAMS[\"input_dir\"]\nthreads = PARAMS.get(\"threads\", 1)\n</code></pre>"},{"location":"pipeline_modules/overview/#2-database-integration-databasepy","title":"2. Database Integration (<code>database.py</code>)","text":"<ul> <li>SQLite database support</li> <li>Table creation and updates</li> <li>Query execution</li> <li>Result caching</li> </ul> <pre><code># Connect to database\ndb = P.connect()\n\n# Execute query\nresults = db.execute(\"SELECT * FROM stats\")\n</code></pre>"},{"location":"pipeline_modules/overview/#3-cloud-integration-s3_integrationpy","title":"3. Cloud Integration (<code>s3_integration.py</code>)","text":"<ul> <li>AWS S3 support</li> <li>Cloud storage access</li> <li>File transfer</li> <li>Credential management</li> </ul> <pre><code># Configure S3\nP.configure_s3()\n\n# Use S3-aware decorators\n@P.s3_transform(\"s3://bucket/input.txt\",\n                suffix(\".txt\"), \".processed\")\ndef process_s3_file(infile, outfile):\n    pass\n</code></pre>"},{"location":"pipeline_modules/overview/#pipeline-development","title":"Pipeline Development","text":""},{"location":"pipeline_modules/overview/#1-task-definition","title":"1. Task Definition","text":"<pre><code>@transform(\"*.fastq.gz\", suffix(\".fastq.gz\"), \".bam\")\ndef map_reads(infile, outfile):\n    \"\"\"Map reads to reference genome.\"\"\"\n    job_threads = 4\n    job_memory = \"8G\"\n\n    statement = \"\"\"\n    bwa mem -t %(job_threads)s \n        reference.fa \n        %(infile)s &gt; %(outfile)s\n    \"\"\"\n    P.run(statement)\n</code></pre>"},{"location":"pipeline_modules/overview/#2-pipeline-flow","title":"2. Pipeline Flow","text":"<pre><code>@follows(map_reads)\n@transform(\"*.bam\", suffix(\".bam\"), \".sorted.bam\")\ndef sort_bam(infile, outfile):\n    \"\"\"Sort BAM files.\"\"\"\n    statement = \"\"\"\n    samtools sort %(infile)s &gt; %(outfile)s\n    \"\"\"\n    P.run(statement)\n\n@follows(sort_bam)\n@merge(\"*.sorted.bam\", \"final_report.txt\")\ndef create_report(infiles, outfile):\n    \"\"\"Generate final report.\"\"\"\n    statement = \"\"\"\n    multiqc . -o %(outfile)s\n    \"\"\"\n    P.run(statement)\n</code></pre>"},{"location":"pipeline_modules/overview/#best-practices","title":"Best Practices","text":""},{"location":"pipeline_modules/overview/#1-code-organization","title":"1. Code Organization","text":"<ul> <li>Group related tasks</li> <li>Use meaningful task names</li> <li>Document pipeline steps</li> <li>Implement error handling</li> </ul>"},{"location":"pipeline_modules/overview/#2-resource-management","title":"2. Resource Management","text":"<ul> <li>Specify memory requirements</li> <li>Set appropriate thread counts</li> <li>Use temporary directories</li> <li>Clean up intermediate files</li> </ul>"},{"location":"pipeline_modules/overview/#3-error-handling","title":"3. Error Handling","text":"<pre><code>try:\n    P.run(statement)\nexcept P.PipelineError as e:\n    L.error(\"Task failed: %s\" % e)\n    # Implement recovery or cleanup\n    cleanup_and_notify()\n</code></pre>"},{"location":"pipeline_modules/overview/#4-documentation","title":"4. Documentation","text":"<ul> <li>Add docstrings to tasks</li> <li>Document configuration options</li> <li>Include usage examples</li> <li>Maintain README files</li> </ul>"},{"location":"pipeline_modules/overview/#pipeline-examples","title":"Pipeline Examples","text":""},{"location":"pipeline_modules/overview/#basic-pipeline","title":"Basic Pipeline","text":"<pre><code>\"\"\"Example pipeline demonstrating core functionality.\"\"\"\n\nfrom cgatcore import pipeline as P\nimport logging as L\n\n# Initialize\nP.initialize()\n\n@transform(\"*.txt\", suffix(\".txt\"), \".processed\")\ndef process_files(infile, outfile):\n    \"\"\"Process input files.\"\"\"\n    statement = \"\"\"\n    process_data %(infile)s &gt; %(outfile)s\n    \"\"\"\n    P.run(statement)\n\n@follows(process_files)\n@merge(\"*.processed\", \"report.txt\")\ndef create_report(infiles, outfile):\n    \"\"\"Generate summary report.\"\"\"\n    statement = \"\"\"\n    cat %(infiles)s &gt; %(outfile)s\n    \"\"\"\n    P.run(statement)\n\nif __name__ == \"__main__\":\n    P.main()\n</code></pre> <p>For more detailed information about specific modules, see: - Cluster Configuration - Task Execution - Parameter Management - Database Integration</p>"},{"location":"pipeline_modules/parameters/","title":"Parameter handling for cgatcore Pipelines","text":"<p>This document provides an overview of the <code>parameters.py</code> module used in cgatcore pipelines to handle configuration and parameter management. It includes functions for loading, validating, and handling parameters, as well as managing global configurations. This module is essential for customising and controlling cgatcore pipelines' behaviour, allowing the user to flexibly specify parameters via configuration files, command-line arguments, or hard-coded defaults.</p>"},{"location":"pipeline_modules/parameters/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Overview</li> <li>Global Constants and Initial Setup</li> <li>Functions Overview</li> <li>get_logger Function</li> <li>get_parameters Function</li> <li>config_to_dictionary Function</li> <li>nested_update Function</li> <li>input_validation Function</li> <li>match_parameter Function</li> <li>substitute_parameters Function</li> <li>as_list Function</li> <li>is_true Function</li> <li>check_parameter Function</li> <li>get_params Function</li> <li>get_parameters_as_namedtuple Function</li> <li>get_param_section Function</li> </ul>"},{"location":"pipeline_modules/parameters/#overview","title":"Overview","text":"<p>The <code>parameters.py</code> module is designed to facilitate the management of configuration values for cgatcore pipelines. The configuration values are read from a variety of sources, including YAML configuration files, hard-coded dictionaries, and user-specific configuration files. The module also provides tools for parameter interpolation, validation, and nested dictionary handling.</p>"},{"location":"pipeline_modules/parameters/#global-constants-and-initial-setup","title":"Global Constants and Initial Setup","text":"<p>The module begins by defining some constants and setting up paths: - <code>SCRIPTS_ROOT_DIR</code> and <code>SCRIPTS_SCRIPTS_DIR</code>: Defines the root directory of scripts used within the pipeline. - <code>HAVE_INITIALIZED</code>: A boolean variable used to indicate if the global parameters have been loaded. - <code>PARAMS</code>: A global dictionary for parameter interpolation. This dictionary can be switched between <code>defaultdict</code> and standard dictionary behaviour to facilitate handling missing parameters.</p>"},{"location":"pipeline_modules/parameters/#functions-overview","title":"Functions Overview","text":""},{"location":"pipeline_modules/parameters/#get_logger-function","title":"get_logger Function","text":"<p><pre><code>def get_logger():\n    return logging.getLogger(\"cgatcore.pipeline\")\n</code></pre> This function returns a logger instance for use in the pipeline, allowing consistent logging across the module.</p>"},{"location":"pipeline_modules/parameters/#get_parameters-function","title":"get_parameters Function","text":"<p><pre><code>def get_parameters(filenames=None, defaults=None, site_ini=True, user=True, only_import=None):\n    # Function code...\n</code></pre> The <code>get_parameters</code> function reads one or more configuration files to build the global <code>PARAMS</code> dictionary. It can read from various configuration files (e.g., <code>pipeline.yml</code>, <code>cgat.yml</code>), and merge configurations from user, site-specific, and default sources.</p> <ul> <li>Arguments:</li> <li><code>filenames (list or str)</code>: A list of filenames for configuration files.</li> <li><code>defaults (dict)</code>: A dictionary of default values.</li> <li><code>site_ini (bool)</code>: If <code>True</code>, configuration files from <code>/etc/cgat/pipeline.yml</code> are also read.</li> <li><code>user (bool)</code>: If <code>True</code>, reads configuration from a user's home directory.</li> <li> <p><code>only_import (bool)</code>: If set, the parameter dictionary will default to a collection type.</p> </li> <li> <p>Returns:</p> </li> <li><code>dict</code>: A global configuration dictionary (<code>PARAMS</code>).</li> </ul>"},{"location":"pipeline_modules/parameters/#config_to_dictionary-function","title":"config_to_dictionary Function","text":"<p><pre><code>def config_to_dictionary(config):\n    # Function code...\n</code></pre> This function converts the contents of a <code>ConfigParser</code> object into a dictionary. Section names are prefixed with an underscore for clarity.</p> <ul> <li>Returns:</li> <li><code>dict</code>: A dictionary containing all configuration values, with nested sections appropriately handled.</li> </ul>"},{"location":"pipeline_modules/parameters/#nested_update-function","title":"nested_update Function","text":"<p><pre><code>def nested_update(old, new):\n    # Function code...\n</code></pre> The <code>nested_update</code> function updates nested dictionaries. If both <code>old[x]</code> and <code>new[x]</code> are dictionaries, they are recursively merged; otherwise, <code>old[x]</code> is updated with <code>new[x]</code>.</p>"},{"location":"pipeline_modules/parameters/#input_validation-function","title":"input_validation Function","text":"<p><pre><code>def input_validation(PARAMS, pipeline_script=\"\"):\n    # Function code...\n</code></pre> The <code>input_validation</code> function inspects the <code>PARAMS</code> dictionary to check for problematic values, such as missing or placeholder inputs.</p> <ul> <li>Validations:</li> <li>Checks for missing parameters (<code>?</code> placeholders).</li> <li>Ensures that all required tools are available on the system PATH.</li> <li>Verifies input file paths are readable.</li> </ul>"},{"location":"pipeline_modules/parameters/#match_parameter-function","title":"match_parameter Function","text":"<p><pre><code>def match_parameter(param):\n    # Function code...\n</code></pre> This function attempts to find an exact or prefix match in the global <code>PARAMS</code> dictionary for the given parameter. If no match is found, a <code>KeyError</code> is raised.</p> <ul> <li>Returns:</li> <li><code>str</code>: The full name of the parameter if found.</li> </ul>"},{"location":"pipeline_modules/parameters/#substitute_parameters-function","title":"substitute_parameters Function","text":"<p><pre><code>def substitute_parameters(**kwargs):\n    # Function code...\n</code></pre> This function returns a dictionary of parameter values for a specific task. It substitutes global parameter values and task-specific configuration values.</p> <ul> <li>Example:</li> <li>If <code>PARAMS</code> has <code>\"sample1.bam.gz_tophat_threads\": 6</code> and <code>outfile = \"sample1.bam.gz\"</code>, it returns <code>{ \"tophat_threads\": 6 }</code>.</li> </ul>"},{"location":"pipeline_modules/parameters/#as_list-function","title":"as_list Function","text":"<p><pre><code>def as_list(value):\n    # Function code...\n</code></pre> This function converts a given value to a list. If the value is a comma-separated string, it splits the string into a list.</p> <ul> <li>Returns:</li> <li><code>list</code>: The input value as a list.</li> </ul>"},{"location":"pipeline_modules/parameters/#is_true-function","title":"is_true Function","text":"<p><pre><code>def is_true(param, **kwargs):\n    # Function code...\n</code></pre> This function checks if a parameter has a truthy value. Values like <code>0</code>, <code>''</code>, <code>false</code>, and <code>False</code> are considered as <code>False</code>.</p> <ul> <li>Returns:</li> <li><code>bool</code>: Whether the parameter is truthy or not.</li> </ul>"},{"location":"pipeline_modules/parameters/#check_parameter-function","title":"check_parameter Function","text":"<p><pre><code>def check_parameter(param):\n    # Function code...\n</code></pre> The <code>check_parameter</code> function checks if the given parameter is set in the global <code>PARAMS</code> dictionary. If it is not set, a <code>ValueError</code> is raised.</p>"},{"location":"pipeline_modules/parameters/#get_params-function","title":"get_params Function","text":"<p><pre><code>def get_params():\n    # Function code...\n</code></pre> This function returns a handle to the global <code>PARAMS</code> dictionary.</p>"},{"location":"pipeline_modules/parameters/#get_parameters_as_namedtuple-function","title":"get_parameters_as_namedtuple Function","text":"<p><pre><code>def get_parameters_as_namedtuple(*args, **kwargs):\n    # Function code...\n</code></pre> The <code>get_parameters_as_namedtuple</code> function returns the <code>PARAMS</code> dictionary as a namedtuple, allowing for more convenient and attribute-based access to parameters.</p>"},{"location":"pipeline_modules/parameters/#get_param_section-function","title":"get_param_section Function","text":"<p><pre><code>def get_param_section(section):\n    # Function code...\n</code></pre> This function returns all configuration values within a specific section of the <code>PARAMS</code> dictionary. Sections are defined by common prefixes.</p> <ul> <li>Returns:</li> <li><code>list</code>: A list of tuples containing section-specific parameters.</li> </ul>"},{"location":"pipeline_modules/parameters/#summary","title":"Summary","text":"<p>The <code>parameters.py</code> module is designed to facilitate flexible and powerful parameter management for cgatcore pipelines. The functions provided allow for seamless integration of configuration from multiple sources, validation, and management of parameters, while also offering tools for introspection and nested dictionary handling. These utilities help create more robust and maintainable cgatcore pipelines, allowing for greater customisation and scalability.</p>"},{"location":"pipeline_modules/run_function/","title":"run_function.py - Documentation","text":"<p>This document provides an overview of the <code>run_function.py</code> script, which is used to execute a function from a specified Python module remotely on a cluster. This utility allows functions from Python modules to be executed with user-defined parameters, input files, and output files, which is useful for running scripts as part of a computational pipeline.</p>"},{"location":"pipeline_modules/run_function/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Purpose</li> <li>Usage</li> <li>Command Line Options</li> <li>Workflow</li> <li>Parsing Options</li> <li>Module Importing</li> <li>Function Invocation</li> <li>Examples</li> <li>Error Handling</li> </ul>"},{"location":"pipeline_modules/run_function/#purpose","title":"Purpose","text":"<p>The <code>run_function.py</code> script allows the execution of a specified function from a Python module with given input and output files, and other parameters. It can be used within a cluster environment to facilitate the remote execution of Python functions for parallel processing tasks or batch jobs.</p>"},{"location":"pipeline_modules/run_function/#usage","title":"Usage","text":"<p>The script is typically used in conjunction with other pipeline tools. Here is an example:</p> <pre><code>statement = \"\"\"python %(scriptsdir)s/run_function.py \\\n              -p infile,outfile,additional_param1 \\\n              -m modulefile \\\n              -f function\"\"\"\n\nP.run()\n</code></pre> <p>If the module to be used is within the <code>$PYTHONPATH</code>, it can be directly named (e.g., \"pipeline\" would refer to <code>pipeline.py</code>). The script is mainly tested for cases involving single input/output pairs.</p>"},{"location":"pipeline_modules/run_function/#command-line-options","title":"Command Line Options","text":"<ul> <li><code>-p, --params, --args</code>: Comma-separated list of additional parameter strings to be passed to the function.</li> <li><code>-m, --module</code>: The full path to the module file from which the function will be imported.</li> <li><code>-i, --input</code>: Input filename(s). Can be specified multiple times for multiple inputs.</li> <li><code>-o, --output-section</code>: Output filename(s). Can be specified multiple times for multiple outputs.</li> <li><code>-f, --function</code>: The name of the function to be executed from the specified module.</li> </ul>"},{"location":"pipeline_modules/run_function/#workflow","title":"Workflow","text":"<p>The workflow of the <code>run_function.py</code> script includes:</p>"},{"location":"pipeline_modules/run_function/#parsing-options","title":"Parsing Options","text":"<p>The script begins by parsing command-line arguments using <code>OptionParser</code>. The user must specify the module file and the function to run, along with any input and output files or additional parameters.</p> <ul> <li>Mandatory Parameters:</li> <li>Module (<code>-m</code>) and Function (<code>-f</code>): Both must be specified.</li> <li>Optional Parameters:</li> <li>Input and output files (<code>-i</code>, <code>-o</code>) are optional depending on the function requirements.</li> <li>Additional parameters (<code>-p</code>) are optional but can be specified to provide custom arguments.</li> </ul>"},{"location":"pipeline_modules/run_function/#module-importing","title":"Module Importing","text":"<p>After parsing the arguments, the script imports the specified module using <code>importlib</code>. This is necessary for dynamically loading the module that contains the function to be executed.</p> <ul> <li>Adding Path: If a full path is provided, the script appends that path to <code>sys.path</code> to ensure that Python can locate the module.</li> <li>Module Import: The <code>importlib.import_module()</code> function is used to import the module by its basename. The script also handles cases where the <code>.py</code> file extension is included.</li> <li>Function Mapping: The specified function is retrieved from the module using <code>getattr()</code>. If the function cannot be found, an error is raised, indicating the available functions within the module.</li> </ul>"},{"location":"pipeline_modules/run_function/#function-invocation","title":"Function Invocation","text":"<p>The function is invoked with the appropriate arguments, depending on which input, output, and parameter combinations are specified.</p> <ul> <li>Handling Inputs and Outputs:</li> <li>The script manages cases where there are multiple or single input/output files, converting them into the expected formats for the function.</li> <li>The <code>infiles</code> and <code>outfiles</code> arguments are handled to ensure they are passed appropriately, either as lists or as single file paths.</li> <li>Parameter Parsing: If additional parameters are provided, they are split into a list and passed as arguments to the function.</li> <li>Function Call: Based on the presence of inputs, outputs, and parameters, the function is called with different argument combinations.</li> </ul>"},{"location":"pipeline_modules/run_function/#examples","title":"Examples","text":"<ol> <li> <p>Basic Function Execution <pre><code>python run_function.py -m mymodule.py -f my_function -i input.txt -o output.txt -p param1,param2\n</code></pre>    In this example, <code>my_function</code> from <code>mymodule.py</code> is executed with <code>input.txt</code> as the input file, <code>output.txt</code> as the output file, and <code>param1</code> and <code>param2</code> as additional parameters.</p> </li> <li> <p>Executing a Function without Input/Output <pre><code>python run_function.py -m utilities.py -f simple_function -p param1,param2\n</code></pre>    This runs <code>simple_function</code> from <code>utilities.py</code> with the specified parameters, but without any input or output files.</p> </li> </ol>"},{"location":"pipeline_modules/run_function/#error-handling","title":"Error Handling","text":"<ul> <li>Missing Module or Function: If either the module (<code>-m</code>) or function (<code>-f</code>) options are missing, the script raises a <code>ValueError</code>, indicating that both must be provided.</li> <li>Import Errors: The script checks if the module exists at the specified location, and if the function is present within the module. It provides debug information (<code>sys.path</code>) to help locate import issues.</li> <li>Attribute Errors: If the specified function is not found in the module, an <code>AttributeError</code> is raised, and the script lists all available functions within the module.</li> <li>Invalid Argument Combinations: If the expected combination of input, output, and parameters is not provided, the script raises a <code>ValueError</code>, clarifying what is expected.</li> </ul>"},{"location":"pipeline_modules/run_function/#conclusion","title":"Conclusion","text":"<p>The <code>run_function.py</code> script is a versatile tool for remotely executing functions from Python modules on a cluster. It supports input/output file handling and passing of additional parameters, making it suitable for use in complex computational pipelines. With its flexible argument parsing and dynamic module importing, it provides an easy way to run Python functions in distributed environments, aiding in the modularisation and parallelisation of tasks.</p>"},{"location":"pipeline_modules/utils/","title":"Utilities for cgatcore Pipelines - Documentation","text":"<p>This document provides an overview of the utility functions and classes defined in <code>utils.py</code> for use in cgatcore pipelines. The utilities include functions for context inspection, testing mode checks, and decorators for managing documentation strings. These functions are particularly helpful for debugging and providing context information during the execution of a cgatcore pipeline.</p>"},{"location":"pipeline_modules/utils/#table-of-contents","title":"Table of Contents","text":"<ul> <li>EmptyRunner Class</li> <li>is_test Function</li> <li>get_caller_locals Function</li> <li>get_caller Function</li> <li>get_calling_function Function</li> <li>add_doc Decorator</li> </ul>"},{"location":"pipeline_modules/utils/#emptyrunner-class","title":"EmptyRunner Class","text":"<pre><code>class EmptyRunner:\n    def __init__(self, name):\n        self.__name__ = name\n\n    def __call__(self, *args, **kwargs):\n        pass\n</code></pre> <p>The <code>EmptyRunner</code> class is a simple utility that creates an object with a callable interface that does nothing when called. It is primarily useful as a placeholder in situations where a no-operation handler is required.</p> <ul> <li>Attributes:</li> <li> <p><code>name</code>: A name assigned to the instance for identification purposes.</p> </li> <li> <p>Methods:</p> </li> <li><code>__call__(self, *args, **kwargs)</code>: This method is a no-operation handler.</li> </ul>"},{"location":"pipeline_modules/utils/#is_test-function","title":"is_test Function","text":"<pre><code>def is_test():\n    return \"--is-test\" in sys.argv\n</code></pre> <p>The <code>is_test()</code> function checks whether the pipeline is being run in testing mode.</p> <ul> <li>Returns:</li> <li><code>bool</code>: Returns <code>True</code> if <code>--is-test</code> is passed as a command-line argument; otherwise, <code>False</code>.</li> </ul> <p>This function is useful for conditionally enabling or disabling testing-specific behaviour.</p>"},{"location":"pipeline_modules/utils/#get_caller_locals-function","title":"get_caller_locals Function","text":"<pre><code>def get_caller_locals(decorators=0):\n    f = sys._getframe(2 + decorators)\n    args = inspect.getargvalues(f)\n    return args.locals\n</code></pre> <p>The <code>get_caller_locals()</code> function returns the local variables of the calling function. This is useful for debugging or inspecting the state of the caller.</p> <ul> <li>Parameters:</li> <li> <p><code>decorators (int)</code>: The number of decorator contexts to go up to find the caller of interest. Default is <code>0</code>.</p> </li> <li> <p>Returns:</p> </li> <li><code>dict</code>: A dictionary of local variables defined in the caller's context.</li> </ul>"},{"location":"pipeline_modules/utils/#get_caller-function","title":"get_caller Function","text":"<pre><code>def get_caller(decorators=0):\n    frm = inspect.stack()\n    return inspect.getmodule(frm[2 + decorators].frame)\n</code></pre> <p>The <code>get_caller()</code> function returns the calling class/module. This helps identify the caller and can be helpful for logging and debugging.</p> <ul> <li>Parameters:</li> <li> <p><code>decorators (int)</code>: The number of decorator contexts to go up to find the caller of interest. Default is <code>0</code>.</p> </li> <li> <p>Returns:</p> </li> <li><code>module</code>: The calling module or <code>None</code> if not found.</li> </ul>"},{"location":"pipeline_modules/utils/#get_calling_function-function","title":"get_calling_function Function","text":"<pre><code>def get_calling_function(decorators=0):\n    frm = inspect.stack()\n    return frm[2 + decorators].function\n</code></pre> <p>The <code>get_calling_function()</code> function returns the name of the calling function. This is useful for introspection and debugging to know which function invoked the current function.</p> <ul> <li>Parameters:</li> <li> <p><code>decorators (int)</code>: The number of decorator contexts to go up to find the caller of interest. Default is <code>0</code>.</p> </li> <li> <p>Returns:</p> </li> <li><code>str</code>: The name of the calling function, or <code>None</code> if not found.</li> </ul>"},{"location":"pipeline_modules/utils/#add_doc-decorator","title":"add_doc Decorator","text":"<pre><code>def add_doc(value, replace=False):\n    def _doc(func):\n        @wraps(func)\n        def wrapped_func(*args, **kwargs):\n            return func(*args, **kwargs)\n\n        if func.__doc__:\n            lines = value.__doc__.split(\"\\n\")\n            for x, line in enumerate(lines):\n                if line.strip() == \"\":\n                    break\n            if not replace:\n                lines.insert(x + 1, \" \" * 4 + func.__doc__)\n                wrapped_func.__doc__ = \"\\n\".join(lines)\n            else:\n                wrapped_func.__doc__ = value.__doc__\n        else:\n            wrapped_func.__doc__ = value.__doc__\n\n        return wrapped_func\n\n    return _doc\n</code></pre> <p>The <code>add_doc()</code> function is a decorator that adds or replaces the docstring of the decorated function.</p> <ul> <li>Parameters:</li> <li><code>value</code>: The function or string whose documentation will be added.</li> <li> <p><code>replace (bool)</code>: If <code>True</code>, the existing documentation is replaced. Otherwise, the new documentation is appended after the existing documentation. Default is <code>False</code>.</p> </li> <li> <p>Returns:</p> </li> <li>A decorated function with the modified or updated docstring.</li> </ul> <p>This utility is helpful for ensuring that custom decorators or utility functions carry informative and up-to-date documentation, which can help when generating automated docs or maintaining code.</p>"},{"location":"pipeline_modules/utils/#general-notes","title":"General Notes","text":"<ul> <li>The functions use Python's <code>inspect</code> and <code>sys</code> libraries for introspection and manipulation of stack frames, which can be useful for debugging complex pipelines.</li> <li>The <code>logging</code> module is used for error handling to ensure that potential issues (e.g., accessing out-of-range stack frames) are logged rather than silently ignored.</li> </ul>"},{"location":"pipeline_modules/utils/#example-usage","title":"Example Usage","text":"<pre><code>@add_doc(is_test)\ndef example_function():\n    \"\"\"This is an example function.\"\"\"\n    print(\"Example function running.\")\n\nif __name__ == \"__main__\":\n    if is_test():\n        print(\"Running in testing mode.\")\n    else:\n        example_function()\n</code></pre> <p>In this example, <code>example_function()</code> is decorated with the <code>add_doc()</code> decorator, using the documentation from <code>is_test()</code>. This effectively appends the <code>is_test()</code> docstring to <code>example_function()</code>.</p>"},{"location":"pipeline_modules/utils/#conclusion","title":"Conclusion","text":"<p>These utilities provide helpful functionality for cgatcore pipelines by allowing developers to inspect caller contexts, easily handle testing conditions, and dynamically update function documentation. The use of the <code>inspect</code> library allows access to stack frames, making these utilities especially useful for debugging and dynamic analysis during pipeline execution.</p>"},{"location":"project_info/citations/","title":"Citing and Citations","text":"<p><code>cgatcore</code> has been developed over the past 10 years and has been used in a number of previously published scientific articles. This page provides information on how to cite <code>cgatcore</code> and highlights some of the key publications where it has been used.</p>"},{"location":"project_info/citations/#citing-cgatcore","title":"Citing cgatcore","text":"<p>When using <code>cgatcore</code> for a publication, please cite the following article in your paper:</p> <p>ADD CITATION HERE</p>"},{"location":"project_info/citations/#more-references","title":"More references","text":"<p>The following is a list of publications that have used tools developed by CGAT developers. Note that this list is not exhaustive:</p> <ul> <li> <p>A ChIP-seq defined genome-wide map of vitamin D receptor binding: associations with disease and evolution   SV Ramagopalan, A Heger, AJ Berlanga, NJ Maugeri, MR Lincoln, ... Genome research 20 (10), 1352-1360 (2010)</p> </li> <li> <p>Sequencing depth and coverage: key considerations in genomic analyses   D Sims, I Sudbery, NE Ilott, A Heger, CP Ponting Nature Reviews Genetics 15 (2), 121 (2014)</p> </li> <li> <p>KDM2B links the Polycomb Repressive Complex 1 (PRC1) to recognition of CpG islands   AM Farcas, NP Blackledge, I Sudbery, HK Long, JF McGouran, NR Rose, ... eLife (2012)</p> </li> <li> <p>Targeting polycomb to pericentric heterochromatin in embryonic stem cells reveals a role for H2AK119u1 in PRC2 recruitment   S Cooper, M Dienstbier, R Hassan, L Schermelleh, J Sharif, ... Cell reports 7 (5), 1456-1470 (2014)</p> </li> <li> <p>Long non-coding RNAs and enhancer RNAs regulate the lipopolysaccharide-induced inflammatory response in human monocytes   NE Ilott, JA Heward, B Roux, E Tsitsiou, PS Fenwick, L Lenzi, I Goodhead, ... Nature communications 5, 3979 (2014)</p> </li> <li> <p>Population and single-cell genomics reveal the Aire dependency, relief from Polycomb silencing, and distribution of self-antigen expression in thymic epithelia   SN Sansom, N Shikama-Dorn, S Zhanybekova, G Nusspaumer, ... Genome research 24 (12), 1918-1931 (2014)</p> </li> <li> <p>Epigenetic conservation at gene regulatory elements revealed by non-methylated DNA profiling in seven vertebrates   HK Long, D Sims, A Heger, NP Blackledge, C Kutter, ML Wright, ... eLife 2 (2013)</p> </li> <li> <p>The long non\u2010coding RNA Paupar regulates the expression of both local and distal genes   KW Vance, SN Sansom, S Lee, V Chalei, L Kong, SE Cooper, PL Oliver, ... The EMBO journal 33 (4), 296-311 (2014)</p> </li> <li> <p>A genome-wide association study implicates the APOE locus in nonpathological cognitive ageing   G Davies, SE Harris, CA Reynolds, A Payton, HM Knight, DC Liewald, ... Molecular Psychiatry 19 (1), 76 (2014)</p> </li> <li> <p>Predicting long non-coding RNAs using RNA sequencing   NE Ilott, CP Ponting Methods 63 (1), 50-59 (2013)</p> </li> <li> <p>Next-generation sequencing of advanced prostate cancer treated with androgen-deprivation therapy   P Rajan, IM Sudbery, MEM Villasevil, E Mui, J Fleming, M Davis, I Ahmad, ... European urology 66 (1), 32-39 (2014)</p> </li> <li> <p>The long non-coding RNA Dali is an epigenetic regulator of neural differentiation   V Chalei, SN Sansom, L Kong, S Lee, JF Montiel, KW Vance, CP Ponting eLife 3 (2014)</p> </li> <li> <p>GAT: a simulation framework for testing the association of genomic intervals   A Heger, C Webber, M Goodson, CP Ponting, G Lunter Bioinformatics 29 (16), 2046-2048 (2013)</p> </li> <li> <p>De novo point mutations in patients diagnosed with ataxic cerebral palsy   R Parolin Schnekenberg, EM Perkins, JW Miller, WIL Davies, ... Brain 138 (7), 1817-1832 (2015)</p> </li> <li> <p>SPG7 mutations are a common cause of undiagnosed ataxia   G Pfeffer, A Pyle, H Griffin, J Miller, V Wilson, L Turnbull, K Fawcett, ... Neurology 84 (11), 1174-1176 (2015)</p> </li> <li> <p>CDK9 inhibitors define elongation checkpoints at both ends of RNA polymerase II\u2013transcribed genes   C Laitem, J Zaborowska, NF Isa, J Kufs, M Dienstbier, S Murphy Nature Structural and Molecular Biology 22 (5), 396 (2015)</p> </li> <li> <p>IRF5: RelA interaction targets inflammatory genes in macrophages   DG Saliba, A Heger, HL Eames, S Oikonomopoulos, A Teixeira, K Blazek, ... Cell reports 8 (5), 1308-1317 (2014)</p> </li> <li> <p>UMI-tools: modeling sequencing errors in Unique Molecular Identifiers to improve quantification accuracy   T Smith, A Heger, I Sudbery Genome research 27 (3), 491-499 (2017)</p> </li> <li> <p>Long noncoding RNAs in B-cell development and activation   TF Braz\u00e3o, JS Johnson, J M\u00fcller, A Heger, CP Ponting, VLJ Tybulewicz Blood 128 (7), e10-e19 (2016)</p> </li> <li> <p>CGAT: computational genomics analysis toolkit   D Sims, NE Ilott, SN Sansom, IM Sudbery, JS Johnson, KA Fawcett, ... Bioinformatics 30 (9), 1290-1291 (2014)</p> </li> </ul>"},{"location":"project_info/contributing/","title":"Developers","text":"<p>The following individuals are the main developers of <code>cgatcore</code>:</p> <ul> <li>Andreas Heger</li> <li>Adam Cribbs</li> <li>Sebastian Luna Valero</li> <li>Hania Pavlou</li> <li>David Sims</li> <li>Charlotte George</li> <li>Tom Smith</li> <li>Ian Sudbery</li> <li>Jakub Scaber</li> <li>Mike Morgan</li> <li>Katy Brown</li> <li>Nick Ilott</li> <li>Jethro Johnson</li> <li>Katherine Fawcett</li> <li>Steven Sansom</li> <li>Antonio Berlanga</li> </ul>"},{"location":"project_info/faq/","title":"Frequently Asked Questions (FAQ)","text":""},{"location":"project_info/faq/#general-questions","title":"General Questions","text":""},{"location":"project_info/faq/#what-is-cgat-core","title":"What is CGAT-core?","text":"<p>CGAT-core is a Python framework for building and executing computational pipelines, particularly suited for bioinformatics and data analysis workflows. It provides robust support for cluster environments and cloud integration.</p>"},{"location":"project_info/faq/#why-use-cgat-core-instead-of-other-workflow-managers","title":"Why use CGAT-core instead of other workflow managers?","text":"<ul> <li>Built on the proven Ruffus framework</li> <li>Native support for multiple cluster platforms</li> <li>Integrated cloud storage support</li> <li>Extensive resource management capabilities</li> <li>Container support for reproducibility</li> </ul>"},{"location":"project_info/faq/#what-platforms-are-supported","title":"What platforms are supported?","text":"<ul> <li>Operating Systems: Linux, macOS</li> <li>Cluster Systems: SLURM, SGE, PBS/Torque</li> <li>Cloud Platforms: AWS S3, Google Cloud, Azure</li> </ul>"},{"location":"project_info/faq/#installation","title":"Installation","text":""},{"location":"project_info/faq/#what-are-the-system-requirements","title":"What are the system requirements?","text":"<ul> <li>Python 3.7 or later</li> <li>Compatible cluster system (optional)</li> <li>Sufficient disk space for pipeline execution</li> <li>Memory requirements depend on specific pipeline needs</li> </ul>"},{"location":"project_info/faq/#how-do-i-install-cgat-core","title":"How do I install CGAT-core?","text":"<pre><code># Using pip\npip install cgatcore\n\n# Using conda\nconda install -c bioconda cgatcore\n</code></pre>"},{"location":"project_info/faq/#how-do-i-verify-my-installation","title":"How do I verify my installation?","text":"<pre><code># Check installation\ncgat --help\n\n# Run test pipeline\ncgat showcase make all\n</code></pre>"},{"location":"project_info/faq/#pipeline-development","title":"Pipeline Development","text":""},{"location":"project_info/faq/#how-do-i-create-a-new-pipeline","title":"How do I create a new pipeline?","text":"<ol> <li>Create a new Python file</li> <li>Import required modules</li> <li>Define pipeline tasks using decorators</li> <li>Add command-line interface</li> </ol> <p>Example: <pre><code>from cgatcore import pipeline as P\n\n@P.transform(\"*.txt\", suffix(\".txt\"), \".processed\")\ndef process_files(infile, outfile):\n    # Processing logic here\n    pass\n\nif __name__ == \"__main__\":\n    P.main()\n</code></pre></p>"},{"location":"project_info/faq/#how-do-i-configure-cluster-settings","title":"How do I configure cluster settings?","text":"<p>Create a <code>.cgat.yml</code> file in your home directory: <pre><code>cluster:\n    queue_manager: slurm\n    queue: main\n    memory_resource: mem\n    memory_default: 4G\n</code></pre></p>"},{"location":"project_info/faq/#troubleshooting","title":"Troubleshooting","text":""},{"location":"project_info/faq/#common-issues","title":"Common Issues","text":""},{"location":"project_info/faq/#pipeline-fails-with-memory-error","title":"Pipeline fails with memory error","text":"<ul> <li>Check available memory with <code>free -h</code></li> <li>Adjust memory settings in <code>.cgat.yml</code></li> <li>Use <code>job_memory</code> parameter in task decorators</li> </ul>"},{"location":"project_info/faq/#cluster-jobs-not-starting","title":"Cluster jobs not starting","text":"<ul> <li>Verify cluster configuration</li> <li>Check queue availability</li> <li>Ensure proper permissions</li> </ul>"},{"location":"project_info/faq/#temporary-files-filling-disk-space","title":"Temporary files filling disk space","text":"<ul> <li>Set appropriate tmpdir in configuration</li> <li>Enable automatic cleanup</li> <li>Monitor disk usage during execution</li> </ul>"},{"location":"project_info/faq/#getting-help","title":"Getting Help","text":"<ol> <li>Check documentation at readthedocs</li> <li>Search GitHub Issues</li> <li>Join our community discussions</li> <li>Create a new issue with:</li> <li>System information</li> <li>Error messages</li> <li>Minimal reproducible example</li> </ol>"},{"location":"project_info/faq/#best-practices","title":"Best Practices","text":""},{"location":"project_info/faq/#resource-management","title":"Resource Management","text":"<ul> <li>Always specify memory requirements</li> <li>Use appropriate number of threads</li> <li>Clean up temporary files</li> </ul>"},{"location":"project_info/faq/#error-handling","title":"Error Handling","text":"<ul> <li>Implement proper error checking</li> <li>Use logging effectively</li> <li>Handle cleanup in failure cases</li> </ul>"},{"location":"project_info/faq/#performance","title":"Performance","text":"<ul> <li>Optimize chunk sizes</li> <li>Monitor resource usage</li> <li>Use appropriate cluster settings</li> </ul> <p>For more detailed information, refer to our documentation.</p>"},{"location":"project_info/how_to_contribute/","title":"Contributing","text":"<p>Contributions are very much encouraged, and we greatly appreciate the time and effort people make to help maintain and support our tools. Every contribution helps, so please don't be shy\u2014we don't bite.</p> <p>You can contribute to the development of our software in a number of different ways:</p>"},{"location":"project_info/how_to_contribute/#reporting-bug-fixes","title":"Reporting bug fixes","text":"<p>Bugs are annoying, and reporting them helps us to fix issues quickly.</p> <p>Bugs can be reported using the issue section on GitHub.</p> <p>When reporting issues, please include:</p> <ul> <li>Steps in your code/command that led to the bug so it can be reproduced.</li> <li>The error message from the log output.</li> <li>Any other helpful information, such as the system/cluster engine or version details.</li> </ul>"},{"location":"project_info/how_to_contribute/#proposing-a-new-featureenhancement","title":"Proposing a new feature/enhancement","text":"<p>If you wish to contribute a new feature to the CGAT-core repository, the best way is to raise this as an issue and label it as an enhancement on GitHub.</p> <p>When proposing a new feature, please:</p> <ul> <li>Explain how your enhancement will work.</li> <li>Describe, as best as you can, how you plan to implement it.</li> <li>If you don't feel you have the necessary skills to implement this on your own, please mention it\u2014we'll try our best to help (or even implement it for you). However, please note that this is community-developed software, and our volunteers have other jobs, so we may not be able to work as quickly as you might hope.</li> </ul>"},{"location":"project_info/how_to_contribute/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Why not contribute to our project? It's a great way of making the project better, and your help is always welcome. We follow the fork/pull request model. To update our documentation, fix bugs, or add enhancements, you'll need to create a pull request through GitHub.</p> <p>To create a pull request, follow these steps:</p> <ol> <li>Create a GitHub account.</li> <li>Create a personal fork of the project on GitHub.</li> <li>Clone the fork onto your local machine. Your remote repo on GitHub is called <code>origin</code>.</li> <li>Add the original repository as a remote called <code>upstream</code>.</li> <li>If you made the fork a while ago, make sure you run <code>git pull upstream</code> to keep your repository up to date.</li> <li>Create a new branch to work on! We usually name our branches with capital initials followed by a dash and something unique. For example: <code>git checkout -b AC-new_doc</code>.</li> <li>Implement your fix/enhancement and make sure your code is effectively documented.</li> <li>Our code has tests, which are run when a pull request is submitted. You can also run the tests beforehand\u2014many of them are in the <code>tests/</code> directory. To run all tests, use <code>pytest --pep8 tests</code>.</li> <li>Add or modify the documentation in the <code>docs/</code> directory.</li> <li>Squash all of your commits into a single commit using Git's interactive rebase.</li> <li>Push your branch to your fork on GitHub: <code>git push origin</code>.</li> <li>From your fork on GitHub, open a pull request in the correct branch.</li> <li>Someone will review your changes, and they may suggest modifications or approve them.</li> <li>Once the pull request is approved and merged, pull the changes from <code>upstream</code> to your local repo and delete your branch.</li> </ol> <p>Note: Always write your commit messages in the present tense. Your commit messages should describe what the commit does to the code, not what you did to the code.</p>"},{"location":"project_info/license/","title":"License","text":"<p>MIT License</p> <p>Copyright (c) 2024 cgat-developers</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"remote/azure/","title":"Azure Blob Storage","text":"<p>This section describes how to interact with Microsoft's Azure Blob Storage, which is used for storing data in containers (similar to buckets in other cloud services). We use the <code>azure-storage-blob</code> package for interacting with Azure storage in Python.</p> <p>This documentation is a work in progress. If you find any bugs or want to request extra features, please report them as issues on GitHub.</p>"},{"location":"remote/azure/#setting-up-credentials","title":"Setting up credentials","text":"<p>To use Azure Blob Storage, you need an account name and an account key from Azure. These credentials can be found in the Azure Portal under \"Access keys\" for the storage account. You will need to use these credentials to interact with the Azure containers through the <code>AzureRemoteObject</code> class.</p>"},{"location":"remote/azure/#using-azure-blob-storage-with-azureremoteobject","title":"Using Azure Blob Storage with <code>AzureRemoteObject</code>","text":"<p>The <code>AzureRemoteObject</code> class allows you to interact with Azure Blob Storage in your Python scripts or workflows. The operations supported by this class include checking for the existence of containers, downloading blobs, uploading blobs, and deleting blobs.</p> <p>First, initiate the class as follows:</p> <pre><code>from cgatcore.remote.azure import AzureRemoteObject\n\naccount_name = \"your_account_name\"\naccount_key = \"your_account_key\"\n\nazure_obj = AzureRemoteObject(account_name=account_name, account_key=account_key)\n</code></pre>"},{"location":"remote/azure/#check-if-a-container-exists","title":"Check if a Container Exists","text":"<p>To check whether a container exists in Azure Blob Storage:</p> <pre><code>azure_obj.exists('my-container')\n</code></pre> <p>If the container does not exist, a <code>KeyError</code> will be raised.</p>"},{"location":"remote/azure/#download-from-azure-blob-storage","title":"Download from Azure Blob Storage","text":"<p>To download a file (blob) from a container, use the <code>download</code> method:</p> <pre><code>azure_obj.download('my-container', 'my-blob.txt', './local-dir/my-blob.txt')\n</code></pre> <p>This command will download the file named <code>my-blob.txt</code> from the container <code>my-container</code> and save it locally to <code>./local-dir/my-blob.txt</code>.</p>"},{"location":"remote/azure/#upload-to-azure-blob-storage","title":"Upload to Azure Blob Storage","text":"<p>To upload a file to an Azure container, use the <code>upload</code> method:</p> <pre><code>azure_obj.upload('my-container', 'my-blob.txt', './local-dir/my-blob.txt')\n</code></pre> <p>This will upload the local file <code>./local-dir/my-blob.txt</code> to the container <code>my-container</code>, where it will be saved as <code>my-blob.txt</code>.</p>"},{"location":"remote/azure/#delete-a-file-from-azure-blob-storage","title":"Delete a File from Azure Blob Storage","text":"<p>To delete a blob from an Azure container, use the <code>delete_file</code> method:</p> <pre><code>azure_obj.delete_file('my-container', 'my-blob.txt')\n</code></pre> <p>This command will delete the blob named <code>my-blob.txt</code> from the container <code>my-container</code>.</p>"},{"location":"remote/azure/#functional-example","title":"Functional Example","text":"<p>Below is an example demonstrating the usage of the <code>AzureRemoteObject</code> class within a data processing pipeline:</p> <pre><code>from ruffus import *\nimport sys\nimport os\nimport cgatcore.experiment as E\nfrom cgatcore import pipeline as P\nfrom cgatcore.remote.azure import AzureRemoteObject\n\n# Set up credentials\naccount_name = \"your_account_name\"\naccount_key = \"your_account_key\"\n\nazure_obj = AzureRemoteObject(account_name=account_name, account_key=account_key)\n\n# Load options from the config file\nPARAMS = P.get_parameters([\n    \"%s/pipeline.yml\" % os.path.splitext(__file__)[0],\n    \"../pipeline.yml\",\n    \"pipeline.yml\"\n])\n\n@transform(azure_obj.download('my-container', 'input.txt', './input.txt'),\n           regex(r\"(.*)\\.(.*)\"),\n           r\"\\1.counts\")\ndef countWords(infile, outfile):\n    '''Count the number of words in the input file.'''\n\n    # Upload file to Azure Blob Storage\n    azure_obj.upload('my-container', 'output.txt', './input.txt')\n\n    # The command line statement we want to execute\n    statement = '''awk 'BEGIN { printf(\"word\\tfreq\\n\"); } \n    {for (i = 1; i &lt;= NF; i++) freq[$i]++}\n    END { for (word in freq) printf \"%%s\\t%%d\\n\", word, freq[word] }'\n    &lt; %(infile)s &gt; %(outfile)s'''\n\n    P.run(statement)\n\n    # Delete file from Azure Blob Storage\n    azure_obj.delete_file('my-container', 'output.txt')\n\n@follows(countWords)\ndef full():\n    pass\n</code></pre> <p>In this example:</p> <ol> <li>Download: The <code>countWords</code> function downloads <code>input.txt</code> from the container <code>my-container</code> to a local path <code>./input.txt</code>.</li> <li>Word Count: The function then counts the number of words in the file using the <code>awk</code> command.</li> <li>Upload: The output is uploaded to Azure Blob Storage.</li> <li>Delete: Finally, the uploaded file is deleted from the container.</li> </ol> <p>This example demonstrates how Azure Blob Storage can be integrated seamlessly into a data pipeline using the <code>AzureRemoteObject</code> class.</p>"},{"location":"remote/gc/","title":"Google Cloud Storage","text":"<p>This section describes how to interact with Google Cloud Storage, specifically dealing with buckets and blobs (files). To interact with the cloud resource, we use the <code>google.cloud</code> API for Python.</p> <p>This documentation is a work in progress. We welcome any feedback for extra features or, if you find any bugs, please report them as issues on GitHub.</p>"},{"location":"remote/gc/#setting-up-credentials","title":"Setting up credentials","text":"<p>To use Google Cloud Storage features, you need to configure your credentials. This is quite easy with the <code>gcloud</code> tool. You need to run the following command before executing a workflow:</p> <pre><code>gcloud auth application-default login\n</code></pre> <p>This command sets up a JSON file with all of the credentials in your home folder, typically located at <code>.config/gcloud/application_default_credentials.json</code>.</p> <p>Next, you need to specify which Google Cloud project you are using. Projects are created in the Google Cloud Console and each has a unique ID. This ID needs to be passed into CGAT-core. You can achieve this in two ways:</p> <ol> <li> <p>Passing the project ID into the JSON file:</p> <pre><code>{\n  \"client_id\": \"764086051850-6qr4p6gpi6hn506pt8ejuq83di341hur.apps.googleusercontent.com\",\n  \"client_secret\": \"d-FL95Q19q7MQmFpd7hHD0Ty\",\n  \"refresh_token\": \"1/d8JxxulX84r3jiJVlt-xMrpDLcIp3RHuxLHtieDu8uA\",\n  \"type\": \"authorized_user\",\n  \"project_id\": \"extended-cache-163811\"\n}\n</code></pre> </li> <li> <p>Setting the project ID in <code>.bashrc</code>:</p> <pre><code>export GCLOUD_PROJECT=extended-cache-163811\n</code></pre> </li> </ol>"},{"location":"remote/gc/#download-from-google-cloud-storage","title":"Download from Google Cloud Storage","text":"<p>Using remote files with Google Cloud can be achieved easily by using the <code>download</code>, <code>upload</code>, and <code>delete_file</code> functions that are part of the <code>GCRemoteObject</code> class.</p> <p>First, initiate the class as follows:</p> <pre><code>from cgatcore.remote.google_cloud import GCRemoteObject\n\nGC = GCRemoteObject()\n</code></pre> <p>To download a file and use it within the decorator, follow this example:</p> <pre><code>@transform(GC.download('gc-test', 'pipeline.yml', './pipeline.yml'),\n           regex(r\"(.*)\\.(.*)\"),\n           r\"\\1.counts\")\n</code></pre> <p>This will download the file <code>pipeline.yml</code> from the Google Cloud bucket <code>gc-test</code> to <code>./pipeline.yml</code>, and it will be picked up by the decorator function as normal.</p>"},{"location":"remote/gc/#upload-to-google-cloud-storage","title":"Upload to Google Cloud Storage","text":"<p>To upload files to Google Cloud, run:</p> <pre><code>GC.upload('gc-test', 'pipeline2.yml', './pipeline.yml')\n</code></pre> <p>This command will upload the local file <code>./pipeline.yml</code> to the <code>gc-test</code> Google Cloud bucket, where it will be saved as <code>pipeline2.yml</code>.</p>"},{"location":"remote/gc/#delete-a-file-from-google-cloud-storage","title":"Delete a File from Google Cloud Storage","text":"<p>To delete a file from a Google Cloud bucket, run:</p> <pre><code>GC.delete_file('gc-test', 'pipeline2.yml')\n</code></pre> <p>This command will delete the file <code>pipeline2.yml</code> from the <code>gc-test</code> bucket.</p>"},{"location":"remote/gc/#functional-example","title":"Functional Example","text":"<p>As a simple example, the following one-function pipeline demonstrates how you can interact with Google Cloud:</p> <pre><code>from ruffus import *\nimport sys\nimport os\nimport cgatcore.experiment as E\nfrom cgatcore import pipeline as P\nfrom cgatcore.remote.google_cloud import GCRemoteObject\n\n# Load options from the config file\nPARAMS = P.get_parameters([\n    \"%s/pipeline.yml\" % os.path.splitext(__file__)[0],\n    \"../pipeline.yml\",\n    \"pipeline.yml\"\n])\n\nGC = GCRemoteObject()\n\n@transform(GC.download('gc-test', 'pipeline.yml', './pipeline.yml'),\n           regex(r\"(.*)\\.(.*)\"),\n           r\"\\1.counts\")\ndef countWords(infile, outfile):\n    '''Count the number of words in the pipeline configuration file.'''\n\n    # Upload file to Google Cloud\n    GC.upload('gc-test', 'pipeline2.yml', '/ifs/projects/adam/test_remote/data/pipeline.yml')\n\n    # The command line statement we want to execute\n    statement = '''awk 'BEGIN { printf(\"word\\tfreq\\n\"); }\n    {for (i = 1; i &lt;= NF; i++) freq[$i]++}\n    END { for (word in freq) printf \"%%s\\t%%d\\n\", word, freq[word] }'\n    &lt; %(infile)s &gt; %(outfile)s'''\n\n    P.run(statement)\n\n    # Delete file from Google Cloud\n    GC.delete_file('gc-test', 'pipeline2.yml')\n\n@follows(countWords)\ndef full():\n    pass\n</code></pre> <p>In this example:</p> <ol> <li>Download: The <code>countWords</code> function downloads <code>pipeline.yml</code> from the <code>gc-test</code> bucket to <code>./pipeline.yml</code>.</li> <li>Word Count: The function counts the number of words in the file using the <code>awk</code> command.</li> <li>Upload: The processed file is then uploaded to Google Cloud.</li> <li>Delete: Finally, the uploaded file is deleted from the bucket.</li> </ol> <p>This functional example provides a simple illustration of how Google Cloud integration can be achieved within a CGAT pipeline.</p>"},{"location":"remote/s3/","title":"AWS S3 Storage","text":"<p>This section describes how to interact with Amazon's cloud storage service (S3). To interact with the S3 resource, we use the <code>boto3</code> SDK.</p> <p>This documentation is a work in progress. We welcome any feedback or requests for extra features. If you find any bugs, please report them as issues on GitHub.</p>"},{"location":"remote/s3/#setting-up-credentials","title":"Setting up credentials","text":"<p>To use the AWS remote feature, you need to configure your credentials (i.e., the access key and secret key). You can set up these credentials by adding them as environment variables in a file <code>~/.aws/credentials</code>, as detailed in the <code>boto3</code> configuration page. In brief, you need to add the keys as follows:</p> <pre><code>[default]\naws_access_key_id = YOUR_ACCESS_KEY\naws_secret_access_key = YOUR_SECRET_KEY\n</code></pre> <p>These access keys can be found within your S3 AWS console by following these steps:</p> <ol> <li>Log in to your AWS Management Console.</li> <li>Click on your username at the top right of the page.</li> <li>Click \"My Security Credentials.\"</li> <li>Click \"Users\" in the left-hand menu and select a user.</li> <li>Click the \"Security credentials\" tab.</li> <li>YOUR_ACCESS_KEY is located in the \"Access key\" section.</li> </ol> <p>If you have lost YOUR_SECRET_KEY, you will need to create a new access key. Please see the AWS documentation for instructions on how to do this. Note that every 90 days, AWS will rotate your access keys.</p> <p>Additionally, you may want to configure the default region:</p> <pre><code>[default]\nregion=us-east-1\n</code></pre> <p>Once configuration variables have been created, you are ready to interact with the S3 storage.</p>"},{"location":"remote/s3/#using-s3-with-s3pipeline","title":"Using S3 with S3Pipeline","text":"<p>The <code>S3Pipeline</code> class in <code>file_handler.py</code> is a convenient tool to integrate AWS S3 operations into data processing workflows. The class provides decorators to simplify working with S3 files in different stages of the pipeline.</p> <p>First, initiate the class as follows:</p> <pre><code>from cgatcore import pipeline as P\nfrom cgatcore.remote.file_handler import S3Pipeline\n\npipeline = S3Pipeline(name=\"MyPipeline\", temp_dir=\"/tmp\")\n</code></pre> <p>The <code>S3Pipeline</code> class provides several decorators:</p>"},{"location":"remote/s3/#download-from-aws-s3-with-s3_transform","title":"Download from AWS S3 with <code>s3_transform</code>","text":"<p>To download a file, process it, and save the output, use the <code>s3_transform</code> decorator. Here's an example:</p> <pre><code>@pipeline.s3_transform('s3://aws-test-boto/pipeline.yml', '_counts', 's3://aws-test-boto/pipeline_counts.yml')\ndef countWords(input_file, output_file):\n    \"\"\"Count the number of words in the file.\"\"\"\n    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n        content = infile.read()\n        words = content.split()\n        outfile.write(f\"word\\tfreq\\n\")\n        word_freq = {word: words.count(word) for word in set(words)}\n        for word, count in word_freq.items():\n            outfile.write(f\"{word}\\t{count}\\n\")\n</code></pre> <p>This decorator downloads <code>pipeline.yml</code> from the S3 bucket <code>aws-test-boto</code> to a local temporary directory, processes it, and saves the results to a new file (<code>pipeline_counts.yml</code>) back on S3.</p>"},{"location":"remote/s3/#merging-multiple-files-with-s3_merge","title":"Merging Multiple Files with <code>s3_merge</code>","text":"<p>If you need to merge multiple files from S3 into one, use the <code>s3_merge</code> decorator. Here's how:</p> <pre><code>@pipeline.s3_merge(['s3://aws-test-boto/file1.txt', 's3://aws-test-boto/file2.txt'], 's3://aws-test-boto/merged_file.txt')\ndef mergeFiles(input_files, output_file):\n    \"\"\"Merge multiple input files into one.\"\"\"\n    with open(output_file, 'w') as outfile:\n        for file in input_files:\n            with open(file, 'r') as infile:\n                outfile.write(infile.read())\n</code></pre>"},{"location":"remote/s3/#splitting-a-file-with-s3_split","title":"Splitting a File with <code>s3_split</code>","text":"<p>To split a single input file into multiple output files, use the <code>s3_split</code> decorator:</p> <pre><code>@pipeline.s3_split('s3://aws-test-boto/largefile.txt', ['s3://aws-test-boto/part1.txt', 's3://aws-test-boto/part2.txt'])\ndef splitFile(input_file, output_files):\n    \"\"\"Split the input file into multiple output files.\"\"\"\n    with open(input_file, 'r') as infile:\n        content = infile.readlines()\n        mid = len(content) // 2\n        with open(output_files[0], 'w') as outfile1:\n            outfile1.writelines(content[:mid])\n        with open(output_files[1], 'w') as outfile2:\n            outfile2.writelines(content[mid:])\n</code></pre> <p>This splits the large input file into two separate parts, saving them as different S3 objects.</p>"},{"location":"remote/s3/#running-the-pipeline","title":"Running the Pipeline","text":"<p>To run all tasks in the pipeline:</p> <pre><code>pipeline.run()\n</code></pre> <p>This will sequentially execute all tasks that have been added to the pipeline through the decorators.</p>"},{"location":"remote/s3/#example-full-pipeline","title":"Example: Full Pipeline","text":"<p>Here is an example of a simple pipeline that uses the <code>S3Pipeline</code> class to count words in a file, merge two files, and then delete a file:</p> <pre><code>from cgatcore.remote.file_handler import S3Pipeline\n\npipeline = S3Pipeline(name=\"ExamplePipeline\", temp_dir=\"/tmp\")\n\n@pipeline.s3_transform('s3://aws-test-boto/pipeline.yml', '_counts', 's3://aws-test-boto/pipeline_counts.yml')\ndef countWords(input_file, output_file):\n    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n        content = infile.read()\n        words = content.split()\n        outfile.write(f\"word\\tfreq\\n\")\n        word_freq = {word: words.count(word) for word in set(words)}\n        for word, count in word_freq.items():\n            outfile.write(f\"{word}\\t{count}\\n\")\n\n@pipeline.s3_merge(['s3://aws-test-boto/file1.txt', 's3://aws-test-boto/file2.txt'], 's3://aws-test-boto/merged_file.txt')\ndef mergeFiles(input_files, output_file):\n    with open(output_file, 'w') as outfile:\n        for file in input_files:\n            with open(file, 'r') as infile:\n                outfile.write(infile.read())\n\npipeline.run()\n</code></pre> <p>In this example:</p> <ol> <li>Download and Transform: The <code>countWords</code> function downloads a file from S3, counts the words, and uploads the output back to S3.</li> <li>Merge: The <code>mergeFiles</code> function merges two files from S3 and writes the merged output back to S3.</li> <li>Run: Finally, all the tasks are executed sequentially with <code>pipeline.run()</code>.</li> </ol> <p>This updated documentation provides a more accurate representation of the current capabilities of the <code>S3Pipeline</code> class, allowing for an easier and more efficient way to handle AWS S3 resources within your pipelines.</p>"},{"location":"s3_integration/configuring_s3/","title":"Configuring AWS S3 Integration","text":"<p>CGAT-core provides native support for working with AWS S3 storage, allowing pipelines to read from and write to S3 buckets seamlessly.</p>"},{"location":"s3_integration/configuring_s3/#prerequisites","title":"Prerequisites","text":"<ol> <li>AWS Account Setup</li> <li>Active AWS account</li> <li>IAM user with S3 access</li> <li> <p>Access key and secret key</p> </li> <li> <p>Required Packages <pre><code>pip install boto3\npip install cgatcore[s3]\n</code></pre></p> </li> </ol>"},{"location":"s3_integration/configuring_s3/#configuration","title":"Configuration","text":""},{"location":"s3_integration/configuring_s3/#1-aws-credentials","title":"1. AWS Credentials","text":"<p>Configure AWS credentials using one of these methods:</p>"},{"location":"s3_integration/configuring_s3/#a-environment-variables","title":"a. Environment Variables","text":"<pre><code>export AWS_ACCESS_KEY_ID='your_access_key'\nexport AWS_SECRET_ACCESS_KEY='your_secret_key'\nexport AWS_DEFAULT_REGION='your_region'\n</code></pre>"},{"location":"s3_integration/configuring_s3/#b-aws-credentials-file","title":"b. AWS Credentials File","text":"<p>Create <code>~/.aws/credentials</code>: <pre><code>[default]\naws_access_key_id = your_access_key\naws_secret_access_key = your_secret_key\nregion = your_region\n</code></pre></p>"},{"location":"s3_integration/configuring_s3/#c-pipeline-configuration","title":"c. Pipeline Configuration","text":"<p>In <code>pipeline.yml</code>: <pre><code>s3:\n    access_key: your_access_key\n    secret_key: your_secret_key\n    region: your_region\n    bucket: your_default_bucket\n</code></pre></p>"},{"location":"s3_integration/configuring_s3/#2-s3-pipeline-configuration","title":"2. S3 Pipeline Configuration","text":"<p>Configure S3-specific settings in <code>pipeline.yml</code>: <pre><code>s3:\n    # Default bucket for pipeline\n    bucket: my-pipeline-bucket\n\n    # Temporary directory for downloaded files\n    local_tmpdir: /tmp/s3_cache\n\n    # File transfer settings\n    transfer:\n        multipart_threshold: 8388608  # 8MB\n        max_concurrency: 10\n        multipart_chunksize: 8388608  # 8MB\n\n    # Retry configuration\n    retry:\n        max_attempts: 5\n        mode: standard\n</code></pre></p>"},{"location":"s3_integration/configuring_s3/#usage-examples","title":"Usage Examples","text":""},{"location":"s3_integration/configuring_s3/#1-basic-s3-operations","title":"1. Basic S3 Operations","text":""},{"location":"s3_integration/configuring_s3/#reading-from-s3","title":"Reading from S3","text":"<pre><code>from cgatcore import pipeline as P\n\n@P.s3_transform(\"s3://bucket/input.txt\", suffix(\".txt\"), \".processed\")\ndef process_s3_file(infile, outfile):\n    \"\"\"Process a file from S3.\"\"\"\n    statement = \"\"\"\n    cat %(infile)s | process_data &gt; %(outfile)s\n    \"\"\"\n    P.run(statement)\n</code></pre>"},{"location":"s3_integration/configuring_s3/#writing-to-s3","title":"Writing to S3","text":"<pre><code>@P.s3_transform(\"input.txt\", suffix(\".txt\"), \n                \"s3://bucket/output.processed\")\ndef write_to_s3(infile, outfile):\n    \"\"\"Write results to S3.\"\"\"\n    statement = \"\"\"\n    process_data %(infile)s &gt; %(outfile)s\n    \"\"\"\n    P.run(statement)\n</code></pre>"},{"location":"s3_integration/configuring_s3/#2-advanced-operations","title":"2. Advanced Operations","text":""},{"location":"s3_integration/configuring_s3/#working-with-multiple-files","title":"Working with Multiple Files","text":"<pre><code>@P.s3_merge([\"s3://bucket/*.txt\"], \"s3://bucket/merged.txt\")\ndef merge_s3_files(infiles, outfile):\n    \"\"\"Merge multiple S3 files.\"\"\"\n    statement = \"\"\"\n    cat %(infiles)s &gt; %(outfile)s\n    \"\"\"\n    P.run(statement)\n</code></pre>"},{"location":"s3_integration/configuring_s3/#conditional-s3-usage","title":"Conditional S3 Usage","text":"<pre><code>@P.transform(\"*.txt\", suffix(\".txt\"), \n             P.s3_path_if(\"use_s3\", \".processed\"))\ndef conditional_s3(infile, outfile):\n    \"\"\"Use S3 based on configuration.\"\"\"\n    statement = \"\"\"\n    process_data %(infile)s &gt; %(outfile)s\n    \"\"\"\n    P.run(statement)\n</code></pre>"},{"location":"s3_integration/configuring_s3/#best-practices","title":"Best Practices","text":""},{"location":"s3_integration/configuring_s3/#1-performance-optimization","title":"1. Performance Optimization","text":"<ul> <li>Batch Operations: Group small files for transfers</li> <li>Multipart Uploads: Configure for large files</li> <li>Concurrent Transfers: Set appropriate concurrency</li> <li>Local Caching: Use temporary directory efficiently</li> </ul> <pre><code>s3:\n    transfer:\n        multipart_threshold: 100_000_000  # 100MB\n        max_concurrency: 20\n        multipart_chunksize: 10_000_000  # 10MB\n    local_tmpdir: /fast/local/disk/s3_cache\n</code></pre>"},{"location":"s3_integration/configuring_s3/#2-cost-management","title":"2. Cost Management","text":"<ul> <li>Data Transfer: Minimize cross-region transfers</li> <li>Storage Classes: Use appropriate storage tiers</li> <li>Cleanup: Remove temporary files</li> <li>Lifecycle Rules: Configure bucket lifecycle</li> </ul>"},{"location":"s3_integration/configuring_s3/#3-error-handling","title":"3. Error Handling","text":"<pre><code>@P.s3_transform(\"s3://bucket/input.txt\", suffix(\".txt\"), \".processed\")\ndef robust_s3_processing(infile, outfile):\n    \"\"\"Handle S3 operations with proper error checking.\"\"\"\n    try:\n        statement = \"\"\"\n        process_data %(infile)s &gt; %(outfile)s\n        \"\"\"\n        P.run(statement)\n    except P.S3Error as e:\n        L.error(\"S3 operation failed: %s\" % e)\n        raise\n    finally:\n        # Clean up local temporary files\n        P.cleanup_tmpdir()\n</code></pre>"},{"location":"s3_integration/configuring_s3/#troubleshooting","title":"Troubleshooting","text":""},{"location":"s3_integration/configuring_s3/#common-issues","title":"Common Issues","text":"<ol> <li>Access Denied</li> <li>Check AWS credentials</li> <li>Verify IAM permissions</li> <li> <p>Ensure bucket policy allows access</p> </li> <li> <p>Transfer Failures</p> </li> <li>Check network connectivity</li> <li>Verify file permissions</li> <li> <p>Monitor transfer logs</p> </li> <li> <p>Performance Issues</p> </li> <li>Adjust multipart settings</li> <li>Check network bandwidth</li> <li>Monitor memory usage</li> </ol>"},{"location":"s3_integration/configuring_s3/#debugging","title":"Debugging","text":"<p>Enable detailed S3 logging: <pre><code>import logging\nlogging.getLogger('boto3').setLevel(logging.DEBUG)\nlogging.getLogger('botocore').setLevel(logging.DEBUG)\n</code></pre></p>"},{"location":"s3_integration/configuring_s3/#security-considerations","title":"Security Considerations","text":"<ol> <li>Credentials Management</li> <li>Use IAM roles when possible</li> <li>Rotate access keys regularly</li> <li> <p>Never commit credentials</p> </li> <li> <p>Data Protection</p> </li> <li>Enable bucket encryption</li> <li>Use HTTPS endpoints</li> <li> <p>Configure appropriate bucket policies</p> </li> <li> <p>Access Control</p> </li> <li>Implement least privilege</li> <li>Use bucket policies</li> <li>Enable access logging</li> </ol> <p>For more examples of using S3 in your pipelines, see the S3 Pipeline Examples section. - AWS S3 Documentation - Boto3 Documentation</p>"},{"location":"s3_integration/s3_decorators/","title":"CGATcore S3 decorators","text":""},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline--pipelinepy-tools-for-cgat-ruffus-pipelines","title":"pipeline.py - Tools for CGAT Ruffus Pipelines","text":"<p>This module provides a comprehensive set of tools to facilitate the creation and management of data processing pipelines using CGAT Ruffus. It includes functionalities for:</p> <ol> <li>Pipeline Control</li> <li>Task execution and dependency management</li> <li>Command-line interface for pipeline operations</li> <li> <p>Logging and error handling</p> </li> <li> <p>Resource Management</p> </li> <li>Cluster job submission and monitoring</li> <li>Memory and CPU allocation</li> <li> <p>Temporary file handling</p> </li> <li> <p>Configuration</p> </li> <li>Parameter management via YAML configuration</li> <li>Cluster settings customization</li> <li> <p>Pipeline state persistence</p> </li> <li> <p>Cloud Integration</p> </li> <li>AWS S3 support for input/output files</li> <li>Cloud-aware pipeline decorators</li> <li>Remote file handling</li> </ol>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline--example-usage","title":"Example Usage","text":"<p>A basic pipeline using local files:</p> <p>.. code-block:: python</p> <pre><code>from cgatcore import pipeline as P\n\n# Standard pipeline task\n@P.transform(\"input.txt\", suffix(\".txt\"), \".processed\")\ndef process_local_file(infile, outfile):\n    # Processing logic here\n    pass\n</code></pre> <p>Using S3 integration:</p> <p>.. code-block:: python</p> <pre><code># S3-aware pipeline task\n@P.s3_transform(\"s3://bucket/input.txt\", suffix(\".txt\"), \".processed\")\ndef process_s3_file(infile, outfile):\n    # Processing logic here\n    pass\n</code></pre> <p>For detailed documentation, see: https://cgat-core.readthedocs.io/</p>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.ContainerConfig","title":"<code>ContainerConfig</code>","text":"<p>Container configuration for pipeline execution.</p> Source code in <code>cgatcore/pipeline/execution.py</code> <pre><code>class ContainerConfig:\n    \"\"\"Container configuration for pipeline execution.\"\"\"\n\n    def __init__(self, image=None, volumes=None, env_vars=None, runtime=\"docker\"):\n        \"\"\"\n        Args:\n            image (str): Container image (e.g., \"ubuntu:20.04\").\n            volumes (list): Volume mappings (e.g., ['/data:/data']).\n            env_vars (dict): Environment variables for the container.\n            runtime (str): Container runtime (\"docker\" or \"singularity\").\n        \"\"\"\n        self.image = image\n        self.volumes = volumes or []\n        self.env_vars = env_vars or {}\n        self.runtime = runtime.lower()  # Normalise to lowercase\n\n        if self.runtime not in [\"docker\", \"singularity\"]:\n            raise ValueError(\"Unsupported container runtime: {}\".format(self.runtime))\n\n    def get_container_command(self, statement):\n        \"\"\"Convert a statement to run inside a container.\"\"\"\n        if not self.image:\n            return statement\n\n        if self.runtime == \"docker\":\n            return self._get_docker_command(statement)\n        elif self.runtime == \"singularity\":\n            return self._get_singularity_command(statement)\n        else:\n            raise ValueError(\"Unsupported container runtime: {}\".format(self.runtime))\n\n    def _get_docker_command(self, statement):\n        \"\"\"Generate a Docker command.\"\"\"\n        volume_args = [f\"-v {volume}\" for volume in self.volumes]\n        env_args = [f\"-e {key}={value}\" for key, value in self.env_vars.items()]\n\n        return \" \".join([\n            \"docker\", \"run\", \"--rm\",\n            *volume_args, *env_args, self.image,\n            \"/bin/bash\", \"-c\", f\"'{statement}'\"\n        ])\n\n    def _get_singularity_command(self, statement):\n        \"\"\"Generate a Singularity command.\"\"\"\n        volume_args = [f\"--bind {volume}\" for volume in self.volumes]\n        env_args = [f\"--env {key}={value}\" for key, value in self.env_vars.items()]\n\n        return \" \".join([\n            \"singularity\", \"exec\",\n            *volume_args, *env_args, self.image,\n            \"bash\", \"-c\", f\"'{statement}'\"\n        ])\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.ContainerConfig.__init__","title":"<code>__init__(image=None, volumes=None, env_vars=None, runtime='docker')</code>","text":"<p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>Container image (e.g., \"ubuntu:20.04\").</p> <code>None</code> <code>volumes</code> <code>list</code> <p>Volume mappings (e.g., ['/data:/data']).</p> <code>None</code> <code>env_vars</code> <code>dict</code> <p>Environment variables for the container.</p> <code>None</code> <code>runtime</code> <code>str</code> <p>Container runtime (\"docker\" or \"singularity\").</p> <code>'docker'</code> Source code in <code>cgatcore/pipeline/execution.py</code> <pre><code>def __init__(self, image=None, volumes=None, env_vars=None, runtime=\"docker\"):\n    \"\"\"\n    Args:\n        image (str): Container image (e.g., \"ubuntu:20.04\").\n        volumes (list): Volume mappings (e.g., ['/data:/data']).\n        env_vars (dict): Environment variables for the container.\n        runtime (str): Container runtime (\"docker\" or \"singularity\").\n    \"\"\"\n    self.image = image\n    self.volumes = volumes or []\n    self.env_vars = env_vars or {}\n    self.runtime = runtime.lower()  # Normalise to lowercase\n\n    if self.runtime not in [\"docker\", \"singularity\"]:\n        raise ValueError(\"Unsupported container runtime: {}\".format(self.runtime))\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.ContainerConfig.get_container_command","title":"<code>get_container_command(statement)</code>","text":"<p>Convert a statement to run inside a container.</p> Source code in <code>cgatcore/pipeline/execution.py</code> <pre><code>def get_container_command(self, statement):\n    \"\"\"Convert a statement to run inside a container.\"\"\"\n    if not self.image:\n        return statement\n\n    if self.runtime == \"docker\":\n        return self._get_docker_command(statement)\n    elif self.runtime == \"singularity\":\n        return self._get_singularity_command(statement)\n    else:\n        raise ValueError(\"Unsupported container runtime: {}\".format(self.runtime))\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.DRMAACluster","title":"<code>DRMAACluster</code>","text":"<p>               Bases: <code>object</code></p> Source code in <code>cgatcore/pipeline/cluster.py</code> <pre><code>class DRMAACluster(object):\n\n    # dictionary mapping resource usage fields returned by DRMAA\n    # to a common set of names.\n    map_drmaa2benchmark_data = {}\n\n    def __init__(self, session, ignore_errors=False):\n        self.session = session\n        self.ignore_errors = ignore_errors\n\n    def get_resource_usage(self, job_id, retval, hostname):\n        retval.resourceUsage[\"hostname\"] = hostname\n        return [retval]\n\n    def setup_drmaa_job_template(self,\n                                 drmaa_session,\n                                 job_name,\n                                 job_memory,\n                                 job_threads,\n                                 working_directory,\n                                 **kwargs):\n        '''Sets up a Drmma job template. Currently SGE, SLURM, Torque and PBSPro are\n        supported'''\n        if not job_memory:\n            raise ValueError(\"Job memory must be specified when running\"\n                             \"DRMAA jobs\")\n\n        jt = drmaa_session.createJobTemplate()\n        jt.workingDirectory = working_directory\n        jt.jobEnvironment = {'BASH_ENV': '~/.bashrc'}\n        jt.args = []\n        if not re.match(\"[a-zA-Z]\", job_name[0]):\n            job_name = \"_\" + job_name\n\n        spec = self.get_native_specification(job_name,\n                                             job_memory,\n                                             job_threads,\n                                             **kwargs)\n\n        jt.nativeSpecification = \" \".join(spec)\n\n        # keep stdout and stderr separate\n        jt.joinFiles = False\n\n        self.update_template(jt)\n        return jt\n\n    def update_template(self, jt):\n        pass\n\n    def collect_single_job_from_cluster(self,\n                                        job_id,\n                                        statement,\n                                        stdout_path, stderr_path,\n                                        job_path):\n        '''collects a single job on the cluster.\n\n        This method waits until a job has completed and returns\n        stdout, stderr and resource usage.\n        '''\n        try:\n            retval = self.session.wait(\n                job_id, drmaa.Session.TIMEOUT_WAIT_FOREVER)\n        except Exception as msg:\n            # ignore message 24, indicates jobs that have been qdel'ed\n            if not str(msg).startswith(\"code 24\"):\n                raise\n            retval = None\n\n        stdout, stderr = self.get_drmaa_job_stdout_stderr(\n            stdout_path, stderr_path)\n\n        if retval is not None:\n            error_msg = None\n            if retval.exitStatus == 0:\n                if retval.wasAborted is True:\n                    error_msg = (\n                        \"Job {} has exit status 0, but marked as hasAborted=True, hasExited={} \"\n                        \"(Job may have been cancelled by the user or the scheduler due to memory constraints)\"\n                        \"The stderr was \\n{}\\nstatement = {}\".format(\n                            job_id, retval.hasExited, \"\".join(stderr), statement))\n                if retval.hasSignal is True:\n                    error_msg = (\"Job {} has zero exitStatus {} but received signal: hasExited={},  wasAborted={}\"\n                                 \"hasSignal={}, terminatedSignal='{}' \"\n                                 \"\\nstatement = {}\".format(\n                                     job_id, retval.exitStatus, retval.hasExited, retval.wasAborted,\n                                     retval.hasSignal, retval.terminatedSignal,\n                                     statement))\n            else:\n                error_msg = (\"Job {} has non-zero exitStatus {}: hasExited={},  wasAborted={}\"\n                             \"hasSignal={}, terminatedSignal='{}' \"\n                             \"\\nstatement = {}\".format(\n                                 job_id, retval.exitStatus, retval.hasExited, retval.wasAborted,\n                                 retval.hasSignal, retval.terminatedSignal,\n                                 statement))\n\n            if error_msg:\n                if stderr:\n                    error_msg += \"\\n stderr = {}\".format(\"\".join(stderr))\n                if self.ignore_errors:\n                    get_logger().warning(error_msg)\n                else:\n                    raise OSError(error_msg)\n        else:\n            retval = JobInfo(job_id, {})\n\n        # get hostname from job script\n        try:\n            hostname = stdout[-3][:-1]\n        except IndexError:\n            hostname = \"unknown\"\n\n        try:\n            resource_usage = self.get_resource_usage(job_id, retval, hostname)\n        except (ValueError, KeyError, TypeError, IndexError) as ex:\n            E.warn(\"could not collect resource usage for job {}: {}\".format(job_id, ex))\n            retval.resourceUsage[\"hostname\"] = hostname\n            resource_usage = [retval]\n\n        try:\n            os.unlink(job_path)\n        except OSError:\n            self.logger.warn(\n                (\"temporary job file %s not present for \"\n                 \"clean-up - ignored\") % job_path)\n\n        return stdout, stderr, resource_usage\n\n    def get_drmaa_job_stdout_stderr(self, stdout_path, stderr_path,\n                                    tries=5, encoding=\"utf-8\"):\n        '''get stdout/stderr allowing for some lag.\n\n        Try at most *tries* times. If unsuccessfull, throw OSError\n\n        Removes the files once they are read.\n\n        Returns tuple of stdout and stderr as unicode strings.\n        '''\n        x = tries\n        while x &gt;= 0:\n            if os.path.exists(stdout_path):\n                break\n            gevent.sleep(GEVENT_TIMEOUT_WAIT)\n            x -= 1\n\n        x = tries\n        while x &gt;= 0:\n            if os.path.exists(stderr_path):\n                break\n            gevent.sleep(GEVENT_TIMEOUT_WAIT)\n            x -= 1\n\n        try:\n            with open(stdout_path, \"r\", encoding=encoding) as inf:\n                stdout = inf.readlines()\n        except IOError as msg:\n            get_logger().warning(\"could not open stdout: %s\" % msg)\n            stdout = []\n\n        try:\n            with open(stderr_path, \"r\", encoding=encoding) as inf:\n                stderr = inf.readlines()\n        except IOError as msg:\n            get_logger().warning(\"could not open stdout: %s\" % msg)\n            stderr = []\n\n        try:\n            os.unlink(stdout_path)\n            os.unlink(stderr_path)\n        except OSError as msg:\n            pass\n\n        return stdout, stderr\n\n    def set_drmaa_job_paths(self, job_template, job_path):\n        '''Adds the job_path, stdout_path and stderr_paths\n           to the job_template.\n        '''\n        job_path = os.path.abspath(job_path)\n        os.chmod(job_path, stat.S_IRWXG | stat.S_IRWXU)\n\n        stdout_path = job_path + \".stdout\"\n        stderr_path = job_path + \".stderr\"\n\n        job_template.remoteCommand = job_path\n        job_template.outputPath = \":\" + stdout_path\n        job_template.errorPath = \":\" + stderr_path\n\n        return stdout_path, stderr_path\n\n    def map_resource_usage(self, resource_usage, data2type):\n        \"\"\"return job metrics mapped to common name and converted to right type.\"\"\"\n        def _convert(key, v, tpe):\n            if v is None:\n                return None\n            else:\n                try:\n                    return tpe(v)\n                except ValueError as ex:\n                    E.warning(\"could not convert {} with value '{}' to {}: {}\".format(\n                        key, v, tpe, ex))\n                    return v\n\n        return dict([(key,\n                      _convert(key, resource_usage.get(self.map_drmaa2benchmark_data.get(key, key), None), tpe))\n                     for key, tpe in data2type.items()])\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.DRMAACluster.collect_single_job_from_cluster","title":"<code>collect_single_job_from_cluster(job_id, statement, stdout_path, stderr_path, job_path)</code>","text":"<p>collects a single job on the cluster.</p> <p>This method waits until a job has completed and returns stdout, stderr and resource usage.</p> Source code in <code>cgatcore/pipeline/cluster.py</code> <pre><code>def collect_single_job_from_cluster(self,\n                                    job_id,\n                                    statement,\n                                    stdout_path, stderr_path,\n                                    job_path):\n    '''collects a single job on the cluster.\n\n    This method waits until a job has completed and returns\n    stdout, stderr and resource usage.\n    '''\n    try:\n        retval = self.session.wait(\n            job_id, drmaa.Session.TIMEOUT_WAIT_FOREVER)\n    except Exception as msg:\n        # ignore message 24, indicates jobs that have been qdel'ed\n        if not str(msg).startswith(\"code 24\"):\n            raise\n        retval = None\n\n    stdout, stderr = self.get_drmaa_job_stdout_stderr(\n        stdout_path, stderr_path)\n\n    if retval is not None:\n        error_msg = None\n        if retval.exitStatus == 0:\n            if retval.wasAborted is True:\n                error_msg = (\n                    \"Job {} has exit status 0, but marked as hasAborted=True, hasExited={} \"\n                    \"(Job may have been cancelled by the user or the scheduler due to memory constraints)\"\n                    \"The stderr was \\n{}\\nstatement = {}\".format(\n                        job_id, retval.hasExited, \"\".join(stderr), statement))\n            if retval.hasSignal is True:\n                error_msg = (\"Job {} has zero exitStatus {} but received signal: hasExited={},  wasAborted={}\"\n                             \"hasSignal={}, terminatedSignal='{}' \"\n                             \"\\nstatement = {}\".format(\n                                 job_id, retval.exitStatus, retval.hasExited, retval.wasAborted,\n                                 retval.hasSignal, retval.terminatedSignal,\n                                 statement))\n        else:\n            error_msg = (\"Job {} has non-zero exitStatus {}: hasExited={},  wasAborted={}\"\n                         \"hasSignal={}, terminatedSignal='{}' \"\n                         \"\\nstatement = {}\".format(\n                             job_id, retval.exitStatus, retval.hasExited, retval.wasAborted,\n                             retval.hasSignal, retval.terminatedSignal,\n                             statement))\n\n        if error_msg:\n            if stderr:\n                error_msg += \"\\n stderr = {}\".format(\"\".join(stderr))\n            if self.ignore_errors:\n                get_logger().warning(error_msg)\n            else:\n                raise OSError(error_msg)\n    else:\n        retval = JobInfo(job_id, {})\n\n    # get hostname from job script\n    try:\n        hostname = stdout[-3][:-1]\n    except IndexError:\n        hostname = \"unknown\"\n\n    try:\n        resource_usage = self.get_resource_usage(job_id, retval, hostname)\n    except (ValueError, KeyError, TypeError, IndexError) as ex:\n        E.warn(\"could not collect resource usage for job {}: {}\".format(job_id, ex))\n        retval.resourceUsage[\"hostname\"] = hostname\n        resource_usage = [retval]\n\n    try:\n        os.unlink(job_path)\n    except OSError:\n        self.logger.warn(\n            (\"temporary job file %s not present for \"\n             \"clean-up - ignored\") % job_path)\n\n    return stdout, stderr, resource_usage\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.DRMAACluster.get_drmaa_job_stdout_stderr","title":"<code>get_drmaa_job_stdout_stderr(stdout_path, stderr_path, tries=5, encoding='utf-8')</code>","text":"<p>get stdout/stderr allowing for some lag.</p> <p>Try at most tries times. If unsuccessfull, throw OSError</p> <p>Removes the files once they are read.</p> <p>Returns tuple of stdout and stderr as unicode strings.</p> Source code in <code>cgatcore/pipeline/cluster.py</code> <pre><code>def get_drmaa_job_stdout_stderr(self, stdout_path, stderr_path,\n                                tries=5, encoding=\"utf-8\"):\n    '''get stdout/stderr allowing for some lag.\n\n    Try at most *tries* times. If unsuccessfull, throw OSError\n\n    Removes the files once they are read.\n\n    Returns tuple of stdout and stderr as unicode strings.\n    '''\n    x = tries\n    while x &gt;= 0:\n        if os.path.exists(stdout_path):\n            break\n        gevent.sleep(GEVENT_TIMEOUT_WAIT)\n        x -= 1\n\n    x = tries\n    while x &gt;= 0:\n        if os.path.exists(stderr_path):\n            break\n        gevent.sleep(GEVENT_TIMEOUT_WAIT)\n        x -= 1\n\n    try:\n        with open(stdout_path, \"r\", encoding=encoding) as inf:\n            stdout = inf.readlines()\n    except IOError as msg:\n        get_logger().warning(\"could not open stdout: %s\" % msg)\n        stdout = []\n\n    try:\n        with open(stderr_path, \"r\", encoding=encoding) as inf:\n            stderr = inf.readlines()\n    except IOError as msg:\n        get_logger().warning(\"could not open stdout: %s\" % msg)\n        stderr = []\n\n    try:\n        os.unlink(stdout_path)\n        os.unlink(stderr_path)\n    except OSError as msg:\n        pass\n\n    return stdout, stderr\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.DRMAACluster.map_resource_usage","title":"<code>map_resource_usage(resource_usage, data2type)</code>","text":"<p>return job metrics mapped to common name and converted to right type.</p> Source code in <code>cgatcore/pipeline/cluster.py</code> <pre><code>def map_resource_usage(self, resource_usage, data2type):\n    \"\"\"return job metrics mapped to common name and converted to right type.\"\"\"\n    def _convert(key, v, tpe):\n        if v is None:\n            return None\n        else:\n            try:\n                return tpe(v)\n            except ValueError as ex:\n                E.warning(\"could not convert {} with value '{}' to {}: {}\".format(\n                    key, v, tpe, ex))\n                return v\n\n    return dict([(key,\n                  _convert(key, resource_usage.get(self.map_drmaa2benchmark_data.get(key, key), None), tpe))\n                 for key, tpe in data2type.items()])\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.DRMAACluster.set_drmaa_job_paths","title":"<code>set_drmaa_job_paths(job_template, job_path)</code>","text":"<p>Adds the job_path, stdout_path and stderr_paths to the job_template.</p> Source code in <code>cgatcore/pipeline/cluster.py</code> <pre><code>def set_drmaa_job_paths(self, job_template, job_path):\n    '''Adds the job_path, stdout_path and stderr_paths\n       to the job_template.\n    '''\n    job_path = os.path.abspath(job_path)\n    os.chmod(job_path, stat.S_IRWXG | stat.S_IRWXU)\n\n    stdout_path = job_path + \".stdout\"\n    stderr_path = job_path + \".stderr\"\n\n    job_template.remoteCommand = job_path\n    job_template.outputPath = \":\" + stdout_path\n    job_template.errorPath = \":\" + stderr_path\n\n    return stdout_path, stderr_path\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.DRMAACluster.setup_drmaa_job_template","title":"<code>setup_drmaa_job_template(drmaa_session, job_name, job_memory, job_threads, working_directory, **kwargs)</code>","text":"<p>Sets up a Drmma job template. Currently SGE, SLURM, Torque and PBSPro are supported</p> Source code in <code>cgatcore/pipeline/cluster.py</code> <pre><code>def setup_drmaa_job_template(self,\n                             drmaa_session,\n                             job_name,\n                             job_memory,\n                             job_threads,\n                             working_directory,\n                             **kwargs):\n    '''Sets up a Drmma job template. Currently SGE, SLURM, Torque and PBSPro are\n    supported'''\n    if not job_memory:\n        raise ValueError(\"Job memory must be specified when running\"\n                         \"DRMAA jobs\")\n\n    jt = drmaa_session.createJobTemplate()\n    jt.workingDirectory = working_directory\n    jt.jobEnvironment = {'BASH_ENV': '~/.bashrc'}\n    jt.args = []\n    if not re.match(\"[a-zA-Z]\", job_name[0]):\n        job_name = \"_\" + job_name\n\n    spec = self.get_native_specification(job_name,\n                                         job_memory,\n                                         job_threads,\n                                         **kwargs)\n\n    jt.nativeSpecification = \" \".join(spec)\n\n    # keep stdout and stderr separate\n    jt.joinFiles = False\n\n    self.update_template(jt)\n    return jt\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.EventPool","title":"<code>EventPool</code>","text":"<p>               Bases: <code>Pool</code></p> Source code in <code>cgatcore/pipeline/control.py</code> <pre><code>class EventPool(gevent.pool.Pool):\n\n    def __len__(self):\n        \"\"\"make sure that pool always evaluates to true.\"\"\"\n        line = gevent.pool.Pool.__len__(self)\n        if not line:\n            return 1\n        return line\n\n    def close(self):\n        pass\n\n    def terminate(self):\n        self.kill()\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.EventPool.__len__","title":"<code>__len__()</code>","text":"<p>make sure that pool always evaluates to true.</p> Source code in <code>cgatcore/pipeline/control.py</code> <pre><code>def __len__(self):\n    \"\"\"make sure that pool always evaluates to true.\"\"\"\n    line = gevent.pool.Pool.__len__(self)\n    if not line:\n        return 1\n    return line\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.Executor","title":"<code>Executor</code>","text":"<p>               Bases: <code>object</code></p> Source code in <code>cgatcore/pipeline/execution.py</code> <pre><code>class Executor(object):\n\n    def __init__(self, **kwargs):\n\n        self.logger = get_logger()\n        self.queue_manager = None\n        self.run_on_cluster = will_run_on_cluster(kwargs)\n        self.job_threads = kwargs.get(\"job_threads\", 1)\n        self.active_jobs = []  # List to track active jobs\n\n        if \"job_memory\" in kwargs and \"job_total_memory\" in kwargs:\n            raise ValueError(\n                \"both job_memory and job_total_memory have been given\")\n\n        self.job_total_memory = kwargs.get('job_total_memory', None)\n        self.job_memory = kwargs.get('job_memory', None)\n\n        if self.job_total_memory == \"unlimited\" or self.job_memory == \"unlimited\":\n            self.job_total_memory = self.job_memory = \"unlimited\"\n        else:\n            if self.job_total_memory:\n                self.job_memory = iotools.bytes2human(\n                    iotools.human2bytes(self.job_total_memory) / self.job_threads)\n            elif self.job_memory:\n                self.job_total_memory = self.job_memory * self.job_threads\n            else:\n                self.job_memory = get_params()[\"cluster\"].get(\n                    \"memory_default\", \"4G\")\n                if self.job_memory == \"unlimited\":\n                    self.job_total_memory = \"unlimited\"\n                else:\n                    self.job_total_memory = self.job_memory * self.job_threads\n\n        self.ignore_pipe_errors = kwargs.get('ignore_pipe_errors', False)\n        self.ignore_errors = kwargs.get('ignore_errors', False)\n\n        self.job_name = kwargs.get(\"job_name\", \"unknow_job_name\")\n        self.task_name = kwargs.get(\"task_name\", \"unknown_task_name\")\n\n        # deduce output directory/directories, requires somewhat\n        # consistent naming in the calling function.\n        outfiles = []\n        if \"outfile\" in kwargs:\n            outfiles.append(kwargs[\"outfile\"])\n        if \"outfiles\" in kwargs:\n            outfiles.extend(kwargs[\"outfiles\"])\n\n        self.output_directories = set(sorted(\n            [os.path.dirname(x) for x in outfiles]))\n\n        self.options = kwargs\n\n        self.work_dir = get_params()[\"work_dir\"]\n\n        self.shellfile = kwargs.get(\"shell_logfile\", None)\n        if self.shellfile:\n            if not self.shellfile.startswith(os.sep):\n                self.shellfile = os.path.join(\n                    self.work_dir, os.path.basename(self.shellfile))\n\n        self.monitor_interval_queued = kwargs.get('monitor_interval_queued', None)\n        if self.monitor_interval_queued is None:\n            self.monitor_interval_queued = get_params()[\"cluster\"].get(\n                'monitor_interval_queued_default', GEVENT_TIMEOUT_WAIT)\n        self.monitor_interval_running = kwargs.get('monitor_interval_running', None)\n        if self.monitor_interval_running is None:\n            self.monitor_interval_running = get_params()[\"cluster\"].get(\n                'monitor_interval_running_default', GEVENT_TIMEOUT_WAIT)\n        # Set up signal handlers for clean-up on interruption\n        self.setup_signal_handlers()\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        pass\n\n    def expand_statement(self, statement):\n        '''add generic commands before and after statement.\n\n        The method scans the statement for arvados mount points and\n        inserts appropriate prefixes to make sure that the mount point\n        exists.\n\n        Arguments\n        ---------\n        statement : string\n            Command line statement to expand\n\n        Returns\n        -------\n        statement : string\n            The expanded statement.\n\n        '''\n\n        setup_cmds = []\n        teardown_cmds = []\n        cleanup_funcs = []\n\n        setup_cmds.append(\"umask 002\")\n\n        for var in [\"MKL_NUM_THREADS\",\n                    \"OPENBLAS_NUM_THREADS\",\n                    \"OMP_NUM_THREADS\"]:\n            setup_cmds.append(\"export {}={}\".format(\n                var, self.options.get(\"job_threads\", 1)))\n\n        if \"arv=\" in statement:\n\n            # Todo: permit setting this in params\n            arvados_api_token = os.environ.get(\"ARVADOS_API_TOKEN\", None)\n            arvados_api_host = os.environ.get(\"ARVADOS_API_HOST\", None)\n            if not arvados_api_token:\n                raise ValueError(\n                    \"arvados mount encountered in statement {}, \"\n                    \"but ARVADOS_API_TOKEN not defined\".format(statement))\n\n            if not arvados_api_host:\n                raise ValueError(\n                    \"arvados mount encountered in statement {}, \"\n                    \"but ARVADOS_API_HOST not defined\".format(statement))\n\n            mountpoint = get_temp_filename(clear=True)\n\n            arvados_options = \"--disable-event-listening --read-only\"\n            setup_cmds.append(\"\\n\".join(\n                ('export ARVADOS_API_TOKEN=\"{arvados_api_token}\"',\n                 'export ARVADOS_API_HOST=\"{arvados_api_host}\"',\n                 'export ARVADOS_API_HOST_INSECURE=true',\n                 'export ARVADOS_MOUNT_POINT=\"{mountpoint}\"',\n                 'mkdir -p \"{mountpoint}\"',\n                 'arv-mount {arvados_options} \"{mountpoint}\" 2&gt;&gt; /dev/null')).format(**locals()))\n\n            statement = re.sub(\"arv=\", mountpoint + \"/\", statement)\n\n            # \"arv-mount --unmount {mountpoint}\" not available in newer\n            # arvados installs (0.1.20170707152712), so keep using\n            # fusermount. However, do not fail if you can't clean up, as\n            # there are arvados racing issues.\n            cleanup_funcs.append((\"unmount_arvados\",\n                                  '''{{\n                                  set +e &amp;&amp;\n                                  fusermount -u {mountpoint} &amp;&amp;\n                                  rm -rf {mountpoint} &amp;&amp;\n                                  set -e\n                                  }}'''.format(**locals())))\n\n        if \"job_condaenv\" in self.options:\n            # In conda &lt; 4.4 there is an issue with parallel activations,\n            # see https://github.com/conda/conda/issues/2837 .\n            # This has been fixed in conda 4.4, but we are on conda\n            # 4.3, presumably because we are still on py35. A work-around\n            # to source activate is to add the explicit path of the environment\n            # in version &gt;= 4.4, do\n            # setup_cmds.append(\n            #     \"conda activate {}\".format(self.options[\"job_condaenv\"]))\n            # For old conda versions (note this will not work for tools that require\n            # additional environment variables)\n            setup_cmds.append(\n                \"export PATH={}:$PATH\".format(\n                    os.path.join(\n                        get_conda_environment_directory(\n                            self.options[\"job_condaenv\"]),\n                        \"bin\")))\n\n        statement = \"\\n\".join((\n            \"\\n\".join(setup_cmds),\n            statement,\n            \"\\n\".join(teardown_cmds)))\n\n        return statement, cleanup_funcs\n\n    def build_job_script(self,\n                         statement):\n        '''build job script from statement.\n\n        returns (name_of_script, stdout_path, stderr_path)\n        '''\n        tmpfilename = get_temp_filename(dir=self.work_dir, clear=True)\n        tmpfilename = tmpfilename + \".sh\"\n\n        expanded_statement, cleanup_funcs = self.expand_statement(statement)\n\n        with open(tmpfilename, \"w\") as tmpfile:\n            # disabled: -l -O expand_aliases\\n\" )\n\n            # make executable\n            tmpfile.write(\"#!/bin/bash -eu\\n\")\n            if not self.ignore_pipe_errors:\n                tmpfile.write(\"set -o pipefail\\n\")\n\n            os.chmod(tmpfilename, stat.S_IRWXG | stat.S_IRWXU)\n\n            tmpfile.write(\"\\ncd {}\\n\".format(self.work_dir))\n            if self.output_directories is not None:\n                for outdir in self.output_directories:\n                    if outdir:\n                        tmpfile.write(\"\\nmkdir -p {}\\n\".format(outdir))\n\n            # create and set system scratch dir for temporary files\n            tmpfile.write(\"umask 002\\n\")\n\n            cluster_tmpdir = get_params()[\"cluster_tmpdir\"]\n\n            if self.run_on_cluster and cluster_tmpdir:\n                tmpdir = cluster_tmpdir\n                tmpfile.write(\"TMPDIR=`mktemp -d -p {}`\\n\".format(tmpdir))\n                tmpfile.write(\"export TMPDIR\\n\")\n            else:\n                tmpdir = get_temp_dir(dir=get_params()[\"tmpdir\"],\n                                      clear=True)\n                tmpfile.write(\"mkdir -p {}\\n\".format(tmpdir))\n                tmpfile.write(\"export TMPDIR={}\\n\".format(tmpdir))\n\n            cleanup_funcs.append(\n                (\"clean_temp\",\n                 \"{{ rm -rf {}; }}\".format(tmpdir)))\n\n            # output times whenever script exits, preserving\n            # return status\n            cleanup_funcs.append((\"info\",\n                                  \"{ echo 'benchmark'; hostname; times; }\"))\n            for cleanup_func, cleanup_code in cleanup_funcs:\n                tmpfile.write(\"\\n{}() {}\\n\".format(cleanup_func, cleanup_code))\n\n            tmpfile.write(\"\\nclean_all() {{ {}; }}\\n\".format(\n                \"; \".join([x[0] for x in cleanup_funcs])))\n\n            tmpfile.write(\"\\ntrap clean_all EXIT\\n\\n\")\n\n            if self.job_memory not in (\"unlimited\", \"etc\") and \\\n               self.options.get(\"cluster_memory_ulimit\", False):\n                # restrict virtual memory\n                # Note that there are resources in SGE which could do this directly\n                # such as v_hmem.\n                # Note that limiting resident set sizes (RSS) with ulimit is not\n                # possible in newer kernels.\n                # -v and -m accept memory in kb\n                requested_memory_kb = max(\n                    1000,\n                    int(math.ceil(\n                        iotools.human2bytes(self.job_memory) / 1024 * self.job_threads)))\n                # unsetting error exit as often not permissions\n                tmpfile.write(\"set +e\\n\")\n                tmpfile.write(\"ulimit -v {} &gt; /dev/null \\n\".format(\n                    requested_memory_kb))\n                tmpfile.write(\"ulimit -m {} &gt; /dev/null \\n\".format(\n                    requested_memory_kb))\n                # set as hard limit\n                tmpfile.write(\"ulimit -H -v &gt; /dev/null \\n\")\n                tmpfile.write(\"set -e\\n\")\n\n            if self.shellfile:\n\n                # make sure path exists that we want to write to\n                tmpfile.write(\"mkdir -p $(dirname \\\"{}\\\")\\n\".format(\n                    self.shellfile))\n\n                # output low-level debugging information to a shell log file\n                tmpfile.write(\n                    'echo \"%s : START -&gt; %s\" &gt;&gt; %s\\n' %\n                    (self.job_name, tmpfilename, self.shellfile))\n                # disabled - problems with quoting\n                # tmpfile.write( '''echo 'statement=%s' &gt;&gt; %s\\n''' %\n                # (shellquote(statement), self.shellfile) )\n                tmpfile.write(\"set | sed 's/^/%s : /' &gt;&gt; %s\\n\" %\n                              (self.job_name, self.shellfile))\n                tmpfile.write(\"pwd | sed 's/^/%s : /' &gt;&gt; %s\\n\" %\n                              (self.job_name, self.shellfile))\n                tmpfile.write(\"hostname | sed 's/^/%s: /' &gt;&gt; %s\\n\" %\n                              (self.job_name, self.shellfile))\n                # cat /proc/meminfo is Linux specific\n                if get_params()['os'] == 'Linux':\n                    tmpfile.write(\"cat /proc/meminfo | sed 's/^/%s: /' &gt;&gt; %s\\n\" %\n                                  (self.job_name, self.shellfile))\n                elif get_params()['os'] == 'Darwin':\n                    tmpfile.write(\"vm_stat | sed 's/^/%s: /' &gt;&gt; %s\\n\" %\n                                  (self.job_name, self.shellfile))\n                tmpfile.write(\n                    'echo \"%s : END -&gt; %s\" &gt;&gt; %s\\n' %\n                    (self.job_name, tmpfilename, self.shellfile))\n                tmpfile.write(\"ulimit | sed 's/^/%s: /' &gt;&gt; %s\\n\" %\n                              (self.job_name, self.shellfile))\n\n            job_path = os.path.abspath(tmpfilename)\n\n            tmpfile.write(expanded_statement)\n            tmpfile.write(\"\\n\\n\")\n            tmpfile.close()\n\n        return statement, job_path\n\n    def collect_benchmark_data(self,\n                               statements,\n                               resource_usage):\n        \"\"\"collect benchmark data from a job's stdout and any resource usage\n        information that might be present.\n\n        If time_data is given, read output from time command.\n        \"\"\"\n\n        benchmark_data = []\n\n        def get_val(d, v, alt):\n            val = d.get(v, alt)\n            if val == \"unknown\" or val is None:\n                val = alt\n            return val\n\n        # build resource usage data structure - part native, part\n        # mapped to common fields\n        for jobinfo, statement in zip(resource_usage, statements):\n\n            if resource_usage is None:\n                E.warn(\"no resource usage for {}\".format(self.task_name))\n                continue\n\n            # add some common fields\n            data = {\"task\": self.task_name,\n                    \"engine\": self.__class__.__name__,\n                    \"statement\": statement,\n                    \"job_id\": jobinfo.jobId,\n                    \"slots\": self.job_threads}\n\n            # native specs\n            data.update(jobinfo.resourceUsage)\n\n            # translate specs\n            if self.queue_manager:\n                data.update(\n                    self.queue_manager.map_resource_usage(data, DATA2TYPE))\n\n            cpu_time = float(get_val(data, \"cpu_t\", 0))\n            start_time = float(get_val(data, \"start_time\", 0))\n            end_time = float(get_val(data, \"end_time\", 0))\n            data.update({\n                # avoid division by 0 error\n                \"percent_cpu\": (\n                    100.0 * cpu_time / max(1.0, (end_time - start_time)) / self.job_threads),\n                \"total_t\": end_time - start_time\n            })\n            benchmark_data.append(data)\n\n        return benchmark_data\n\n    def set_container_config(self, image, volumes=None, env_vars=None, runtime=\"docker\"):\n        \"\"\"Set container configuration for all tasks executed by this executor.\"\"\"\n\n        if not image:\n            raise ValueError(\"An image must be specified for the container configuration.\")\n        self.container_config = ContainerConfig(image=image, volumes=volumes, env_vars=env_vars, runtime=runtime)\n\n    def start_job(self, job_info):\n        \"\"\"Add a job to active_jobs list when it starts.\"\"\"\n        self.active_jobs.append(job_info)\n        self.logger.info(f\"Job started: {job_info}\")\n\n    def finish_job(self, job_info):\n        \"\"\"Remove a job from active_jobs list when it finishes.\"\"\"\n        if job_info in self.active_jobs:\n            self.active_jobs.remove(job_info)\n            self.logger.info(f\"Job completed: {job_info}\")\n\n    def cleanup_all_jobs(self):\n        \"\"\"Clean up all remaining active jobs on interruption.\"\"\"\n        self.logger.info(\"Cleaning up all job outputs due to pipeline interruption\")\n        for job_info in self.active_jobs:\n            self.cleanup_failed_job(job_info)\n        self.active_jobs.clear()  # Clear the list after cleanup\n\n    def setup_signal_handlers(self):\n        \"\"\"Set up signal handlers to clean up jobs on SIGINT and SIGTERM.\"\"\"\n\n        def signal_handler(signum, frame):\n            self.logger.info(f\"Received signal {signum}. Starting clean-up.\")\n            self.cleanup_all_jobs()\n            exit(1)\n\n        signal.signal(signal.SIGINT, signal_handler)\n        signal.signal(signal.SIGTERM, signal_handler)\n\n    def cleanup_failed_job(self, job_info):\n        \"\"\"Clean up files generated by a failed job.\"\"\"\n        if \"outfile\" in job_info:\n            outfiles = [job_info[\"outfile\"]]\n        elif \"outfiles\" in job_info:\n            outfiles = job_info[\"outfiles\"]\n        else:\n            self.logger.warning(f\"No output files found for job {job_info.get('job_name', 'unknown')}\")\n            return\n\n        for outfile in outfiles:\n            if os.path.exists(outfile):\n                try:\n                    os.remove(outfile)\n                    self.logger.info(f\"Removed failed job output file: {outfile}\")\n                except OSError as e:\n                    self.logger.error(f\"Error removing file {outfile}: {str(e)}\")\n            else:\n                self.logger.info(f\"Output file not found (already removed or not created): {outfile}\")\n\n    def run(\n            self,\n            statement_list,\n            job_memory=None,\n            job_threads=None,\n            container_runtime=None,\n            image=None,\n            volumes=None,\n            env_vars=None,\n            **kwargs,):\n\n        \"\"\"\n        Execute a list of statements with optional container support.\n\n            Args:\n                statement_list (list): List of commands to execute.\n                job_memory (str): Memory requirements (e.g., \"4G\").\n                job_threads (int): Number of threads to use.\n                container_runtime (str): Container runtime (\"docker\" or \"singularity\").\n                image (str): Container image to use.\n                volumes (list): Volume mappings (e.g., ['/data:/data']).\n                env_vars (dict): Environment variables for the container.\n                **kwargs: Additional arguments.\n        \"\"\"\n        # Validation checks\n        if container_runtime and container_runtime not in [\"docker\", \"singularity\"]:\n            self.logger.error(f\"Invalid container_runtime: {container_runtime}\")\n            raise ValueError(\"Container runtime must be 'docker' or 'singularity'\")\n\n        if container_runtime and not image:\n            self.logger.error(f\"Container runtime specified without an image: {container_runtime}\")\n            raise ValueError(\"An image must be specified when using a container runtime\")\n\n        benchmark_data = []\n\n        for statement in statement_list:\n            job_info = {\"statement\": statement}\n            self.start_job(job_info)\n\n            try:\n                # Prepare containerized execution\n                if container_runtime:\n                    self.set_container_config(image=image, volumes=volumes, env_vars=env_vars, runtime=container_runtime)\n                    statement = self.container_config.get_container_command(statement)\n\n                # Add memory and thread environment variables\n                if job_memory:\n                    env_vars = env_vars or {}\n                    env_vars[\"JOB_MEMORY\"] = job_memory\n                if job_threads:\n                    env_vars = env_vars or {}\n                    env_vars[\"JOB_THREADS\"] = job_threads\n\n                # Debugging: Log the constructed command\n                self.logger.info(f\"Executing command: {statement}\")\n\n                # Build and execute the statement\n                full_statement, job_path = self.build_job_script(statement)\n                process = subprocess.Popen(\n                    full_statement, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE\n                )\n                stdout, stderr = process.communicate()\n\n                if process.returncode != 0:\n                    raise OSError(\n                        f\"Job failed with return code {process.returncode}.\\n\"\n                        f\"stderr: {stderr.decode('utf-8')}\\ncommand: {statement}\"\n                    )\n\n                # Collect benchmark data for successful jobs\n                benchmark_data.append(\n                    self.collect_benchmark_data(\n                        statement, resource_usage={\"job_id\": process.pid}\n                    )\n                )\n                self.finish_job(job_info)\n\n            except Exception as e:\n                self.logger.error(f\"Job failed: {e}\")\n                self.cleanup_failed_job(job_info)\n                if not self.ignore_errors:\n                    raise\n\n        return benchmark_data\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.Executor.build_job_script","title":"<code>build_job_script(statement)</code>","text":"<p>build job script from statement.</p> <p>returns (name_of_script, stdout_path, stderr_path)</p> Source code in <code>cgatcore/pipeline/execution.py</code> <pre><code>def build_job_script(self,\n                     statement):\n    '''build job script from statement.\n\n    returns (name_of_script, stdout_path, stderr_path)\n    '''\n    tmpfilename = get_temp_filename(dir=self.work_dir, clear=True)\n    tmpfilename = tmpfilename + \".sh\"\n\n    expanded_statement, cleanup_funcs = self.expand_statement(statement)\n\n    with open(tmpfilename, \"w\") as tmpfile:\n        # disabled: -l -O expand_aliases\\n\" )\n\n        # make executable\n        tmpfile.write(\"#!/bin/bash -eu\\n\")\n        if not self.ignore_pipe_errors:\n            tmpfile.write(\"set -o pipefail\\n\")\n\n        os.chmod(tmpfilename, stat.S_IRWXG | stat.S_IRWXU)\n\n        tmpfile.write(\"\\ncd {}\\n\".format(self.work_dir))\n        if self.output_directories is not None:\n            for outdir in self.output_directories:\n                if outdir:\n                    tmpfile.write(\"\\nmkdir -p {}\\n\".format(outdir))\n\n        # create and set system scratch dir for temporary files\n        tmpfile.write(\"umask 002\\n\")\n\n        cluster_tmpdir = get_params()[\"cluster_tmpdir\"]\n\n        if self.run_on_cluster and cluster_tmpdir:\n            tmpdir = cluster_tmpdir\n            tmpfile.write(\"TMPDIR=`mktemp -d -p {}`\\n\".format(tmpdir))\n            tmpfile.write(\"export TMPDIR\\n\")\n        else:\n            tmpdir = get_temp_dir(dir=get_params()[\"tmpdir\"],\n                                  clear=True)\n            tmpfile.write(\"mkdir -p {}\\n\".format(tmpdir))\n            tmpfile.write(\"export TMPDIR={}\\n\".format(tmpdir))\n\n        cleanup_funcs.append(\n            (\"clean_temp\",\n             \"{{ rm -rf {}; }}\".format(tmpdir)))\n\n        # output times whenever script exits, preserving\n        # return status\n        cleanup_funcs.append((\"info\",\n                              \"{ echo 'benchmark'; hostname; times; }\"))\n        for cleanup_func, cleanup_code in cleanup_funcs:\n            tmpfile.write(\"\\n{}() {}\\n\".format(cleanup_func, cleanup_code))\n\n        tmpfile.write(\"\\nclean_all() {{ {}; }}\\n\".format(\n            \"; \".join([x[0] for x in cleanup_funcs])))\n\n        tmpfile.write(\"\\ntrap clean_all EXIT\\n\\n\")\n\n        if self.job_memory not in (\"unlimited\", \"etc\") and \\\n           self.options.get(\"cluster_memory_ulimit\", False):\n            # restrict virtual memory\n            # Note that there are resources in SGE which could do this directly\n            # such as v_hmem.\n            # Note that limiting resident set sizes (RSS) with ulimit is not\n            # possible in newer kernels.\n            # -v and -m accept memory in kb\n            requested_memory_kb = max(\n                1000,\n                int(math.ceil(\n                    iotools.human2bytes(self.job_memory) / 1024 * self.job_threads)))\n            # unsetting error exit as often not permissions\n            tmpfile.write(\"set +e\\n\")\n            tmpfile.write(\"ulimit -v {} &gt; /dev/null \\n\".format(\n                requested_memory_kb))\n            tmpfile.write(\"ulimit -m {} &gt; /dev/null \\n\".format(\n                requested_memory_kb))\n            # set as hard limit\n            tmpfile.write(\"ulimit -H -v &gt; /dev/null \\n\")\n            tmpfile.write(\"set -e\\n\")\n\n        if self.shellfile:\n\n            # make sure path exists that we want to write to\n            tmpfile.write(\"mkdir -p $(dirname \\\"{}\\\")\\n\".format(\n                self.shellfile))\n\n            # output low-level debugging information to a shell log file\n            tmpfile.write(\n                'echo \"%s : START -&gt; %s\" &gt;&gt; %s\\n' %\n                (self.job_name, tmpfilename, self.shellfile))\n            # disabled - problems with quoting\n            # tmpfile.write( '''echo 'statement=%s' &gt;&gt; %s\\n''' %\n            # (shellquote(statement), self.shellfile) )\n            tmpfile.write(\"set | sed 's/^/%s : /' &gt;&gt; %s\\n\" %\n                          (self.job_name, self.shellfile))\n            tmpfile.write(\"pwd | sed 's/^/%s : /' &gt;&gt; %s\\n\" %\n                          (self.job_name, self.shellfile))\n            tmpfile.write(\"hostname | sed 's/^/%s: /' &gt;&gt; %s\\n\" %\n                          (self.job_name, self.shellfile))\n            # cat /proc/meminfo is Linux specific\n            if get_params()['os'] == 'Linux':\n                tmpfile.write(\"cat /proc/meminfo | sed 's/^/%s: /' &gt;&gt; %s\\n\" %\n                              (self.job_name, self.shellfile))\n            elif get_params()['os'] == 'Darwin':\n                tmpfile.write(\"vm_stat | sed 's/^/%s: /' &gt;&gt; %s\\n\" %\n                              (self.job_name, self.shellfile))\n            tmpfile.write(\n                'echo \"%s : END -&gt; %s\" &gt;&gt; %s\\n' %\n                (self.job_name, tmpfilename, self.shellfile))\n            tmpfile.write(\"ulimit | sed 's/^/%s: /' &gt;&gt; %s\\n\" %\n                          (self.job_name, self.shellfile))\n\n        job_path = os.path.abspath(tmpfilename)\n\n        tmpfile.write(expanded_statement)\n        tmpfile.write(\"\\n\\n\")\n        tmpfile.close()\n\n    return statement, job_path\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.Executor.cleanup_all_jobs","title":"<code>cleanup_all_jobs()</code>","text":"<p>Clean up all remaining active jobs on interruption.</p> Source code in <code>cgatcore/pipeline/execution.py</code> <pre><code>def cleanup_all_jobs(self):\n    \"\"\"Clean up all remaining active jobs on interruption.\"\"\"\n    self.logger.info(\"Cleaning up all job outputs due to pipeline interruption\")\n    for job_info in self.active_jobs:\n        self.cleanup_failed_job(job_info)\n    self.active_jobs.clear()  # Clear the list after cleanup\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.Executor.cleanup_failed_job","title":"<code>cleanup_failed_job(job_info)</code>","text":"<p>Clean up files generated by a failed job.</p> Source code in <code>cgatcore/pipeline/execution.py</code> <pre><code>def cleanup_failed_job(self, job_info):\n    \"\"\"Clean up files generated by a failed job.\"\"\"\n    if \"outfile\" in job_info:\n        outfiles = [job_info[\"outfile\"]]\n    elif \"outfiles\" in job_info:\n        outfiles = job_info[\"outfiles\"]\n    else:\n        self.logger.warning(f\"No output files found for job {job_info.get('job_name', 'unknown')}\")\n        return\n\n    for outfile in outfiles:\n        if os.path.exists(outfile):\n            try:\n                os.remove(outfile)\n                self.logger.info(f\"Removed failed job output file: {outfile}\")\n            except OSError as e:\n                self.logger.error(f\"Error removing file {outfile}: {str(e)}\")\n        else:\n            self.logger.info(f\"Output file not found (already removed or not created): {outfile}\")\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.Executor.collect_benchmark_data","title":"<code>collect_benchmark_data(statements, resource_usage)</code>","text":"<p>collect benchmark data from a job's stdout and any resource usage information that might be present.</p> <p>If time_data is given, read output from time command.</p> Source code in <code>cgatcore/pipeline/execution.py</code> <pre><code>def collect_benchmark_data(self,\n                           statements,\n                           resource_usage):\n    \"\"\"collect benchmark data from a job's stdout and any resource usage\n    information that might be present.\n\n    If time_data is given, read output from time command.\n    \"\"\"\n\n    benchmark_data = []\n\n    def get_val(d, v, alt):\n        val = d.get(v, alt)\n        if val == \"unknown\" or val is None:\n            val = alt\n        return val\n\n    # build resource usage data structure - part native, part\n    # mapped to common fields\n    for jobinfo, statement in zip(resource_usage, statements):\n\n        if resource_usage is None:\n            E.warn(\"no resource usage for {}\".format(self.task_name))\n            continue\n\n        # add some common fields\n        data = {\"task\": self.task_name,\n                \"engine\": self.__class__.__name__,\n                \"statement\": statement,\n                \"job_id\": jobinfo.jobId,\n                \"slots\": self.job_threads}\n\n        # native specs\n        data.update(jobinfo.resourceUsage)\n\n        # translate specs\n        if self.queue_manager:\n            data.update(\n                self.queue_manager.map_resource_usage(data, DATA2TYPE))\n\n        cpu_time = float(get_val(data, \"cpu_t\", 0))\n        start_time = float(get_val(data, \"start_time\", 0))\n        end_time = float(get_val(data, \"end_time\", 0))\n        data.update({\n            # avoid division by 0 error\n            \"percent_cpu\": (\n                100.0 * cpu_time / max(1.0, (end_time - start_time)) / self.job_threads),\n            \"total_t\": end_time - start_time\n        })\n        benchmark_data.append(data)\n\n    return benchmark_data\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.Executor.expand_statement","title":"<code>expand_statement(statement)</code>","text":"<p>add generic commands before and after statement.</p> <p>The method scans the statement for arvados mount points and inserts appropriate prefixes to make sure that the mount point exists.</p>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.Executor.expand_statement--arguments","title":"Arguments","text":"<p>statement : string     Command line statement to expand</p>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.Executor.expand_statement--returns","title":"Returns","text":"<p>statement : string     The expanded statement.</p> Source code in <code>cgatcore/pipeline/execution.py</code> <pre><code>def expand_statement(self, statement):\n    '''add generic commands before and after statement.\n\n    The method scans the statement for arvados mount points and\n    inserts appropriate prefixes to make sure that the mount point\n    exists.\n\n    Arguments\n    ---------\n    statement : string\n        Command line statement to expand\n\n    Returns\n    -------\n    statement : string\n        The expanded statement.\n\n    '''\n\n    setup_cmds = []\n    teardown_cmds = []\n    cleanup_funcs = []\n\n    setup_cmds.append(\"umask 002\")\n\n    for var in [\"MKL_NUM_THREADS\",\n                \"OPENBLAS_NUM_THREADS\",\n                \"OMP_NUM_THREADS\"]:\n        setup_cmds.append(\"export {}={}\".format(\n            var, self.options.get(\"job_threads\", 1)))\n\n    if \"arv=\" in statement:\n\n        # Todo: permit setting this in params\n        arvados_api_token = os.environ.get(\"ARVADOS_API_TOKEN\", None)\n        arvados_api_host = os.environ.get(\"ARVADOS_API_HOST\", None)\n        if not arvados_api_token:\n            raise ValueError(\n                \"arvados mount encountered in statement {}, \"\n                \"but ARVADOS_API_TOKEN not defined\".format(statement))\n\n        if not arvados_api_host:\n            raise ValueError(\n                \"arvados mount encountered in statement {}, \"\n                \"but ARVADOS_API_HOST not defined\".format(statement))\n\n        mountpoint = get_temp_filename(clear=True)\n\n        arvados_options = \"--disable-event-listening --read-only\"\n        setup_cmds.append(\"\\n\".join(\n            ('export ARVADOS_API_TOKEN=\"{arvados_api_token}\"',\n             'export ARVADOS_API_HOST=\"{arvados_api_host}\"',\n             'export ARVADOS_API_HOST_INSECURE=true',\n             'export ARVADOS_MOUNT_POINT=\"{mountpoint}\"',\n             'mkdir -p \"{mountpoint}\"',\n             'arv-mount {arvados_options} \"{mountpoint}\" 2&gt;&gt; /dev/null')).format(**locals()))\n\n        statement = re.sub(\"arv=\", mountpoint + \"/\", statement)\n\n        # \"arv-mount --unmount {mountpoint}\" not available in newer\n        # arvados installs (0.1.20170707152712), so keep using\n        # fusermount. However, do not fail if you can't clean up, as\n        # there are arvados racing issues.\n        cleanup_funcs.append((\"unmount_arvados\",\n                              '''{{\n                              set +e &amp;&amp;\n                              fusermount -u {mountpoint} &amp;&amp;\n                              rm -rf {mountpoint} &amp;&amp;\n                              set -e\n                              }}'''.format(**locals())))\n\n    if \"job_condaenv\" in self.options:\n        # In conda &lt; 4.4 there is an issue with parallel activations,\n        # see https://github.com/conda/conda/issues/2837 .\n        # This has been fixed in conda 4.4, but we are on conda\n        # 4.3, presumably because we are still on py35. A work-around\n        # to source activate is to add the explicit path of the environment\n        # in version &gt;= 4.4, do\n        # setup_cmds.append(\n        #     \"conda activate {}\".format(self.options[\"job_condaenv\"]))\n        # For old conda versions (note this will not work for tools that require\n        # additional environment variables)\n        setup_cmds.append(\n            \"export PATH={}:$PATH\".format(\n                os.path.join(\n                    get_conda_environment_directory(\n                        self.options[\"job_condaenv\"]),\n                    \"bin\")))\n\n    statement = \"\\n\".join((\n        \"\\n\".join(setup_cmds),\n        statement,\n        \"\\n\".join(teardown_cmds)))\n\n    return statement, cleanup_funcs\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.Executor.finish_job","title":"<code>finish_job(job_info)</code>","text":"<p>Remove a job from active_jobs list when it finishes.</p> Source code in <code>cgatcore/pipeline/execution.py</code> <pre><code>def finish_job(self, job_info):\n    \"\"\"Remove a job from active_jobs list when it finishes.\"\"\"\n    if job_info in self.active_jobs:\n        self.active_jobs.remove(job_info)\n        self.logger.info(f\"Job completed: {job_info}\")\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.Executor.run","title":"<code>run(statement_list, job_memory=None, job_threads=None, container_runtime=None, image=None, volumes=None, env_vars=None, **kwargs)</code>","text":"<p>Execute a list of statements with optional container support.</p> <pre><code>Args:\n    statement_list (list): List of commands to execute.\n    job_memory (str): Memory requirements (e.g., \"4G\").\n    job_threads (int): Number of threads to use.\n    container_runtime (str): Container runtime (\"docker\" or \"singularity\").\n    image (str): Container image to use.\n    volumes (list): Volume mappings (e.g., ['/data:/data']).\n    env_vars (dict): Environment variables for the container.\n    **kwargs: Additional arguments.\n</code></pre> Source code in <code>cgatcore/pipeline/execution.py</code> <pre><code>def run(\n        self,\n        statement_list,\n        job_memory=None,\n        job_threads=None,\n        container_runtime=None,\n        image=None,\n        volumes=None,\n        env_vars=None,\n        **kwargs,):\n\n    \"\"\"\n    Execute a list of statements with optional container support.\n\n        Args:\n            statement_list (list): List of commands to execute.\n            job_memory (str): Memory requirements (e.g., \"4G\").\n            job_threads (int): Number of threads to use.\n            container_runtime (str): Container runtime (\"docker\" or \"singularity\").\n            image (str): Container image to use.\n            volumes (list): Volume mappings (e.g., ['/data:/data']).\n            env_vars (dict): Environment variables for the container.\n            **kwargs: Additional arguments.\n    \"\"\"\n    # Validation checks\n    if container_runtime and container_runtime not in [\"docker\", \"singularity\"]:\n        self.logger.error(f\"Invalid container_runtime: {container_runtime}\")\n        raise ValueError(\"Container runtime must be 'docker' or 'singularity'\")\n\n    if container_runtime and not image:\n        self.logger.error(f\"Container runtime specified without an image: {container_runtime}\")\n        raise ValueError(\"An image must be specified when using a container runtime\")\n\n    benchmark_data = []\n\n    for statement in statement_list:\n        job_info = {\"statement\": statement}\n        self.start_job(job_info)\n\n        try:\n            # Prepare containerized execution\n            if container_runtime:\n                self.set_container_config(image=image, volumes=volumes, env_vars=env_vars, runtime=container_runtime)\n                statement = self.container_config.get_container_command(statement)\n\n            # Add memory and thread environment variables\n            if job_memory:\n                env_vars = env_vars or {}\n                env_vars[\"JOB_MEMORY\"] = job_memory\n            if job_threads:\n                env_vars = env_vars or {}\n                env_vars[\"JOB_THREADS\"] = job_threads\n\n            # Debugging: Log the constructed command\n            self.logger.info(f\"Executing command: {statement}\")\n\n            # Build and execute the statement\n            full_statement, job_path = self.build_job_script(statement)\n            process = subprocess.Popen(\n                full_statement, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE\n            )\n            stdout, stderr = process.communicate()\n\n            if process.returncode != 0:\n                raise OSError(\n                    f\"Job failed with return code {process.returncode}.\\n\"\n                    f\"stderr: {stderr.decode('utf-8')}\\ncommand: {statement}\"\n                )\n\n            # Collect benchmark data for successful jobs\n            benchmark_data.append(\n                self.collect_benchmark_data(\n                    statement, resource_usage={\"job_id\": process.pid}\n                )\n            )\n            self.finish_job(job_info)\n\n        except Exception as e:\n            self.logger.error(f\"Job failed: {e}\")\n            self.cleanup_failed_job(job_info)\n            if not self.ignore_errors:\n                raise\n\n    return benchmark_data\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.Executor.set_container_config","title":"<code>set_container_config(image, volumes=None, env_vars=None, runtime='docker')</code>","text":"<p>Set container configuration for all tasks executed by this executor.</p> Source code in <code>cgatcore/pipeline/execution.py</code> <pre><code>def set_container_config(self, image, volumes=None, env_vars=None, runtime=\"docker\"):\n    \"\"\"Set container configuration for all tasks executed by this executor.\"\"\"\n\n    if not image:\n        raise ValueError(\"An image must be specified for the container configuration.\")\n    self.container_config = ContainerConfig(image=image, volumes=volumes, env_vars=env_vars, runtime=runtime)\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.Executor.setup_signal_handlers","title":"<code>setup_signal_handlers()</code>","text":"<p>Set up signal handlers to clean up jobs on SIGINT and SIGTERM.</p> Source code in <code>cgatcore/pipeline/execution.py</code> <pre><code>def setup_signal_handlers(self):\n    \"\"\"Set up signal handlers to clean up jobs on SIGINT and SIGTERM.\"\"\"\n\n    def signal_handler(signum, frame):\n        self.logger.info(f\"Received signal {signum}. Starting clean-up.\")\n        self.cleanup_all_jobs()\n        exit(1)\n\n    signal.signal(signal.SIGINT, signal_handler)\n    signal.signal(signal.SIGTERM, signal_handler)\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.Executor.start_job","title":"<code>start_job(job_info)</code>","text":"<p>Add a job to active_jobs list when it starts.</p> Source code in <code>cgatcore/pipeline/execution.py</code> <pre><code>def start_job(self, job_info):\n    \"\"\"Add a job to active_jobs list when it starts.\"\"\"\n    self.active_jobs.append(job_info)\n    self.logger.info(f\"Job started: {job_info}\")\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.KubernetesExecutor","title":"<code>KubernetesExecutor</code>","text":"<p>               Bases: <code>BaseExecutor</code></p> <p>Executor for managing and running jobs on a Kubernetes cluster.</p> <p>This class is responsible for submitting jobs to a Kubernetes cluster, monitoring their execution, and collecting benchmark data related to their performance.</p> <p>Attributes:</p> Name Type Description <code>namespace</code> <code>str</code> <p>The Kubernetes namespace in which to run the jobs. Defaults to 'default'.</p> <code>api</code> <code>CoreV1Api</code> <p>The Kubernetes Core API client for interacting with the cluster.</p> <code>batch_api</code> <code>BatchV1Api</code> <p>The Kubernetes Batch API client for managing jobs.</p> Source code in <code>cgatcore/pipeline/kubernetes.py</code> <pre><code>class KubernetesExecutor(BaseExecutor):\n    \"\"\"Executor for managing and running jobs on a Kubernetes cluster.\n\n    This class is responsible for submitting jobs to a Kubernetes cluster, monitoring their execution,\n    and collecting benchmark data related to their performance.\n\n    Attributes:\n        namespace (str): The Kubernetes namespace in which to run the jobs. Defaults to 'default'.\n        api (CoreV1Api): The Kubernetes Core API client for interacting with the cluster.\n        batch_api (BatchV1Api): The Kubernetes Batch API client for managing jobs.\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Initializes the KubernetesExecutor with the specified configuration options.\n\n        Args:\n            **kwargs: Additional configuration options, including the namespace.\n        \"\"\"\n        super().__init__(**kwargs)\n        self.namespace = kwargs.get(\"namespace\", \"default\")\n\n        # Load Kubernetes configuration\n        try:\n            config.load_kube_config()\n            self.api = client.CoreV1Api()\n            self.batch_api = client.BatchV1Api()\n            logger.info(\"Kubernetes configuration loaded successfully.\")\n        except exceptions.ConfigException as e:\n            logger.error(\"Failed to load Kubernetes configuration\", exc_info=True)\n            raise e\n\n    def run(self, statement, job_path, job_condaenv):\n        \"\"\"Submits a job to the Kubernetes cluster to run the specified command.\n\n        This method creates a Kubernetes Job object and submits it to the cluster. The job runs the\n        specified command in a container, using the provided Conda environment.\n\n        Args:\n            statement (str): The command to execute in the job.\n            job_path (str): The path to the job script.\n            job_condaenv (str): The name of the Conda environment to use.\n        \"\"\"\n        job_name = f\"cgat-{os.path.basename(job_path)}-{int(time.time())}\"\n        container_image = \"your-docker-image:tag\"  # Replace with your Docker image\n\n        # Define Kubernetes Job spec\n        job_spec = client.V1Job(\n            metadata=client.V1ObjectMeta(name=job_name),\n            spec=client.V1JobSpec(\n                template=client.V1PodTemplateSpec(\n                    spec=client.V1PodSpec(\n                        containers=[\n                            client.V1Container(\n                                name=\"cgat-job\",\n                                image=container_image,\n                                command=[\"/bin/bash\", \"-c\", statement],\n                                env=[client.V1EnvVar(name=\"CONDA_ENV\", value=job_condaenv)],\n                            )\n                        ],\n                        restart_policy=\"Never\"\n                    )\n                ),\n                backoff_limit=4  # Retry policy in case of transient failures\n            )\n        )\n\n        # Create and monitor Kubernetes Job\n        try:\n            logger.info(f\"Creating Kubernetes Job '{job_name}' in namespace '{self.namespace}'.\")\n            start_time = datetime.now()\n            self.batch_api.create_namespaced_job(self.namespace, job_spec)\n            self._wait_for_job_completion(job_name)\n            end_time = datetime.now()\n            logs = self._get_pod_logs(job_name)\n            self.collect_metric_data(\"Kubernetes Job\", start_time, end_time, \"time_data.json\")\n        finally:\n            self._cleanup_job(job_name)\n\n        return logs\n\n    def _wait_for_job_completion(self, job_name):\n        \"\"\"Wait until the job completes or fails.\"\"\"\n        while True:\n            job_status = self.batch_api.read_namespaced_job_status(job_name, self.namespace).status\n            if job_status.succeeded:\n                logger.info(f\"Job '{job_name}' completed successfully.\")\n                return\n            if job_status.failed:\n                logger.error(f\"Job '{job_name}' failed.\")\n                raise RuntimeError(f\"Kubernetes Job {job_name} failed.\")\n            time.sleep(5)\n\n    def _get_pod_logs(self, job_name):\n        \"\"\"Retrieve logs from the Job's pod.\"\"\"\n        pods = self.api.list_namespaced_pod(self.namespace, label_selector=f\"job-name={job_name}\").items\n        if not pods:\n            logger.error(f\"No pod found for job '{job_name}'.\")\n            raise RuntimeError(f\"No pod found for job '{job_name}'.\")\n\n        pod_name = pods[0].metadata.name\n        logger.info(f\"Fetching logs from pod '{pod_name}'.\")\n        return self.api.read_namespaced_pod_log(pod_name, self.namespace)\n\n    def _cleanup_job(self, job_name):\n        \"\"\"Delete the Job and its pods.\"\"\"\n        try:\n            self.batch_api.delete_namespaced_job(job_name, self.namespace, propagation_policy=\"Background\")\n            logger.info(f\"Job '{job_name}' cleaned up successfully.\")\n        except exceptions.ApiException as e:\n            logger.warning(f\"Failed to delete Job '{job_name}'\", exc_info=True)\n\n    def collect_benchmark_data(self, statements, resource_usage=None):\n        \"\"\"Collect benchmark data for Kubernetes jobs.\n\n        This method gathers information about the executed statements and any resource usage data.\n\n        Args:\n            statements (list): List of executed statements.\n            resource_usage (list, optional): Resource usage data.\n\n        Returns:\n            dict: A dictionary containing the task name, total execution time, executed statements,\n                  and resource usage data.\n        \"\"\"\n        return {\n            \"task\": \"kubernetes_task\",\n            \"total_t\": 12,  # Example value, adjust as needed\n            \"statements\": statements,\n            \"resource_usage\": resource_usage or []\n        }\n\n    def collect_metric_data(self, process, start_time, end_time, time_data_file):\n        \"\"\"\n        Collects metric data related to job duration and writes it to a file.\n\n        Parameters:\n        - process (str): Process name for tracking purposes.\n        - start_time (datetime): Timestamp when the job started.\n        - end_time (datetime): Timestamp when the job ended.\n        - time_data_file (str): Path to a file where timing data will be saved.\n        \"\"\"\n        duration = (end_time - start_time).total_seconds()\n        metric_data = {\n            \"process\": process,\n            \"start_time\": start_time.isoformat(),\n            \"end_time\": end_time.isoformat(),\n            \"duration_seconds\": duration\n        }\n\n        # Log metric data\n        logger.info(\n            f\"Metric data collected for process '{process}': start time = {start_time}, end time = {end_time}, \"\n            f\"duration = {duration} seconds.\"\n        )\n\n        # Write metric data to file\n        try:\n            with open(time_data_file, \"w\") as f:\n                json.dump(metric_data, f, indent=4)\n            logger.info(f\"Metric data saved to {time_data_file}\")\n\n        except Exception as e:\n            logger.error(\"Error writing metric data to file\", exc_info=True)\n            raise e\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.KubernetesExecutor.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initializes the KubernetesExecutor with the specified configuration options.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional configuration options, including the namespace.</p> <code>{}</code> Source code in <code>cgatcore/pipeline/kubernetes.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initializes the KubernetesExecutor with the specified configuration options.\n\n    Args:\n        **kwargs: Additional configuration options, including the namespace.\n    \"\"\"\n    super().__init__(**kwargs)\n    self.namespace = kwargs.get(\"namespace\", \"default\")\n\n    # Load Kubernetes configuration\n    try:\n        config.load_kube_config()\n        self.api = client.CoreV1Api()\n        self.batch_api = client.BatchV1Api()\n        logger.info(\"Kubernetes configuration loaded successfully.\")\n    except exceptions.ConfigException as e:\n        logger.error(\"Failed to load Kubernetes configuration\", exc_info=True)\n        raise e\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.KubernetesExecutor.collect_benchmark_data","title":"<code>collect_benchmark_data(statements, resource_usage=None)</code>","text":"<p>Collect benchmark data for Kubernetes jobs.</p> <p>This method gathers information about the executed statements and any resource usage data.</p> <p>Parameters:</p> Name Type Description Default <code>statements</code> <code>list</code> <p>List of executed statements.</p> required <code>resource_usage</code> <code>list</code> <p>Resource usage data.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the task name, total execution time, executed statements,   and resource usage data.</p> Source code in <code>cgatcore/pipeline/kubernetes.py</code> <pre><code>def collect_benchmark_data(self, statements, resource_usage=None):\n    \"\"\"Collect benchmark data for Kubernetes jobs.\n\n    This method gathers information about the executed statements and any resource usage data.\n\n    Args:\n        statements (list): List of executed statements.\n        resource_usage (list, optional): Resource usage data.\n\n    Returns:\n        dict: A dictionary containing the task name, total execution time, executed statements,\n              and resource usage data.\n    \"\"\"\n    return {\n        \"task\": \"kubernetes_task\",\n        \"total_t\": 12,  # Example value, adjust as needed\n        \"statements\": statements,\n        \"resource_usage\": resource_usage or []\n    }\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.KubernetesExecutor.collect_metric_data","title":"<code>collect_metric_data(process, start_time, end_time, time_data_file)</code>","text":"<p>Collects metric data related to job duration and writes it to a file.</p> <p>Parameters: - process (str): Process name for tracking purposes. - start_time (datetime): Timestamp when the job started. - end_time (datetime): Timestamp when the job ended. - time_data_file (str): Path to a file where timing data will be saved.</p> Source code in <code>cgatcore/pipeline/kubernetes.py</code> <pre><code>def collect_metric_data(self, process, start_time, end_time, time_data_file):\n    \"\"\"\n    Collects metric data related to job duration and writes it to a file.\n\n    Parameters:\n    - process (str): Process name for tracking purposes.\n    - start_time (datetime): Timestamp when the job started.\n    - end_time (datetime): Timestamp when the job ended.\n    - time_data_file (str): Path to a file where timing data will be saved.\n    \"\"\"\n    duration = (end_time - start_time).total_seconds()\n    metric_data = {\n        \"process\": process,\n        \"start_time\": start_time.isoformat(),\n        \"end_time\": end_time.isoformat(),\n        \"duration_seconds\": duration\n    }\n\n    # Log metric data\n    logger.info(\n        f\"Metric data collected for process '{process}': start time = {start_time}, end time = {end_time}, \"\n        f\"duration = {duration} seconds.\"\n    )\n\n    # Write metric data to file\n    try:\n        with open(time_data_file, \"w\") as f:\n            json.dump(metric_data, f, indent=4)\n        logger.info(f\"Metric data saved to {time_data_file}\")\n\n    except Exception as e:\n        logger.error(\"Error writing metric data to file\", exc_info=True)\n        raise e\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.KubernetesExecutor.run","title":"<code>run(statement, job_path, job_condaenv)</code>","text":"<p>Submits a job to the Kubernetes cluster to run the specified command.</p> <p>This method creates a Kubernetes Job object and submits it to the cluster. The job runs the specified command in a container, using the provided Conda environment.</p> <p>Parameters:</p> Name Type Description Default <code>statement</code> <code>str</code> <p>The command to execute in the job.</p> required <code>job_path</code> <code>str</code> <p>The path to the job script.</p> required <code>job_condaenv</code> <code>str</code> <p>The name of the Conda environment to use.</p> required Source code in <code>cgatcore/pipeline/kubernetes.py</code> <pre><code>def run(self, statement, job_path, job_condaenv):\n    \"\"\"Submits a job to the Kubernetes cluster to run the specified command.\n\n    This method creates a Kubernetes Job object and submits it to the cluster. The job runs the\n    specified command in a container, using the provided Conda environment.\n\n    Args:\n        statement (str): The command to execute in the job.\n        job_path (str): The path to the job script.\n        job_condaenv (str): The name of the Conda environment to use.\n    \"\"\"\n    job_name = f\"cgat-{os.path.basename(job_path)}-{int(time.time())}\"\n    container_image = \"your-docker-image:tag\"  # Replace with your Docker image\n\n    # Define Kubernetes Job spec\n    job_spec = client.V1Job(\n        metadata=client.V1ObjectMeta(name=job_name),\n        spec=client.V1JobSpec(\n            template=client.V1PodTemplateSpec(\n                spec=client.V1PodSpec(\n                    containers=[\n                        client.V1Container(\n                            name=\"cgat-job\",\n                            image=container_image,\n                            command=[\"/bin/bash\", \"-c\", statement],\n                            env=[client.V1EnvVar(name=\"CONDA_ENV\", value=job_condaenv)],\n                        )\n                    ],\n                    restart_policy=\"Never\"\n                )\n            ),\n            backoff_limit=4  # Retry policy in case of transient failures\n        )\n    )\n\n    # Create and monitor Kubernetes Job\n    try:\n        logger.info(f\"Creating Kubernetes Job '{job_name}' in namespace '{self.namespace}'.\")\n        start_time = datetime.now()\n        self.batch_api.create_namespaced_job(self.namespace, job_spec)\n        self._wait_for_job_completion(job_name)\n        end_time = datetime.now()\n        logs = self._get_pod_logs(job_name)\n        self.collect_metric_data(\"Kubernetes Job\", start_time, end_time, \"time_data.json\")\n    finally:\n        self._cleanup_job(job_name)\n\n    return logs\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.LoggingFilterProgress","title":"<code>LoggingFilterProgress</code>","text":"<p>               Bases: <code>Filter</code></p> <p>add progress information to the log-stream.</p> <p>A :term:<code>task</code> is a ruffus_ decorated function, which will execute one or more :term:<code>jobs</code>.</p> <p>Valid task/job status: update    task/job needs updating completed    task/job completed successfully failed    task/job failed running    task/job is running ignore    ignore task/job (is up-to-date)</p> <p>This filter adds the following context to a log record:</p> <p>task    task_name</p> <p>task_status    task status</p> <p>task_total    number of jobs in task</p> <p>task_completed    number of jobs in task completed</p> <p>task_completed_percent    percentage of task completed</p> <p>The filter will also generate an additional log message in json format with the fields above.</p>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.LoggingFilterProgress--arguments","title":"Arguments","text":"<p>ruffus_text : string     Log messages from ruffus.pipeline_printout. These are used     to collect all tasks that will be executed during pipeline     execution.</p> Source code in <code>cgatcore/pipeline/control.py</code> <pre><code>class LoggingFilterProgress(logging.Filter):\n    \"\"\"add progress information to the log-stream.\n\n    A :term:`task` is a ruffus_ decorated function, which will execute\n    one or more :term:`jobs`.\n\n    Valid task/job status:\n    update\n       task/job needs updating\n    completed\n       task/job completed successfully\n    failed\n       task/job failed\n    running\n       task/job is running\n    ignore\n       ignore task/job (is up-to-date)\n\n    This filter adds the following context to a log record:\n\n    task\n       task_name\n\n    task_status\n       task status\n\n    task_total\n       number of jobs in task\n\n    task_completed\n       number of jobs in task completed\n\n    task_completed_percent\n       percentage of task completed\n\n    The filter will also generate an additional log message in json format\n    with the fields above.\n\n    Arguments\n    ---------\n    ruffus_text : string\n        Log messages from ruffus.pipeline_printout. These are used\n        to collect all tasks that will be executed during pipeline\n        execution.\n\n    \"\"\"\n\n    def __init__(self,\n                 ruffus_text):\n\n        # dictionary of jobs to run\n        self.jobs = {}\n        self.tasks = {}\n        self.map_job2task = {}\n        self.logger = get_logger()\n\n        def split_by_job(text):\n            # ignore optional docstring at beginning (is bracketed by '\"')\n            text = re.sub(r'^\\\"[^\"]+\\\"', \"\", \"\".join(text))\n            for line in re.split(r\"Job\\s+=\", text):\n                if not line.strip():\n                    continue\n                if \"Make missing directories\" in line:\n                    continue\n                try:\n                    # long file names cause additional wrapping and\n                    # additional white-space characters\n                    job_name = re.search(\n                        r\"\\[.*-&gt; ([^\\]]+)\\]\", line).groups()[0]\n                except AttributeError:\n                    continue\n                    # raise AttributeError(\"could not parse '%s'\" % line)\n                job_status = \"ignore\"\n                if \"Job needs update\" in line:\n                    job_status = \"update\"\n\n                yield job_name, job_status\n\n        def split_by_task(text):\n            block, task_name = [], None\n            task_status = None\n            for line in text.splitlines():\n                line = line.strip()\n\n                if line.startswith(\"Tasks which will be run\"):\n                    task_status = \"update\"\n                    block = []\n                    continue\n                elif line.startswith(\"Tasks which are up-to-date\"):\n                    task_status = \"ignore\"\n                    block = []\n                    continue\n\n                if line.startswith(\"Task = \"):\n                    if task_name:\n                        yield task_name, task_status, list(split_by_job(block))\n                    block = []\n                    task_name = re.match(\"Task = (.*)\", line).groups()[0]\n                    continue\n                if line:\n                    block.append(line)\n            if task_name:\n                yield task_name, task_status, list(split_by_job(block))\n\n        # populate with initial messages\n        for task_name, task_status, jobs in split_by_task(ruffus_text):\n            if task_name.startswith(\"(mkdir\"):\n                continue\n\n            to_run = 0\n            for job_name, job_status in jobs:\n                self.jobs[job_name] = (task_name, job_name)\n                if job_status == \"update\":\n                    to_run += 1\n                self.map_job2task[re.sub(r\"\\s\", \"\", job_name)] = task_name\n\n            self.tasks[task_name] = [task_status,\n                                     len(jobs),\n                                     len(jobs) - to_run]\n\n    def filter(self, record):\n\n        if not record.filename.endswith(\"task.py\"):\n            return True\n\n        # update task counts and status\n        job_name, task_name = None, None\n        if re.search(r\"Job\\s+=\", record.msg):\n            try:\n                job_name = re.search(\n                    r\"\\[.*-&gt; ([^\\]]+)\\]\", record.msg).groups()[0]\n            except AttributeError:\n                return True\n            job_name = re.sub(r\"\\s\", \"\", job_name)\n            task_name = self.map_job2task.get(job_name, None)\n            if task_name is None:\n                return\n            if \"completed\" in record.msg:\n                self.tasks[task_name][2] += 1\n\n        elif re.search(r\"Task\\s+=\", record.msg):\n            try:\n                before, task_name = record.msg.strip().split(\" = \")\n            except ValueError:\n                return True\n\n            # ignore the mkdir, etc tasks\n            if task_name not in self.tasks:\n                return True\n\n            if before == \"Task enters queue\":\n                self.tasks[task_name][0] = \"running\"\n            elif before == \"Completed Task\":\n                self.tasks[task_name][0] = \"completed\"\n            elif before == \"Uptodate Task\":\n                self.tasks[task_name][0] = \"uptodate\"\n            else:\n                return True\n        else:\n            return True\n\n        if task_name is None:\n            return\n\n        # update log record\n        task_status, task_total, task_completed = self.tasks[task_name]\n        if task_total &gt; 0:\n            task_completed_percent = 100.0 * task_completed / task_total\n        else:\n            task_completed_percent = 0\n\n        # ignore prefix:: in task_name for output\n        task_name = re.sub(\"^[^:]+::\", \"\", task_name)\n        data = {\n            \"task\": task_name,\n            \"task_status\": task_status,\n            \"task_total\": task_total,\n            \"task_completed\": task_completed,\n            \"task_completed_percent\": task_completed_percent}\n\n        record.task_status = task_status\n        record.task_total = task_total\n        record.task_completed = task_completed\n        record.task_completed_percent = task_completed_percent\n\n        # log status\n        self.logger.info(json.dumps(data))\n\n        return True\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.LoggingFilterpipelineName","title":"<code>LoggingFilterpipelineName</code>","text":"<p>               Bases: <code>Filter</code></p> <p>add pipeline name to log message.</p> <p>With this filter, %(app_name)s can be used in log formats.</p> Source code in <code>cgatcore/pipeline/control.py</code> <pre><code>class LoggingFilterpipelineName(logging.Filter):\n    \"\"\"add pipeline name to log message.\n\n    With this filter, %(app_name)s can be used in log formats.\n    \"\"\"\n\n    def __init__(self, name, *args, **kwargs):\n        logging.Filter.__init__(self, *args, **kwargs)\n        self.app_name = name\n\n    def filter(self, record):\n        record.app_name = self.app_name\n        message = record.getMessage()\n        if message.startswith(\"- {\"):\n            json_message = json.loads(message[2:])\n        elif message.startswith(\"{\"):\n            json_message = json.loads(message)\n        else:\n            json_message = None\n        if json_message:\n            for k, v in list(json_message.items()):\n                setattr(record, k, v)\n\n        return True\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.MultiLineFormatter","title":"<code>MultiLineFormatter</code>","text":"<p>               Bases: <code>Formatter</code></p> <p>logfile formatter: add identation for multi-line entries.</p> Source code in <code>cgatcore/experiment.py</code> <pre><code>class MultiLineFormatter(logging.Formatter):\n    '''logfile formatter: add identation for multi-line entries.'''\n\n    def format(self, record):\n\n        s = logging.Formatter.format(self, record)\n        if s.startswith(\"#\"):\n            prefix = \"#\"\n        else:\n            prefix = \"\"\n        if record.message:\n            header, footer = s.split(record.message)\n            s = s.replace(\"\\n\", \" \\\\\\n%s\" % prefix + \" \" * (len(header) - 1))\n        return s\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.SGEExecutor","title":"<code>SGEExecutor</code>","text":"<p>               Bases: <code>BaseExecutor</code></p> Source code in <code>cgatcore/pipeline/executors.py</code> <pre><code>class SGEExecutor(BaseExecutor):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.logger = logging.getLogger(__name__)\n        self.task_name = \"sge_task\"\n        self.default_total_time = 8\n\n    def run(self, statement_list):\n        benchmark_data = []\n        for statement in statement_list:\n            self.logger.info(f\"Running statement on SGE: {statement}\")\n\n            full_statement, job_path = self.build_job_script(statement)\n\n            # Build the SGE job submission command\n            sge_command = f\"qsub -N {self.config.get('job_name', 'default_job')} -cwd -o {job_path}.o -e {job_path}.e {job_path}\"\n\n            process = subprocess.run(sge_command, shell=True, capture_output=True, text=True)\n\n            if process.returncode != 0:\n                self.logger.error(f\"SGE job submission failed: {process.stderr}\")\n                raise RuntimeError(f\"SGE job submission failed: {process.stderr}\")\n\n            self.logger.info(f\"SGE job submitted: {process.stdout.strip()}\")\n\n            # Monitor job completion\n            self.monitor_job_completion(process.stdout.strip())\n\n            benchmark_data.append(self.collect_benchmark_data([statement], resource_usage=[]))\n\n        return benchmark_data\n\n    def build_job_script(self, statement):\n        \"\"\"Custom build job script for SGE.\"\"\"\n        return super().build_job_script(statement)\n\n    def monitor_job_completion(self, job_id):\n        \"\"\"Monitor the completion of an SGE job.\n\n        Args:\n            job_id (str): The SGE job ID to monitor.\n\n        Raises:\n            RuntimeError: If the job fails or times out.\n        \"\"\"\n        while True:\n            # Use qstat to get job status\n            cmd = f\"qstat -j {job_id}\"\n            process = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n\n            if process.returncode != 0:\n                # Job not found in qstat could mean it's completed\n                # Use qacct to get final status\n                cmd = f\"qacct -j {job_id}\"\n                process = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n\n                if \"exit_status\" in process.stdout:\n                    exit_status = process.stdout.split(\"exit_status\")[1].split()[0]\n                    if exit_status == \"0\":\n                        self.logger.info(f\"Job {job_id} completed successfully\")\n                        break\n                    else:\n                        self.logger.error(f\"Job {job_id} failed with exit status: {exit_status}\")\n                        raise RuntimeError(f\"Job {job_id} failed with exit status: {exit_status}\")\n\n                self.logger.error(f\"Failed to get job status: {process.stderr}\")\n                raise RuntimeError(f\"Failed to get job status: {process.stderr}\")\n\n            # Wait before checking again\n            time.sleep(10)\n\n    def collect_benchmark_data(self, statements, resource_usage=None):\n        \"\"\"Collect benchmark data for SGE jobs.\n\n        Args:\n            statements (list): List of executed statements\n            resource_usage (list, optional): Resource usage data\n\n        Returns:\n            dict: Benchmark data including task name and execution time\n        \"\"\"\n        return {\n            \"task\": self.task_name,\n            \"total_t\": self.default_total_time,\n            \"statements\": statements,\n            \"resource_usage\": resource_usage or []\n        }\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.SGEExecutor.build_job_script","title":"<code>build_job_script(statement)</code>","text":"<p>Custom build job script for SGE.</p> Source code in <code>cgatcore/pipeline/executors.py</code> <pre><code>def build_job_script(self, statement):\n    \"\"\"Custom build job script for SGE.\"\"\"\n    return super().build_job_script(statement)\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.SGEExecutor.collect_benchmark_data","title":"<code>collect_benchmark_data(statements, resource_usage=None)</code>","text":"<p>Collect benchmark data for SGE jobs.</p> <p>Parameters:</p> Name Type Description Default <code>statements</code> <code>list</code> <p>List of executed statements</p> required <code>resource_usage</code> <code>list</code> <p>Resource usage data</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>Benchmark data including task name and execution time</p> Source code in <code>cgatcore/pipeline/executors.py</code> <pre><code>def collect_benchmark_data(self, statements, resource_usage=None):\n    \"\"\"Collect benchmark data for SGE jobs.\n\n    Args:\n        statements (list): List of executed statements\n        resource_usage (list, optional): Resource usage data\n\n    Returns:\n        dict: Benchmark data including task name and execution time\n    \"\"\"\n    return {\n        \"task\": self.task_name,\n        \"total_t\": self.default_total_time,\n        \"statements\": statements,\n        \"resource_usage\": resource_usage or []\n    }\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.SGEExecutor.monitor_job_completion","title":"<code>monitor_job_completion(job_id)</code>","text":"<p>Monitor the completion of an SGE job.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>str</code> <p>The SGE job ID to monitor.</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the job fails or times out.</p> Source code in <code>cgatcore/pipeline/executors.py</code> <pre><code>def monitor_job_completion(self, job_id):\n    \"\"\"Monitor the completion of an SGE job.\n\n    Args:\n        job_id (str): The SGE job ID to monitor.\n\n    Raises:\n        RuntimeError: If the job fails or times out.\n    \"\"\"\n    while True:\n        # Use qstat to get job status\n        cmd = f\"qstat -j {job_id}\"\n        process = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n\n        if process.returncode != 0:\n            # Job not found in qstat could mean it's completed\n            # Use qacct to get final status\n            cmd = f\"qacct -j {job_id}\"\n            process = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n\n            if \"exit_status\" in process.stdout:\n                exit_status = process.stdout.split(\"exit_status\")[1].split()[0]\n                if exit_status == \"0\":\n                    self.logger.info(f\"Job {job_id} completed successfully\")\n                    break\n                else:\n                    self.logger.error(f\"Job {job_id} failed with exit status: {exit_status}\")\n                    raise RuntimeError(f\"Job {job_id} failed with exit status: {exit_status}\")\n\n            self.logger.error(f\"Failed to get job status: {process.stderr}\")\n            raise RuntimeError(f\"Failed to get job status: {process.stderr}\")\n\n        # Wait before checking again\n        time.sleep(10)\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.SlurmExecutor","title":"<code>SlurmExecutor</code>","text":"<p>               Bases: <code>BaseExecutor</code></p> <p>Executor for running jobs on Slurm cluster.</p> Source code in <code>cgatcore/pipeline/executors.py</code> <pre><code>class SlurmExecutor(BaseExecutor):\n    \"\"\"Executor for running jobs on Slurm cluster.\"\"\"\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.logger = logging.getLogger(__name__)\n        self.task_name = \"slurm_task\"\n        self.default_total_time = 10\n\n    def run(self, statement_list):\n        benchmark_data = []\n        for statement in statement_list:\n            self.logger.info(f\"Running statement on Slurm: {statement}\")\n\n            full_statement, job_path = self.build_job_script(statement)\n\n            # Build the Slurm job submission command\n            slurm_command = f\"sbatch --job-name={self.config.get('job_name', 'default_job')} --output={job_path}.o --error={job_path}.e {job_path}\"\n\n            process = subprocess.run(slurm_command, shell=True, capture_output=True, text=True)\n\n            if process.returncode != 0:\n                self.logger.error(f\"Slurm job submission failed: {process.stderr}\")\n                raise RuntimeError(f\"Slurm job submission failed: {process.stderr}\")\n\n            job_id = process.stdout.strip()\n            self.logger.info(f\"Slurm job submitted with ID: {job_id}\")\n\n            # Monitor job completion\n            self.monitor_job_completion(job_id)\n\n            benchmark_data.append(self.collect_benchmark_data([statement], resource_usage=[]))\n\n        return benchmark_data\n\n    def build_job_script(self, statement):\n        \"\"\"Custom build job script for Slurm.\"\"\"\n        return super().build_job_script(statement)\n\n    def monitor_job_completion(self, job_id):\n        \"\"\"Monitor the completion of a Slurm job.\n\n        Args:\n            job_id (str): The Slurm job ID to monitor.\n\n        Raises:\n            RuntimeError: If the job fails or times out.\n        \"\"\"\n        while True:\n            # Use sacct to get job status\n            cmd = f\"sacct -j {job_id} --format=State --noheader --parsable2\"\n            process = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n\n            if process.returncode != 0:\n                self.logger.error(f\"Failed to get job status: {process.stderr}\")\n                raise RuntimeError(f\"Failed to get job status: {process.stderr}\")\n\n            status = process.stdout.strip()\n\n            # Check job status\n            if status in [\"COMPLETED\", \"COMPLETED+\"]:\n                self.logger.info(f\"Job {job_id} completed successfully\")\n                break\n            elif status in [\"FAILED\", \"TIMEOUT\", \"CANCELLED\", \"NODE_FAIL\"]:\n                self.logger.error(f\"Job {job_id} failed with status: {status}\")\n                raise RuntimeError(f\"Job {job_id} failed with status: {status}\")\n\n            # Wait before checking again\n            time.sleep(10)\n\n    def collect_benchmark_data(self, statements, resource_usage=None):\n        \"\"\"Collect benchmark data for Slurm jobs.\n\n        Args:\n            statements (list): List of executed statements\n            resource_usage (list, optional): Resource usage data\n\n        Returns:\n            dict: Benchmark data including task name and execution time\n        \"\"\"\n        return {\n            \"task\": self.task_name,\n            \"total_t\": self.default_total_time,\n            \"statements\": statements,\n            \"resource_usage\": resource_usage or []\n        }\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.SlurmExecutor.build_job_script","title":"<code>build_job_script(statement)</code>","text":"<p>Custom build job script for Slurm.</p> Source code in <code>cgatcore/pipeline/executors.py</code> <pre><code>def build_job_script(self, statement):\n    \"\"\"Custom build job script for Slurm.\"\"\"\n    return super().build_job_script(statement)\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.SlurmExecutor.collect_benchmark_data","title":"<code>collect_benchmark_data(statements, resource_usage=None)</code>","text":"<p>Collect benchmark data for Slurm jobs.</p> <p>Parameters:</p> Name Type Description Default <code>statements</code> <code>list</code> <p>List of executed statements</p> required <code>resource_usage</code> <code>list</code> <p>Resource usage data</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>Benchmark data including task name and execution time</p> Source code in <code>cgatcore/pipeline/executors.py</code> <pre><code>def collect_benchmark_data(self, statements, resource_usage=None):\n    \"\"\"Collect benchmark data for Slurm jobs.\n\n    Args:\n        statements (list): List of executed statements\n        resource_usage (list, optional): Resource usage data\n\n    Returns:\n        dict: Benchmark data including task name and execution time\n    \"\"\"\n    return {\n        \"task\": self.task_name,\n        \"total_t\": self.default_total_time,\n        \"statements\": statements,\n        \"resource_usage\": resource_usage or []\n    }\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.SlurmExecutor.monitor_job_completion","title":"<code>monitor_job_completion(job_id)</code>","text":"<p>Monitor the completion of a Slurm job.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>str</code> <p>The Slurm job ID to monitor.</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the job fails or times out.</p> Source code in <code>cgatcore/pipeline/executors.py</code> <pre><code>def monitor_job_completion(self, job_id):\n    \"\"\"Monitor the completion of a Slurm job.\n\n    Args:\n        job_id (str): The Slurm job ID to monitor.\n\n    Raises:\n        RuntimeError: If the job fails or times out.\n    \"\"\"\n    while True:\n        # Use sacct to get job status\n        cmd = f\"sacct -j {job_id} --format=State --noheader --parsable2\"\n        process = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n\n        if process.returncode != 0:\n            self.logger.error(f\"Failed to get job status: {process.stderr}\")\n            raise RuntimeError(f\"Failed to get job status: {process.stderr}\")\n\n        status = process.stdout.strip()\n\n        # Check job status\n        if status in [\"COMPLETED\", \"COMPLETED+\"]:\n            self.logger.info(f\"Job {job_id} completed successfully\")\n            break\n        elif status in [\"FAILED\", \"TIMEOUT\", \"CANCELLED\", \"NODE_FAIL\"]:\n            self.logger.error(f\"Job {job_id} failed with status: {status}\")\n            raise RuntimeError(f\"Job {job_id} failed with status: {status}\")\n\n        # Wait before checking again\n        time.sleep(10)\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.TorqueExecutor","title":"<code>TorqueExecutor</code>","text":"<p>               Bases: <code>BaseExecutor</code></p> <p>Executor for running jobs on Torque cluster.</p> Source code in <code>cgatcore/pipeline/executors.py</code> <pre><code>class TorqueExecutor(BaseExecutor):\n    \"\"\"Executor for running jobs on Torque cluster.\"\"\"\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.logger = logging.getLogger(__name__)\n        self.task_name = \"torque_task\"\n        self.default_total_time = 7\n\n    def run(self, statement_list):\n        benchmark_data = []\n        for statement in statement_list:\n            self.logger.info(f\"Running statement on Torque: {statement}\")\n\n            full_statement, job_path = self.build_job_script(statement)\n\n            # Build the Torque job submission command\n            torque_command = f\"qsub -N {self.config.get('job_name', 'default_job')} -o {job_path}.o -e {job_path}.e {job_path}\"\n\n            process = subprocess.run(torque_command, shell=True, capture_output=True, text=True)\n\n            if process.returncode != 0:\n                self.logger.error(f\"Torque job submission failed: {process.stderr}\")\n                raise RuntimeError(f\"Torque job submission failed: {process.stderr}\")\n\n            job_id = process.stdout.strip()\n            self.logger.info(f\"Torque job submitted with ID: {job_id}\")\n\n            # Monitor job completion\n            self.monitor_job_completion(job_id)\n\n            benchmark_data.append(self.collect_benchmark_data([statement], resource_usage=[]))\n\n        return benchmark_data\n\n    def build_job_script(self, statement):\n        \"\"\"Custom build job script for Torque.\"\"\"\n        return super().build_job_script(statement)\n\n    def monitor_job_completion(self, job_id):\n        \"\"\"Monitor the completion of a Torque job.\n\n        Args:\n            job_id (str): The Torque job ID to monitor.\n\n        Raises:\n            RuntimeError: If the job fails or times out.\n        \"\"\"\n        while True:\n            # Use qstat to get job status\n            cmd = f\"qstat -f {job_id}\"\n            process = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n\n            if process.returncode != 0:\n                # Job not found in qstat could mean it's completed\n                # Use tracejob to get final status\n                cmd = f\"tracejob {job_id}\"\n                process = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n\n                if \"Exit_status=\" in process.stdout:\n                    if \"Exit_status=0\" in process.stdout:\n                        self.logger.info(f\"Job {job_id} completed successfully\")\n                        break\n                    else:\n                        status = process.stdout.split(\"Exit_status=\")[1].split()[0]\n                        self.logger.error(f\"Job {job_id} failed with exit status: {status}\")\n                        raise RuntimeError(f\"Job {job_id} failed with exit status: {status}\")\n\n                self.logger.error(f\"Failed to get job status: {process.stderr}\")\n                raise RuntimeError(f\"Failed to get job status: {process.stderr}\")\n\n            # Wait before checking again\n            time.sleep(10)\n\n    def collect_benchmark_data(self, statements, resource_usage=None):\n        \"\"\"Collect benchmark data for Torque jobs.\n\n        Args:\n            statements (list): List of executed statements\n            resource_usage (list, optional): Resource usage data\n\n        Returns:\n            dict: Benchmark data including task name and execution time\n        \"\"\"\n        return {\n            \"task\": self.task_name,\n            \"total_t\": self.default_total_time,\n            \"statements\": statements,\n            \"resource_usage\": resource_usage or []\n        }\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.TorqueExecutor.build_job_script","title":"<code>build_job_script(statement)</code>","text":"<p>Custom build job script for Torque.</p> Source code in <code>cgatcore/pipeline/executors.py</code> <pre><code>def build_job_script(self, statement):\n    \"\"\"Custom build job script for Torque.\"\"\"\n    return super().build_job_script(statement)\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.TorqueExecutor.collect_benchmark_data","title":"<code>collect_benchmark_data(statements, resource_usage=None)</code>","text":"<p>Collect benchmark data for Torque jobs.</p> <p>Parameters:</p> Name Type Description Default <code>statements</code> <code>list</code> <p>List of executed statements</p> required <code>resource_usage</code> <code>list</code> <p>Resource usage data</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>Benchmark data including task name and execution time</p> Source code in <code>cgatcore/pipeline/executors.py</code> <pre><code>def collect_benchmark_data(self, statements, resource_usage=None):\n    \"\"\"Collect benchmark data for Torque jobs.\n\n    Args:\n        statements (list): List of executed statements\n        resource_usage (list, optional): Resource usage data\n\n    Returns:\n        dict: Benchmark data including task name and execution time\n    \"\"\"\n    return {\n        \"task\": self.task_name,\n        \"total_t\": self.default_total_time,\n        \"statements\": statements,\n        \"resource_usage\": resource_usage or []\n    }\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.TorqueExecutor.monitor_job_completion","title":"<code>monitor_job_completion(job_id)</code>","text":"<p>Monitor the completion of a Torque job.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>str</code> <p>The Torque job ID to monitor.</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the job fails or times out.</p> Source code in <code>cgatcore/pipeline/executors.py</code> <pre><code>def monitor_job_completion(self, job_id):\n    \"\"\"Monitor the completion of a Torque job.\n\n    Args:\n        job_id (str): The Torque job ID to monitor.\n\n    Raises:\n        RuntimeError: If the job fails or times out.\n    \"\"\"\n    while True:\n        # Use qstat to get job status\n        cmd = f\"qstat -f {job_id}\"\n        process = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n\n        if process.returncode != 0:\n            # Job not found in qstat could mean it's completed\n            # Use tracejob to get final status\n            cmd = f\"tracejob {job_id}\"\n            process = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n\n            if \"Exit_status=\" in process.stdout:\n                if \"Exit_status=0\" in process.stdout:\n                    self.logger.info(f\"Job {job_id} completed successfully\")\n                    break\n                else:\n                    status = process.stdout.split(\"Exit_status=\")[1].split()[0]\n                    self.logger.error(f\"Job {job_id} failed with exit status: {status}\")\n                    raise RuntimeError(f\"Job {job_id} failed with exit status: {status}\")\n\n            self.logger.error(f\"Failed to get job status: {process.stderr}\")\n            raise RuntimeError(f\"Failed to get job status: {process.stderr}\")\n\n        # Wait before checking again\n        time.sleep(10)\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.add_doc","title":"<code>add_doc(value, replace=False)</code>","text":"<p>add doc string of value to function that is decorated.</p> <p>The original doc-string is added as the first paragraph(s) inside the new doc-string.</p>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.add_doc--parameter","title":"Parameter","text":"bool <p>If True, replace documentation rather than appending</p> Source code in <code>cgatcore/pipeline/utils.py</code> <pre><code>def add_doc(value, replace=False):\n    \"\"\"add doc string of value to function that is decorated.\n\n    The original doc-string is added as the first paragraph(s)\n    inside the new doc-string.\n\n    Parameter\n    ---------\n\n    replace : bool\n       If True, replace documentation rather than appending\n    \"\"\"\n    def _doc(func):\n        if func.__doc__:\n            lines = value.__doc__.split(\"\\n\")\n            for x, line in enumerate(lines):\n                if line.strip() == \"\":\n                    break\n            # insert appropriate indentiation\n            # currently hard-coded, can be derived\n            # from doc string?\n            if not replace:\n                lines.insert(x + 1, \" \" * 4 + func.__doc__)\n                func.__doc__ = \"\\n\".join(lines)\n            else:\n                func.__doc__ = value.__doc__\n        else:\n            func.__doc__ = value.__doc__\n        return func\n    return _doc\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.as_list","title":"<code>as_list(value)</code>","text":"<p>return a value as a list.</p> <p>If the value is a string and contains a <code>,</code>, the string will be split at <code>,</code>.</p>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.as_list--returns","title":"Returns","text":"<p>list</p> Source code in <code>cgatcore/pipeline/parameters.py</code> <pre><code>def as_list(value):\n    '''return a value as a list.\n\n    If the value is a string and contains a ``,``, the string will\n    be split at ``,``.\n\n    Returns\n    -------\n    list\n\n    '''\n    if isinstance(value, str):\n        try:\n            values = [x.strip() for x in value.strip().split(\",\")]\n        except AttributeError:\n            values = [value.strip()]\n        return [x for x in values if x != \"\"]\n    elif type(value) in (list, tuple):\n        return value\n    else:\n        return [value]\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.build_load_statement","title":"<code>build_load_statement(tablename, retry=True, options='')</code>","text":"<p>build a command line statement to upload data.</p> <p>Upload is performed via the :doc:<code>csv2db</code> script.</p> <p>The returned statement is suitable to use in pipe expression. This method is aware of the configuration values for database access and the chosen database backend.</p> <p>For example::</p> <pre><code>load_statement = P.build_load_statement(\"data\")\nstatement = \"cat data.txt | %(load_statement)s\"\nP.run(statement)\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.build_load_statement--arguments","title":"Arguments","text":"<p>tablename : string     Tablename for upload retry : bool     Add the <code>--retry</code> option to <code>csv2db.py</code> options : string     Command line options to be passed on to <code>csv2db.py</code></p>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.build_load_statement--returns","title":"Returns","text":"<p>string</p> Source code in <code>cgatcore/pipeline/database.py</code> <pre><code>def build_load_statement(tablename, retry=True, options=\"\"):\n    \"\"\"build a command line statement to upload data.\n\n    Upload is performed via the :doc:`csv2db` script.\n\n    The returned statement is suitable to use in pipe expression.\n    This method is aware of the configuration values for database\n    access and the chosen database backend.\n\n    For example::\n\n        load_statement = P.build_load_statement(\"data\")\n        statement = \"cat data.txt | %(load_statement)s\"\n        P.run(statement)\n\n    Arguments\n    ---------\n    tablename : string\n        Tablename for upload\n    retry : bool\n        Add the ``--retry`` option to `csv2db.py`\n    options : string\n        Command line options to be passed on to `csv2db.py`\n\n    Returns\n    -------\n    string\n\n    \"\"\"\n\n    opts = []\n\n    if retry:\n        opts.append(\" --retry \")\n\n    params = get_params()\n    opts.append(\"--database-url={}\".format(params[\"database\"][\"url\"]))\n\n    db_options = \" \".join(opts)\n    load_statement = (\n        \"python -m cgatcore.csv2db {db_options} {options} --table={tablename}\".format(**locals()))\n\n    return load_statement\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.check_executables","title":"<code>check_executables(filenames)</code>","text":"<p>check for the presence/absence of executables</p> Source code in <code>cgatcore/pipeline/files.py</code> <pre><code>def check_executables(filenames):\n    \"\"\"check for the presence/absence of executables\"\"\"\n\n    missing = []\n\n    for filename in filenames:\n        if not iotools.which(filename):\n            missing.append(filename)\n\n    if missing:\n        raise ValueError(\"missing executables: %s\" % \",\".join(missing))\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.check_parameter","title":"<code>check_parameter(param)</code>","text":"<p>check if parameter <code>key</code> is set</p> Source code in <code>cgatcore/pipeline/parameters.py</code> <pre><code>def check_parameter(param):\n    \"\"\"check if parameter ``key`` is set\"\"\"\n    if param not in PARAMS:\n        raise ValueError(\"need `%s` to be set\" % param)\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.check_scripts","title":"<code>check_scripts(filenames)</code>","text":"<p>check for the presence/absence of scripts</p> Source code in <code>cgatcore/pipeline/files.py</code> <pre><code>def check_scripts(filenames):\n    \"\"\"check for the presence/absence of scripts\"\"\"\n    missing = []\n    for filename in filenames:\n        if not os.path.exists(filename):\n            missing.append(filename)\n\n    if missing:\n        raise ValueError(\"missing scripts: %s\" % \",\".join(missing))\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.clean","title":"<code>clean(files, logfile)</code>","text":"<p>clean up files given by glob expressions.</p> <p>Files are cleaned up by zapping, i.e. the files are set to size 0. Links to files are replaced with place-holders.</p> <p>Information about the original file is written to <code>logfile</code>.</p>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.clean--arguments","title":"Arguments","text":"<p>files : list     List of glob expressions of files to clean up. logfile : string     Filename of logfile.</p> Source code in <code>cgatcore/pipeline/control.py</code> <pre><code>def clean(files, logfile):\n    '''clean up files given by glob expressions.\n\n    Files are cleaned up by zapping, i.e. the files are set to size\n    0. Links to files are replaced with place-holders.\n\n    Information about the original file is written to `logfile`.\n\n    Arguments\n    ---------\n    files : list\n        List of glob expressions of files to clean up.\n    logfile : string\n        Filename of logfile.\n\n    '''\n    fields = ('st_atime', 'st_blksize', 'st_blocks',\n              'st_ctime', 'st_dev', 'st_gid', 'st_ino',\n              'st_mode', 'st_mtime', 'st_nlink',\n              'st_rdev', 'st_size', 'st_uid')\n\n    dry_run = get_params().get(\"dryrun\", False)\n\n    if not dry_run:\n        if not os.path.exists(logfile):\n            outfile = iotools.open_file(logfile, \"w\")\n            outfile.write(\"filename\\tzapped\\tlinkdest\\t%s\\n\" %\n                          \"\\t\".join(fields))\n        else:\n            outfile = iotools.open_file(logfile, \"a\")\n\n    c = E.Counter()\n    for fn in files:\n        c.files += 1\n        if not dry_run:\n            stat, linkdest = iotools.zap_file(fn)\n            if stat is not None:\n                c.zapped += 1\n                if linkdest is not None:\n                    c.links += 1\n                outfile.write(\"%s\\t%s\\t%s\\t%s\\n\" % (\n                    fn,\n                    time.asctime(time.localtime(time.time())),\n                    linkdest,\n                    \"\\t\".join([str(getattr(stat, x)) for x in fields])))\n\n    get_logger().info(\"zapped: %s\" % (c))\n    outfile.close()\n\n    return c\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.clone_pipeline","title":"<code>clone_pipeline(srcdir, destdir=None)</code>","text":"<p>clone a pipeline.</p> <p>Cloning entails creating a mirror of the source pipeline. Generally, data files are mirrored by linking. Configuration files and the pipeline database will be copied.</p> <p>Without modification of any files, building the cloned pipeline in <code>destdir</code> should not re-run any commands. However, on deleting selected files, the pipeline should run from the appropriate point.  Newly created files will not affect the original pipeline.</p> <p>Cloning pipelines permits sharing partial results between pipelines, for example for parameter optimization.</p>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.clone_pipeline--arguments","title":"Arguments","text":"<p>scrdir : string     Source directory destdir : string     Destination directory. If None, use the current directory.</p> Source code in <code>cgatcore/pipeline/control.py</code> <pre><code>def clone_pipeline(srcdir, destdir=None):\n    '''clone a pipeline.\n\n    Cloning entails creating a mirror of the source pipeline.\n    Generally, data files are mirrored by linking. Configuration\n    files and the pipeline database will be copied.\n\n    Without modification of any files, building the cloned pipeline in\n    `destdir` should not re-run any commands. However, on deleting\n    selected files, the pipeline should run from the appropriate\n    point.  Newly created files will not affect the original pipeline.\n\n    Cloning pipelines permits sharing partial results between\n    pipelines, for example for parameter optimization.\n\n    Arguments\n    ---------\n    scrdir : string\n        Source directory\n    destdir : string\n        Destination directory. If None, use the current directory.\n\n    '''\n\n    if destdir is None:\n        destdir = os.path.curdir\n\n    get_logger().info(\"cloning pipeline from %s to %s\" % (srcdir, destdir))\n\n    copy_files = (\"conf.py\", \"pipeline.yml\", \"benchmark.yml\", \"csvdb\")\n    ignore_prefix = (\n        \"report\", \"_cache\", \"export\", \"tmp\", \"ctmp\",\n        \"_static\", \"_templates\", \"shell.log\", \"pipeline.log\",\n        \"results.commit\")\n\n    def _ignore(p):\n        for x in ignore_prefix:\n            if p.startswith(x):\n                return True\n        return False\n\n    for root, dirs, files in os.walk(srcdir):\n\n        relpath = os.path.relpath(root, srcdir)\n        if _ignore(relpath):\n            continue\n\n        for d in dirs:\n            if _ignore(d):\n                continue\n            dest = os.path.join(os.path.join(destdir, relpath, d))\n            os.mkdir(dest)\n            # touch\n            s = os.stat(os.path.join(root, d))\n            os.utime(dest, (s.st_atime, s.st_mtime))\n\n        for f in files:\n            if _ignore(f):\n                continue\n\n            fn = os.path.join(root, f)\n            dest_fn = os.path.join(destdir, relpath, f)\n            if f in copy_files:\n                shutil.copyfile(fn, dest_fn)\n            else:\n                # realpath resolves links - thus links will be linked to\n                # the original target\n                os.symlink(os.path.realpath(fn),\n                           dest_fn)\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.close_session","title":"<code>close_session()</code>","text":"<p>close the global DRMAA session.</p> Source code in <code>cgatcore/pipeline/execution.py</code> <pre><code>def close_session():\n    \"\"\"close the global DRMAA session.\"\"\"\n    global GLOBAL_SESSION\n\n    if GLOBAL_SESSION is not None:\n        GLOBAL_SESSION.exit()\n        GLOBAL_SESSION = None\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.cluster_runnable","title":"<code>cluster_runnable(func)</code>","text":"<p>A dectorator that allows a function to be run on the cluster.</p> <p>The decorated function now takes extra arguments. The most important is submit. If set to true, it will submit the function to the cluster via the pipeline.submit framework. Arguments to the function are pickled, so this will only work if arguments are picklable. Other arguments to submit are also accepted.</p> <p>Note that this allows the unusal combination of submit false, and to_cluster true. This will submit the function as an external job, but run it on the local machine.</p> <p>Note: all arguments in the decorated function must be passed as key-word arguments.</p> Source code in <code>cgatcore/pipeline/execution.py</code> <pre><code>def cluster_runnable(func):\n    '''A dectorator that allows a function to be run on the cluster.\n\n    The decorated function now takes extra arguments. The most important\n    is *submit*. If set to true, it will submit the function to the cluster\n    via the pipeline.submit framework. Arguments to the function are\n    pickled, so this will only work if arguments are picklable. Other\n    arguments to submit are also accepted.\n\n    Note that this allows the unusal combination of *submit* false,\n    and *to_cluster* true. This will submit the function as an external\n    job, but run it on the local machine.\n\n    Note: all arguments in the decorated function must be passed as\n    key-word arguments.\n    '''\n\n    # MM: when decorating functions with cluster_runnable, provide\n    # them as kwargs, else will throw attribute error\n\n    function_name = func.__name__\n\n    def submit_function(*args, **kwargs):\n\n        if \"submit\" in kwargs and kwargs[\"submit\"]:\n            del kwargs[\"submit\"]\n            submit_args, args_file = _pickle_args(args, kwargs)\n            module_file = os.path.abspath(\n                sys.modules[func.__module__].__file__)\n            submit(iotools.snip(__file__),\n                   \"run_pickled\",\n                   args=[iotools.snip(module_file), function_name, args_file],\n                   **submit_args)\n        else:\n            # remove job contral options before running function\n            for x in (\"submit\", \"job_options\", \"job_queue\"):\n                if x in kwargs:\n                    del kwargs[x]\n            return func(*args, **kwargs)\n\n    return submit_function\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.concatenate_and_load","title":"<code>concatenate_and_load(infiles, outfile, regex_filename=None, header=None, cat='track', has_titles=True, missing_value='na', retry=True, tablename=None, options='', job_memory=None, to_cluster=True)</code>","text":"<p>concatenate multiple tab-separated files and upload into database.</p> <p>The table name is given by outfile without the \".load\" suffix.</p> <p>A typical concatenate and load task in ruffus would look like this::</p> <pre><code>@merge(\"*.tsv.gz\", \".load\")\ndef loadData(infile, outfile):\n    P.concatenateAndLoad(infiles, outfile)\n</code></pre> <p>Upload is performed via the :doc:<code>csv2db</code> script.</p>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.concatenate_and_load--arguments","title":"Arguments","text":"<p>infiles : list     Filenames of the input data outfile : string     Output filename. This will contain the logging information. The     table name is derived from <code>outfile</code>. regex_filename : string     If given, regex_filename is applied to the filename to extract     the track name. If the pattern contains multiple groups, they are     added as additional columns. For example, if <code>cat</code> is set to     <code>track,method</code> and <code>regex_filename</code> is <code>(.*)_(.*).tsv.gz</code>     it will add the columns <code>track</code> and method to the table. header : string     Comma-separated list of values for header. cat : string     Column title for column containing the track name. The track name     is derived from the filename, see <code>regex_filename</code>. has_titles : bool     If True, files are expected to have column titles in their first row. missing_value : string     String to use for missing values. retry : bool     If True, multiple attempts will be made if the data can     not be loaded at the first try, for example if a table is locked. tablename: string     Name to use for table. If unset derive from outfile. options : string     Command line options for the <code>csv2db.py</code> script. job_memory : string     Amount of memory to allocate for job. If unset, uses the global     default. Implies to_cluster=True. to_cluster : bool     By default load jobs are not submitted to the cluster as they sometimes     become blocked. Setting this true will override this behavoir.</p> Source code in <code>cgatcore/pipeline/database.py</code> <pre><code>def concatenate_and_load(infiles,\n                         outfile,\n                         regex_filename=None,\n                         header=None,\n                         cat=\"track\",\n                         has_titles=True,\n                         missing_value=\"na\",\n                         retry=True,\n                         tablename=None,\n                         options=\"\",\n                         job_memory=None,\n                         to_cluster=True):\n    \"\"\"concatenate multiple tab-separated files and upload into database.\n\n    The table name is given by outfile without the\n    \".load\" suffix.\n\n    A typical concatenate and load task in ruffus would look like this::\n\n        @merge(\"*.tsv.gz\", \".load\")\n        def loadData(infile, outfile):\n            P.concatenateAndLoad(infiles, outfile)\n\n    Upload is performed via the :doc:`csv2db` script.\n\n    Arguments\n    ---------\n    infiles : list\n        Filenames of the input data\n    outfile : string\n        Output filename. This will contain the logging information. The\n        table name is derived from `outfile`.\n    regex_filename : string\n        If given, *regex_filename* is applied to the filename to extract\n        the track name. If the pattern contains multiple groups, they are\n        added as additional columns. For example, if `cat` is set to\n        ``track,method`` and `regex_filename` is ``(.*)_(.*).tsv.gz``\n        it will add the columns ``track`` and method to the table.\n    header : string\n        Comma-separated list of values for header.\n    cat : string\n        Column title for column containing the track name. The track name\n        is derived from the filename, see `regex_filename`.\n    has_titles : bool\n        If True, files are expected to have column titles in their first row.\n    missing_value : string\n        String to use for missing values.\n    retry : bool\n        If True, multiple attempts will be made if the data can\n        not be loaded at the first try, for example if a table is locked.\n    tablename: string\n        Name to use for table. If unset derive from outfile.\n    options : string\n        Command line options for the `csv2db.py` script.\n    job_memory : string\n        Amount of memory to allocate for job. If unset, uses the global\n        default. Implies to_cluster=True.\n    to_cluster : bool\n        By default load jobs are not submitted to the cluster as they sometimes\n        become blocked. Setting this true will override this behavoir.\n    \"\"\"\n    if job_memory is None:\n        job_memory = get_params()[\"cluster_memory_default\"]\n\n    if tablename is None:\n        tablename = to_table(outfile)\n\n    infiles = \" \".join(infiles)\n\n    passed_options = options\n    load_options, cat_options = [\"--add-index=track\"], []\n\n    if regex_filename:\n        cat_options.append(\"--regex-filename='%s'\" % regex_filename)\n\n    if header:\n        load_options.append(\"--header-names=%s\" % header)\n\n    if not has_titles:\n        cat_options.append(\"--no-titles\")\n\n    cat_options = \" \".join(cat_options)\n    load_options = \" \".join(load_options) + \" \" + passed_options\n\n    load_statement = build_load_statement(tablename,\n                                          options=load_options,\n                                          retry=retry)\n\n    statement = '''python -m cgatcore.tables\n    --cat=%(cat)s\n    --missing-value=%(missing_value)s\n    %(cat_options)s\n    %(infiles)s\n    | %(load_statement)s\n    &gt; %(outfile)s'''\n\n    run(statement)\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.config_to_dictionary","title":"<code>config_to_dictionary(config)</code>","text":"<p>convert the contents of a :py:class:<code>ConfigParser.ConfigParser</code> object to a dictionary</p> <p>This method works by iterating over all configuration values in a :py:class:<code>ConfigParser.ConfigParser</code> object and inserting values into a dictionary. Section names are prefixed using and underscore. Thus::</p> <pre><code>[sample]\nname=12\n</code></pre> <p>is entered as <code>sample_name=12</code> into the dictionary. The sections <code>general</code> and <code>DEFAULT</code> are treated specially in that both the prefixed and the unprefixed values are inserted: ::</p> <p>[general]    genome=hg19</p> <p>will be added as <code>general_genome=hg19</code> and <code>genome=hg19</code>.</p> <p>Numbers will be automatically recognized as such and converted into integers or floats.</p>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.config_to_dictionary--returns","title":"Returns","text":"<p>config : dict     A dictionary of configuration values</p> Source code in <code>cgatcore/pipeline/parameters.py</code> <pre><code>def config_to_dictionary(config):\n    \"\"\"convert the contents of a :py:class:`ConfigParser.ConfigParser`\n    object to a dictionary\n\n    This method works by iterating over all configuration values in a\n    :py:class:`ConfigParser.ConfigParser` object and inserting values\n    into a dictionary. Section names are prefixed using and underscore.\n    Thus::\n\n        [sample]\n        name=12\n\n    is entered as ``sample_name=12`` into the dictionary. The sections\n    ``general`` and ``DEFAULT`` are treated specially in that both\n    the prefixed and the unprefixed values are inserted: ::\n\n       [general]\n       genome=hg19\n\n    will be added as ``general_genome=hg19`` and ``genome=hg19``.\n\n    Numbers will be automatically recognized as such and converted into\n    integers or floats.\n\n    Returns\n    -------\n    config : dict\n        A dictionary of configuration values\n\n    \"\"\"\n    p = defaultdict(lambda: defaultdict(TriggeredDefaultFactory()))\n    for section in config.sections():\n        for key, value in config.items(section):\n            try:\n                v = iotools.str2val(value)\n            except TypeError:\n                E.error(\"error converting key %s, value %s\" % (key, value))\n                E.error(\"Possible multiple concurrent attempts to \"\n                        \"read configuration\")\n                raise\n\n            p[\"%s_%s\" % (section, key)] = v\n\n            # IMS: new heirarchical format\n            try:\n                p[section][key] = v\n            except TypeError:\n                # fails with things like genome_dir=abc\n                # if [genome] does not exist.\n                continue\n\n            if section in (\"general\", \"DEFAULT\"):\n                p[\"%s\" % (key)] = v\n\n    for key, value in config.defaults().items():\n        p[\"%s\" % (key)] = iotools.str2val(value)\n\n    return p\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.connect","title":"<code>connect()</code>","text":"<p>connect to SQLite database used in this pipeline.</p> <p>.. note::    This method is currently only implemented for sqlite    databases. It needs refactoring for generic access.    Alternatively, use an full or partial ORM.</p> <p>If <code>annotations_database</code> is in params, this method will attach the named database as <code>annotations</code>.</p>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.connect--returns","title":"Returns","text":"<p>dbh    a database handle</p> Source code in <code>cgatcore/pipeline/database.py</code> <pre><code>def connect():\n    \"\"\"connect to SQLite database used in this pipeline.\n\n    .. note::\n       This method is currently only implemented for sqlite\n       databases. It needs refactoring for generic access.\n       Alternatively, use an full or partial ORM.\n\n    If ``annotations_database`` is in params, this method\n    will attach the named database as ``annotations``.\n\n    Returns\n    -------\n    dbh\n       a database handle\n\n    \"\"\"\n\n    # Note that in the future this might return an sqlalchemy or\n    # db.py handle.\n    url = get_params()[\"database\"][\"url\"]\n    is_sqlite3 = url.startswith(\"sqlite\")\n\n    if is_sqlite3:\n        connect_args = {'check_same_thread': False}\n    else:\n        connect_args = {}\n\n    creator = None\n    if is_sqlite3 and \"annotations_dir\" in get_params():\n        # not sure what the correct way is for url\n        # sqlite:///./csvdb -&gt; ./csvdb\n        # sqlite:////path/to/csvdb -&gt; /path/to/csvdb\n        filename = os.path.abspath(url[len(\"sqlite:///\"):])\n\n        def creator():\n            conn = sqlite3.connect(filename)\n            conn.execute(\"ATTACH DATABASE '{}' as annotations\".format(\n                os.path.join(get_params()[\"annotations_dir\"], \"csvdb\")))\n            return conn\n\n    engine = sqlalchemy.create_engine(\n        url,\n        connect_args=connect_args,\n        creator=creator)\n\n    return engine\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.create_view","title":"<code>create_view(dbhandle, tables, tablename, outfile, view_type='TABLE', ignore_duplicates=True)</code>","text":"<p>create a database view for a list of tables.</p> <p>This method performs a join across multiple tables and stores the result either as a view or a table in the database.</p>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.create_view--arguments","title":"Arguments","text":"<p>dbhandle :     A database handle. tables : list of tuples     Tables to merge. Each tuple contains the name of a table and     the field to join with the first table. For example::</p> <pre><code>    tables = (\n        \"reads_summary\", \"track\",\n        \"bam_stats\", \"track\",\n        \"context_stats\", \"track\",\n        \"picard_stats_alignment_summary_metrics\", \"track\")\n</code></pre> string <p>Name of the view or table to be created.</p> <p>outfile : string     Output filename for status information. view_type : string     Type of view, either <code>VIEW</code> or <code>TABLE</code>.  If a view is to be     created across multiple databases, use <code>TABLE</code>. ignore_duplicates : bool     If set to False, duplicate column names will be added with the     tablename as prefix. The default is to ignore.</p> Source code in <code>cgatcore/pipeline/database.py</code> <pre><code>def create_view(dbhandle, tables, tablename, outfile,\n                view_type=\"TABLE\",\n                ignore_duplicates=True):\n    '''create a database view for a list of tables.\n\n    This method performs a join across multiple tables and stores the\n    result either as a view or a table in the database.\n\n    Arguments\n    ---------\n    dbhandle :\n        A database handle.\n    tables : list of tuples\n        Tables to merge. Each tuple contains the name of a table and\n        the field to join with the first table. For example::\n\n            tables = (\n                \"reads_summary\", \"track\",\n                \"bam_stats\", \"track\",\n                \"context_stats\", \"track\",\n                \"picard_stats_alignment_summary_metrics\", \"track\")\n\n    tablename : string\n        Name of the view or table to be created.\n    outfile : string\n        Output filename for status information.\n    view_type : string\n        Type of view, either ``VIEW`` or ``TABLE``.  If a view is to be\n        created across multiple databases, use ``TABLE``.\n    ignore_duplicates : bool\n        If set to False, duplicate column names will be added with the\n        tablename as prefix. The default is to ignore.\n\n    '''\n\n    database.executewait(\n        dbhandle,\n        \"DROP %(view_type)s IF EXISTS %(tablename)s\" % locals())\n\n    tracks, columns = [], []\n    tablenames = [x[0] for x in tables]\n    for table, track in tables:\n        d = database.executewait(\n            dbhandle,\n            \"SELECT COUNT(DISTINCT %s) FROM %s\" % (track, table))\n        tracks.append(d.fetchone()[0])\n        columns.append(\n            [x.lower() for x in database.getColumnNames(dbhandle, table)\n             if x != track])\n\n    E.info(\"creating %s from the following tables: %s\" %\n           (tablename, str(list(zip(tablenames, tracks)))))\n    if min(tracks) != max(tracks):\n        raise ValueError(\n            \"number of rows not identical - will not create view\")\n\n    from_statement = \" , \".join(\n        [\"%s as t%i\" % (y[0], x) for x, y in enumerate(tables)])\n    f = tables[0][1]\n    where_statement = \" AND \".join(\n        [\"t0.%s = t%i.%s\" % (f, x + 1, y[1])\n         for x, y in enumerate(tables[1:])])\n\n    all_columns, taken = [], set()\n    for x, c in enumerate(columns):\n        i = set(taken).intersection(set(c))\n        if i:\n            E.warn(\"duplicate column names: %s \" % i)\n            if not ignore_duplicates:\n                table = tables[x][0]\n                all_columns.extend(\n                    [\"t%i.%s AS %s_%s\" % (x, y, table, y) for y in i])\n                c = [y for y in c if y not in i]\n\n        all_columns.extend([\"t%i.%s\" % (x, y) for y in c])\n        taken.update(set(c))\n\n    all_columns = \",\".join(all_columns)\n    statement = '''\n    CREATE %(view_type)s %(tablename)s AS SELECT t0.track, %(all_columns)s\n    FROM %(from_statement)s\n    WHERE %(where_statement)s\n    ''' % locals()\n    database.executewait(dbhandle, statement)\n\n    nrows = database.executewait(\n        dbhandle, \"SELECT COUNT(*) FROM view_mapping\").fetchone()[0]\n\n    if nrows == 0:\n        raise ValueError(\n            \"empty view mapping, check statement = %s\" %\n            (statement % locals()))\n    if nrows != min(tracks):\n        E.warn(\"view creates duplicate rows, got %i, expected %i\" %\n               (nrows, min(tracks)))\n\n    E.info(\"created view_mapping with %i rows\" % nrows)\n    touch_file(outfile)\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.execute","title":"<code>execute(statement, **kwargs)</code>","text":"<p>execute a statement locally.</p> <p>This method implements the same parameter interpolation as the function :func:<code>run</code>.</p>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.execute--arguments","title":"Arguments","text":"<p>statement : string     Command line statement to run.</p>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.execute--returns","title":"Returns","text":"<p>stdout : string     Data sent to standard output by command stderr : string     Data sent to standard error by command</p> Source code in <code>cgatcore/pipeline/execution.py</code> <pre><code>def execute(statement, **kwargs):\n    '''execute a statement locally.\n\n    This method implements the same parameter interpolation\n    as the function :func:`run`.\n\n    Arguments\n    ---------\n    statement : string\n        Command line statement to run.\n\n    Returns\n    -------\n    stdout : string\n        Data sent to standard output by command\n    stderr : string\n        Data sent to standard error by command\n    '''\n\n    if not kwargs:\n        kwargs = get_caller_locals()\n\n    kwargs = dict(list(get_params().items()) + list(kwargs.items()))\n\n    logger = get_logger()\n    logger.info(\"running %s\" % (statement % kwargs))\n\n    if \"cwd\" not in kwargs:\n        cwd = get_params()[\"work_dir\"]\n    else:\n        cwd = kwargs[\"cwd\"]\n\n    # cleaning up of statement\n    # remove new lines and superfluous spaces and tabs\n    statement = \" \".join(re.sub(\"\\t+\", \" \", statement).split(\"\\n\")).strip()\n    if statement.endswith(\";\"):\n        statement = statement[:-1]\n\n    # always use bash\n    os.environ.update(\n        {'BASH_ENV': os.path.join(os.environ['HOME'], '.bashrc')})\n    process = subprocess.Popen(statement % kwargs,\n                               cwd=cwd,\n                               shell=True,\n                               stdin=sys.stdin,\n                               stdout=sys.stdout,\n                               stderr=sys.stderr,\n                               env=os.environ.copy(),\n                               executable=\"/bin/bash\")\n\n    # process.stdin.close()\n    stdout, stderr = process.communicate()\n\n    if process.returncode != 0:\n        raise OSError(\n            \"Child was terminated by signal %i: \\n\"\n            \"The stderr was: \\n%s\\n%s\\n\" %\n            (-process.returncode, stderr, statement))\n\n    return stdout, stderr\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.file_is_mounted","title":"<code>file_is_mounted(filename)</code>","text":"<p>return True if filename is mounted.</p> <p>A file is likely to be mounted if it is located inside a subdirectory of the local scratch directory.</p> Source code in <code>cgatcore/pipeline/execution.py</code> <pre><code>def file_is_mounted(filename):\n    \"\"\"return True if filename is mounted.\n\n    A file is likely to be mounted if it is located\n    inside a subdirectory of the local scratch directory.\n    \"\"\"\n    if get_params()[\"mount_point\"]:\n        return os.path.abspath(filename).startswith(get_params()[\"mount_point\"])\n    else:\n        return False\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.get_caller","title":"<code>get_caller(decorators=0)</code>","text":"<p>return the name of the calling class/module</p>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.get_caller--arguments","title":"Arguments","text":"<p>decorators : int     Number of contexts to go up to reach calling function     of interest.</p>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.get_caller--returns","title":"Returns","text":"<p>mod : object     The calling module/class</p> Source code in <code>cgatcore/pipeline/utils.py</code> <pre><code>def get_caller(decorators=0):\n    \"\"\"return the name of the calling class/module\n\n    Arguments\n    ---------\n    decorators : int\n        Number of contexts to go up to reach calling function\n        of interest.\n\n    Returns\n    -------\n    mod : object\n        The calling module/class\n    \"\"\"\n\n    frm = inspect.stack()\n    return inspect.getmodule(frm[2 + decorators].frame)\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.get_caller_locals","title":"<code>get_caller_locals(decorators=0)</code>","text":"<p>returns the locals of the calling function.</p> <p>from http://pylab.blogspot.com/2009/02/      python-accessing-caller-locals-from.html</p>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.get_caller_locals--arguments","title":"Arguments","text":"<p>decorators : int     Number of contexts to go up to reach calling function     of interest.</p>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.get_caller_locals--returns","title":"Returns","text":"<p>locals : dict     Dictionary of variable defined in the context of the     calling function.</p> Source code in <code>cgatcore/pipeline/utils.py</code> <pre><code>def get_caller_locals(decorators=0):\n    '''returns the locals of the calling function.\n\n    from http://pylab.blogspot.com/2009/02/\n         python-accessing-caller-locals-from.html\n\n    Arguments\n    ---------\n    decorators : int\n        Number of contexts to go up to reach calling function\n        of interest.\n\n    Returns\n    -------\n    locals : dict\n        Dictionary of variable defined in the context of the\n        calling function.\n    '''\n    f = sys._getframe(2 + decorators)\n    args = inspect.getargvalues(f)\n    return args[3]\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.get_calling_function","title":"<code>get_calling_function(decorators=0)</code>","text":"<p>return the name of the calling function</p>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.get_calling_function--arguments","title":"Arguments","text":"<p>decorators : int     Number of contexts to go up to reach calling function     of interest.</p>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.get_calling_function--returns","title":"Returns","text":"<p>mod : object     The calling module</p> Source code in <code>cgatcore/pipeline/utils.py</code> <pre><code>def get_calling_function(decorators=0):\n    \"\"\"return the name of the calling function\n\n    Arguments\n    ---------\n    decorators : int\n        Number of contexts to go up to reach calling function\n        of interest.\n\n    Returns\n    -------\n    mod : object\n        The calling module\n    \"\"\"\n\n    frm = inspect.stack()\n    return frm[2 + decorators].function\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.get_database_name","title":"<code>get_database_name()</code>","text":"<p>Return the database name associated with the pipeline.</p> <p>This method lookis in different sections in the ini file to permit both old style <code>database</code> and new style <code>database_name</code>.</p> <p>This method has been implemented for backwards compatibility.</p>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.get_database_name--returns","title":"Returns","text":"<p>databasename : string     database name. Returns empty string if not found.</p>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.get_database_name--raises","title":"Raises","text":"<p>KeyError    If no database name is found</p> Source code in <code>cgatcore/pipeline/database.py</code> <pre><code>def get_database_name():\n    '''Return the database name associated with the pipeline.\n\n    This method lookis in different sections in the ini file to permit\n    both old style ``database`` and new style ``database_name``.\n\n    This method has been implemented for backwards compatibility.\n\n    Returns\n    -------\n    databasename : string\n        database name. Returns empty string if not found.\n\n    Raises\n    ------\n    KeyError\n       If no database name is found\n\n    '''\n\n    locations = [\"database_name\", \"database\"]\n    params = get_params()\n    for location in locations:\n        database = params.get(location, None)\n        if database is not None:\n            return database\n\n    raise KeyError(\"database name not found\")\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.get_executor","title":"<code>get_executor(options=None)</code>","text":"<p>Return an executor instance based on the specified queue manager in options.</p> <ul> <li>options (dict): Dictionary containing execution options,                    including \"cluster_queue_manager\".</li> </ul> <p>Returns: - Executor instance appropriate for the specified queue manager.</p> Source code in <code>cgatcore/pipeline/execution.py</code> <pre><code>def get_executor(options=None):\n    \"\"\"\n    Return an executor instance based on the specified queue manager in options.\n\n    Parameters:\n    - options (dict): Dictionary containing execution options, \n                      including \"cluster_queue_manager\".\n\n    Returns:\n    - Executor instance appropriate for the specified queue manager.\n    \"\"\"\n    if options is None:\n        options = get_params()\n\n    if options.get(\"testing\", False):\n        return LocalExecutor(**options)\n\n    # Check if to_cluster is explicitly set to False\n    if not options.get(\"to_cluster\", True):  # Defaults to True if not specified\n        return LocalExecutor(**options)\n\n    queue_manager = options.get(\"cluster_queue_manager\", None)\n\n    # Check for KubernetesExecutor\n    if queue_manager == \"kubernetes\" and KubernetesExecutor is not None:\n        return KubernetesExecutor(**options)\n\n    # Check for SGEExecutor (Sun Grid Engine)\n    elif queue_manager == \"sge\" and shutil.which(\"qsub\") is not None:\n        return SGEExecutor(**options)\n\n    # Check for SlurmExecutor\n    elif queue_manager == \"slurm\" and shutil.which(\"sbatch\") is not None:\n        return SlurmExecutor(**options)\n\n    # Check for TorqueExecutor\n    elif queue_manager == \"torque\" and shutil.which(\"qsub\") is not None:\n        return TorqueExecutor(**options)\n\n    # Fallback to LocalExecutor, not sure if this should raise an error though, feels like it should\n    else:\n        return LocalExecutor(**options)\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.get_header","title":"<code>get_header()</code>","text":"<p>return a header string with command line options and timestamp</p> Source code in <code>cgatcore/experiment.py</code> <pre><code>def get_header():\n    \"\"\"return a header string with command line options and timestamp\n\n    \"\"\"\n    system, host, release, version, machine = os.uname()\n\n    return \"output generated by %s\\njob started at %s on %s -- %s\\npid: %i, system: %s %s %s %s\" %\\\n           (\" \".join(sys.argv),\n            time.asctime(time.localtime(time.time())),\n            host,\n            global_id,\n            os.getpid(),\n            system, release, version, machine)\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.get_mounted_location","title":"<code>get_mounted_location(filename)</code>","text":"<p>return location of filename within mounted directory</p> Source code in <code>cgatcore/pipeline/execution.py</code> <pre><code>def get_mounted_location(filename):\n    \"\"\"return location of filename within mounted directory\n\n    \"\"\"\n    return os.path.abspath(filename)[len(get_params()[\"mount_point\"]):]\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.get_param_output","title":"<code>get_param_output(options=None)</code>","text":"<p>return a string containing script parameters.</p> <p>Parameters are all variables that start with <code>param_</code>.</p> Source code in <code>cgatcore/pipeline/control.py</code> <pre><code>def get_param_output(options=None):\n    \"\"\"return a string containing script parameters.\n\n    Parameters are all variables that start with ``param_``.\n    \"\"\"\n    result = []\n    if options:\n        members = options\n        for k, v in sorted(members.items()):\n            result.append(\"%-40s: %s\" % (k, str(v)))\n    else:\n        vars = inspect.currentframe().f_back.f_locals\n        for var in [x for x in list(vars.keys()) if re.match(\"param_\", x)]:\n            result.append(\"%-40s: %s\" %\n                          (var, str(vars[var])))\n\n    if result:\n        return \"\\n\".join(result)\n    else:\n        return \"# no parameters.\"\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.get_param_section","title":"<code>get_param_section(section)</code>","text":"<p>return config values in section</p> <p>Sections are built by common prefixes.</p> Source code in <code>cgatcore/pipeline/parameters.py</code> <pre><code>def get_param_section(section):\n    \"\"\"return config values in section\n\n    Sections are built by common prefixes.\n    \"\"\"\n    if not section.endswith(\"_\"):\n        section = section + \"_\"\n    n = len(section)\n    return [(x[n:], y) for x, y in PARAMS.items() if x.startswith(section)]\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.get_parameters","title":"<code>get_parameters(filenames=None, defaults=None, site_ini=True, user=True, only_import=None)</code>","text":"<p>read one or more config files and build global PARAMS configuration dictionary.</p>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.get_parameters--arguments","title":"Arguments","text":"<p>filenames : list    List of filenames of the configuration files to read. defaults : dict    Dictionary with default values. These will be overwrite    any hard-coded parameters, but will be overwritten by user    specified parameters in the configuration files. user : bool    If set, configuration files will also be read from a    file called :file:<code>.cgat.yml</code> in the user<code>s    home directory. only_import : bool    If set to a boolean, the parameter dictionary will be a    defaultcollection. This is useful for pipelines that are    imported (for example for documentation generation) but not    executed as there might not be an appropriate .yml file    available. If</code>only_import` is None, it will be set to the    default, which is to raise an exception unless the calling    script is imported or the option <code>--is-test</code> has been passed    at the command line.</p>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.get_parameters--returns","title":"Returns","text":"<p>params : dict    Global configuration dictionary.</p> Source code in <code>cgatcore/pipeline/parameters.py</code> <pre><code>def get_parameters(filenames=None,\n                   defaults=None,\n                   site_ini=True,\n                   user=True,\n                   only_import=None):\n    '''read one or more config files and build global PARAMS configuration\n    dictionary.\n\n    Arguments\n    ---------\n    filenames : list\n       List of filenames of the configuration files to read.\n    defaults : dict\n       Dictionary with default values. These will be overwrite\n       any hard-coded parameters, but will be overwritten by user\n       specified parameters in the configuration files.\n    user : bool\n       If set, configuration files will also be read from a\n       file called :file:`.cgat.yml` in the user`s\n       home directory.\n    only_import : bool\n       If set to a boolean, the parameter dictionary will be a\n       defaultcollection. This is useful for pipelines that are\n       imported (for example for documentation generation) but not\n       executed as there might not be an appropriate .yml file\n       available. If `only_import` is None, it will be set to the\n       default, which is to raise an exception unless the calling\n       script is imported or the option ``--is-test`` has been passed\n       at the command line.\n\n    Returns\n    -------\n    params : dict\n       Global configuration dictionary.\n    '''\n    global PARAMS, HAVE_INITIALIZED\n    # only execute function once\n    if HAVE_INITIALIZED:\n        return PARAMS\n\n    if filenames is None:\n        filenames = [\"pipeline.yml\", \"cgat.yml\"]\n    elif isinstance(filenames, str):\n        filenames = [filenames]\n\n    old_id = id(PARAMS)\n\n    caller_locals = get_caller_locals()\n\n    # check if this is only for import\n    if only_import is None:\n        only_import = is_test() or \"__name__\" not in caller_locals or \\\n            caller_locals[\"__name__\"] != \"__main__\"\n\n    # important: only update the PARAMS variable as\n    # it is referenced in other modules. Thus the type\n    # needs to be fixed at import. Raise error where this\n    # is not the case.\n    # Note: Parameter sharing in the pipeline module needs\n    # to be reorganized.\n    if only_import:\n        # turn on default dictionary\n        TriggeredDefaultFactory.with_default = True\n\n    # check if the pipeline is in testing mode\n    found = False\n    if 'argv' in caller_locals and caller_locals['argv'] is not None:\n        for e in caller_locals['argv']:\n            if 'template_pipeline.py' in e:\n                found = True\n    PARAMS['testing'] = 'self' in caller_locals or found\n\n    if site_ini:\n        # read configuration from /etc/cgat/pipeline.yml\n        fn = \"/etc/cgat/pipeline.yml\"\n        if os.path.exists(fn):\n            filenames.insert(0, fn)\n\n    if user:\n        # read configuration from a users home directory\n        fn = os.path.join(os.path.expanduser(\"~\"),\n                          \".cgat.yml\")\n        if os.path.exists(fn):\n            if 'pipeline.yml' in filenames:\n                filenames.insert(filenames.index('pipeline.yml'), fn)\n            else:\n                filenames.append(fn)\n\n    filenames = [x.strip() for x in filenames if os.path.exists(x)]\n\n    # save list of config files\n    PARAMS[\"pipeline_yml\"] = filenames\n\n    # update with hard-coded PARAMS\n    nested_update(PARAMS, HARDCODED_PARAMS)\n    if defaults:\n        nested_update(PARAMS, defaults)\n\n    # reset working directory. Set in PARAMS to prevent repeated calls to\n    # os.getcwd() failing if network is busy\n    PARAMS[\"start_dir\"] = os.path.abspath(os.getcwd())\n    # location of pipelines - set via location of top frame (cgatflow command)\n    if '__file__' in caller_locals:\n        PARAMS[\"pipelinedir\"] = os.path.dirname(caller_locals[\"__file__\"])\n    else:\n        PARAMS[\"pipelinedir\"] = 'unknown'\n\n    for filename in filenames:\n        if not os.path.exists(filename):\n            continue\n        get_logger().info(\"reading config from file {}\".format(\n            filename))\n\n        with open(filename, 'rt', encoding='utf8') as inf:\n            p = yaml.load(inf, Loader=yaml.FullLoader)\n            if p:\n                nested_update(PARAMS, p)\n\n    # for backwards compatibility - normalize dictionaries\n    p = {}\n    for k, v in PARAMS.items():\n        if isinstance(v, Mapping):\n            for kk, vv in v.items():\n                new_key = \"{}_{}\".format(k, kk)\n                if new_key in p:\n                    raise ValueError(\n                        \"key {} does already exist\".format(new_key))\n                p[new_key] = vv\n    nested_update(PARAMS, p)\n\n    # interpolate some params with other parameters\n    for param in INTERPOLATE_PARAMS:\n        try:\n            PARAMS[param] = PARAMS[param] % PARAMS\n        except TypeError as msg:\n            raise TypeError('could not interpolate %s: %s' %\n                            (PARAMS[param], msg))\n\n    # expand directory pathnames\n    for param, value in list(PARAMS.items()):\n        if (param.endswith(\"dir\") and isinstance(value, str) and value.startswith(\".\")):\n            PARAMS[param] = os.path.abspath(value)\n\n    # make sure that the dictionary reference has not changed\n    assert id(PARAMS) == old_id\n    HAVE_INITIALIZED = True\n    return PARAMS\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.get_parameters_as_namedtuple","title":"<code>get_parameters_as_namedtuple(*args, **kwargs)</code>","text":"<p>return PARAM dictionary as a namedtuple.</p> Source code in <code>cgatcore/pipeline/parameters.py</code> <pre><code>def get_parameters_as_namedtuple(*args, **kwargs):\n    \"\"\"return PARAM dictionary as a namedtuple.\n    \"\"\"\n    d = get_parameters(*args, **kwargs)\n    return collections.namedtuple('GenericDict', list(d.keys()))(**d)\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.get_params","title":"<code>get_params()</code>","text":"<p>return handle to global parameter dictionary</p> Source code in <code>cgatcore/pipeline/parameters.py</code> <pre><code>def get_params():\n    \"\"\"return handle to global parameter dictionary\"\"\"\n    return PARAMS\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.get_s3_pipeline","title":"<code>get_s3_pipeline()</code>","text":"<p>Instantiate and return the S3Pipeline instance, lazy-loaded to avoid circular imports.</p> Source code in <code>cgatcore/pipeline/__init__.py</code> <pre><code>def get_s3_pipeline():\n    \"\"\"Instantiate and return the S3Pipeline instance, lazy-loaded to avoid circular imports.\"\"\"\n    # Use get_remote() to access the remote functionality\n    remote = cgatcore.get_remote()  # Now properly calls the method to initialize remote if needed\n    return remote.file_handler.S3Pipeline()\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.get_temp_dir","title":"<code>get_temp_dir(dir=None, shared=False, clear=False)</code>","text":"<p>get a temporary directory.</p> <p>The directory is created and the caller needs to delete the temporary directory once it is not used any more.</p> <p>If dir does not exist, it will be created.</p>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.get_temp_dir--arguments","title":"Arguments","text":"<p>dir : string     Directory of the temporary directory and if not given is set to the     default temporary location in the global configuration dictionary. shared : bool     If set, the tempory directory will be in a shared temporary     location.</p>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.get_temp_dir--returns","title":"Returns","text":"<p>filename : string     Absolute pathname of temporary file.</p> Source code in <code>cgatcore/pipeline/files.py</code> <pre><code>def get_temp_dir(dir=None, shared=False, clear=False):\n    '''get a temporary directory.\n\n    The directory is created and the caller needs to delete the temporary\n    directory once it is not used any more.\n\n    If dir does not exist, it will be created.\n\n    Arguments\n    ---------\n    dir : string\n        Directory of the temporary directory and if not given is set to the\n        default temporary location in the global configuration dictionary.\n    shared : bool\n        If set, the tempory directory will be in a shared temporary\n        location.\n\n    Returns\n    -------\n    filename : string\n        Absolute pathname of temporary file.\n\n    '''\n    if dir is None:\n        if shared:\n            dir = get_params()['shared_tmpdir']\n        else:\n            dir = get_params()['tmpdir']\n\n    if not os.path.exists(dir):\n        os.makedirs(dir)\n\n    tmpdir = tempfile.mkdtemp(dir=dir, prefix=\"ctmp\")\n    if clear:\n        os.rmdir(tmpdir)\n    return tmpdir\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.get_temp_file","title":"<code>get_temp_file(dir=None, shared=False, suffix='', mode='w+', encoding='utf-8')</code>","text":"<p>get a temporary file.</p> <p>The file is created and the caller needs to close and delete the temporary file once it is not used any more. By default, the file is opened as a text file (mode <code>w+</code>) with encoding <code>utf-8</code> instead of the default mode <code>w+b</code> used in :class:<code>tempfile.NamedTemporaryFile</code></p> <p>If dir does not exist, it will be created.</p>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.get_temp_file--arguments","title":"Arguments","text":"<p>dir : string     Directory of the temporary file and if not given is set to the     default temporary location in the global configuration dictionary. shared : bool     If set, the tempory file will be in a shared temporary     location (given by the global configuration directory). suffix : string     Filename suffix</p>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.get_temp_file--returns","title":"Returns","text":"<p>file : File     A file object of the temporary file.</p> Source code in <code>cgatcore/pipeline/files.py</code> <pre><code>def get_temp_file(dir=None, shared=False, suffix=\"\", mode=\"w+\", encoding=\"utf-8\"):\n    '''get a temporary file.\n\n    The file is created and the caller needs to close and delete the\n    temporary file once it is not used any more. By default, the file\n    is opened as a text file (mode ``w+``) with encoding ``utf-8``\n    instead of the default mode ``w+b`` used in\n    :class:`tempfile.NamedTemporaryFile`\n\n    If dir does not exist, it will be created.\n\n    Arguments\n    ---------\n    dir : string\n        Directory of the temporary file and if not given is set to the\n        default temporary location in the global configuration dictionary.\n    shared : bool\n        If set, the tempory file will be in a shared temporary\n        location (given by the global configuration directory).\n    suffix : string\n        Filename suffix\n\n    Returns\n    -------\n    file : File\n        A file object of the temporary file.\n\n    '''\n    if dir is None:\n        if shared:\n            dir = get_params()['shared_tmpdir']\n        else:\n            dir = get_params()['tmpdir']\n\n    if not os.path.exists(dir):\n        try:\n            os.makedirs(dir)\n        except OSError:\n            # avoid race condition when several processes try to create\n            # temporary directory.\n            pass\n        if not os.path.exists(dir):\n            raise OSError(\n                \"temporary directory {} could not be created\".format(dir))\n\n    return tempfile.NamedTemporaryFile(dir=dir, delete=False, prefix=\"ctmp\",\n                                       mode=mode,\n                                       encoding=encoding, suffix=suffix)\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.get_temp_filename","title":"<code>get_temp_filename(dir=None, shared=False, clear=True, suffix='')</code>","text":"<p>return a temporary filename.</p> <p>The file is created and the caller needs to delete the temporary file once it is not used any more (unless <code>clear</code> is set`).</p> <p>If dir does not exist, it will be created.</p>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.get_temp_filename--arguments","title":"Arguments","text":"<p>dir : string     Directory of the temporary file and if not given is set to the     default temporary location in the global configuration dictionary. shared : bool     If set, the tempory file will be in a shared temporary     location. clear : bool     If set, remove the temporary file after creation. suffix : string     Filename suffix</p>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.get_temp_filename--returns","title":"Returns","text":"<p>filename : string     Absolute pathname of temporary file.</p> Source code in <code>cgatcore/pipeline/files.py</code> <pre><code>def get_temp_filename(dir=None, shared=False, clear=True, suffix=\"\"):\n    '''return a temporary filename.\n\n    The file is created and the caller needs to delete the temporary\n    file once it is not used any more (unless `clear` is set`).\n\n    If dir does not exist, it will be created.\n\n    Arguments\n    ---------\n    dir : string\n        Directory of the temporary file and if not given is set to the\n        default temporary location in the global configuration dictionary.\n    shared : bool\n        If set, the tempory file will be in a shared temporary\n        location.\n    clear : bool\n        If set, remove the temporary file after creation.\n    suffix : string\n        Filename suffix\n\n    Returns\n    -------\n    filename : string\n        Absolute pathname of temporary file.\n\n    '''\n    tmpfile = get_temp_file(dir=dir, shared=shared, suffix=suffix)\n    tmpfile.close()\n    if clear:\n        os.unlink(tmpfile.name)\n    return tmpfile.name\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.initialize","title":"<code>initialize(argv=None, caller=None, defaults=None, optparse=True, **kwargs)</code>","text":"<p>setup the pipeline framework.</p>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.initialize--arguments","title":"Arguments","text":"<p>options: object     Container for command line arguments. args : list     List of command line arguments. defaults : dictionary     Dictionary with default values to be added to global     parameters dictionary.</p> <p>Additional keyword arguments will be passed to the :func:<code>~.parse_commandline</code> function to set command-line defaults.</p> Source code in <code>cgatcore/pipeline/control.py</code> <pre><code>def initialize(argv=None, caller=None, defaults=None, optparse=True, **kwargs):\n    \"\"\"setup the pipeline framework.\n\n    Arguments\n    ---------\n    options: object\n        Container for command line arguments.\n    args : list\n        List of command line arguments.\n    defaults : dictionary\n        Dictionary with default values to be added to global\n        parameters dictionary.\n\n    Additional keyword arguments will be passed to the\n    :func:`~.parse_commandline` function to set command-line defaults.\n\n    \"\"\"\n    if argv is None:\n        argv = sys.argv\n\n    # load default options from config files\n    if caller:\n        path = os.path.splitext(caller)[0]\n    else:\n        try:\n            path = os.path.splitext(get_caller().__file__)[0]\n        except AttributeError as ex:\n            path = \"unknown\"\n\n    parse_commandline(argv, optparse, **kwargs)\n    args = E.get_args()\n    get_parameters(\n        [os.path.join(path, \"pipeline.yml\"),\n         \"../pipeline.yml\",\n         args.config_file],\n        defaults=defaults)\n\n    logger = logging.getLogger(\"cgatcore.pipeline\")\n    logger.info(\"started in directory: {}\".format(\n        get_params().get(\"start_dir\")))\n\n    # At this point, the PARAMS dictionary has already been\n    # built. It now needs to be updated with selected command\n    # line options as these should always take precedence over\n    # configuration files.\n    update_params_with_commandline_options(get_params(), args)\n\n    logger.info(get_header())\n\n    logger.info(get_param_output(get_params()))\n\n    code_location, version = get_version()\n    logger.info(\"code location: {}\".format(code_location))\n    logger.info(\"code version: {}\".format(version))\n\n    logger.info(\"working directory is: {}\".format(\n        get_params().get(\"work_dir\")))\n    work_dir = get_params().get(\"work_dir\")\n    if not os.path.exists(work_dir):\n        E.info(\"working directory {} does not exist - creating\".format(work_dir))\n        os.makedirs(work_dir)\n    logger.info(\"changing directory to {}\".format(work_dir))\n    os.chdir(work_dir)\n\n    logger.info(\"pipeline has been initialized\")\n\n    return args\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.input_validation","title":"<code>input_validation(PARAMS, pipeline_script='')</code>","text":"<p>Inspects the PARAMS dictionary looking for problematic input values.</p> <p>So far we just check that:</p> <pre><code>* all required 3rd party tools are on the PATH\n\n* input parameters are not empty\n\n* input parameters do not contain the \"?\" character (used as a\n  placeholder in different pipelines)\n\n* if the input is a file, check whether it exists and\n  is readable\n</code></pre> Source code in <code>cgatcore/pipeline/parameters.py</code> <pre><code>def input_validation(PARAMS, pipeline_script=\"\"):\n    '''Inspects the PARAMS dictionary looking for problematic input values.\n\n    So far we just check that:\n\n        * all required 3rd party tools are on the PATH\n\n        * input parameters are not empty\n\n        * input parameters do not contain the \"?\" character (used as a\n          placeholder in different pipelines)\n\n        * if the input is a file, check whether it exists and\n          is readable\n    '''\n\n    E.info('''input Validation starting''')\n    E.info('''checking 3rd party dependencies''')\n\n    # check 3rd party dependencies\n    if len(pipeline_script) &gt; 0:\n        # this import requires the PYTHONPATH in the following order\n        # PYTHONPATH=&lt;src&gt;/CGATpipelines:&lt;src&gt;/cgat\n        import scripts.cgat_check_deps as cd\n        deps, check_path_failures = cd.checkDepedencies(pipeline_script)\n        # print info about dependencies\n        if len(deps) == 0:\n            E.info('no dependencies found')\n        else:\n            # print dictionary ordered by value\n            for k in sorted(deps, key=deps.get, reverse=True):\n                E.info('Program: {0!s} used {1} time(s)'.format(k, deps[k]))\n            n_failures = len(check_path_failures)\n            if n_failures == 0:\n                E.info('All required programs are available on your PATH')\n            else:\n                E.info('The following programs are not on your PATH')\n                for p in check_path_failures:\n                    E.info('{0!s}'.format(p))\n\n    # check PARAMS\n    num_missing = 0\n    num_questions = 0\n\n    E.info('''checking pipeline configuration''')\n\n    for key, value in sorted(PARAMS.items()):\n\n        key = str(key)\n        value = str(value)\n\n        # check for missing values\n        if value == \"\":\n            E.warn('\\n\"{}\" is empty, is that expected?'.format(key))\n            num_missing += 1\n\n        # check for a question mark in the dictironary (indicates\n        # that there is a missing input parameter)\n        if \"?\" in value:\n            E.warn('\\n\"{}\" is not defined (?), is that expected?'.format(key))\n            num_questions += 1\n\n        # validate input files listed in PARAMS\n        if (value.startswith(\"/\") or value.endswith(\".gz\") or value.endswith(\".gtf\")) and \",\" not in value:\n            if not os.access(value, os.R_OK):\n                E.warn('\\n\"{}\": \"{}\" is not readable'.format(key, value))\n\n    if num_missing or num_questions:\n        raise ValueError(\"pipeline has configuration issues\")\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.interpolate_statement","title":"<code>interpolate_statement(statement, kwargs)</code>","text":"<p>interpolate command line statement with parameters</p> <p>The skeleton of the statement should be defined in kwargs.  The method then applies string interpolation using a dictionary built from the global configuration dictionary PARAMS, but augmented by <code>kwargs</code>. The latter takes precedence.</p>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.interpolate_statement--arguments","title":"Arguments","text":"<p>statement: string     Command line statement to be interpolated. kwargs : dict     Keyword arguments that are used for parameter interpolation.</p>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.interpolate_statement--returns","title":"Returns","text":"<p>statement : string     The command line statement with interpolated parameters.</p>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.interpolate_statement--raises","title":"Raises","text":"<p>KeyError     If <code>statement</code> contains unresolved references.</p> Source code in <code>cgatcore/pipeline/execution.py</code> <pre><code>def interpolate_statement(statement, kwargs):\n    '''interpolate command line statement with parameters\n\n    The skeleton of the statement should be defined in kwargs.  The\n    method then applies string interpolation using a dictionary built\n    from the global configuration dictionary PARAMS, but augmented by\n    `kwargs`. The latter takes precedence.\n\n    Arguments\n    ---------\n    statement: string\n        Command line statement to be interpolated.\n    kwargs : dict\n        Keyword arguments that are used for parameter interpolation.\n\n    Returns\n    -------\n    statement : string\n        The command line statement with interpolated parameters.\n\n    Raises\n    ------\n    KeyError\n        If ``statement`` contains unresolved references.\n\n    '''\n\n    local_params = substitute_parameters(**kwargs)\n\n    # build the statement\n    try:\n        statement = statement % local_params\n    except KeyError as msg:\n        raise KeyError(\n            \"Error when creating command: could not \"\n            \"find %s in dictionaries\" % msg)\n    except ValueError as msg:\n        raise ValueError(\n            \"Error when creating command: %s, statement = %s\" % (\n                msg, statement))\n\n    # cleaning up of statement\n    # remove new lines and superfluous spaces and tabs\n    statement = \" \".join(re.sub(\"\\t+\", \" \", statement).split(\"\\n\")).strip()\n    if statement.endswith(\";\"):\n        statement = statement[:-1]\n\n    # mark arvados mount points in statement\n    if get_params().get(\"mount_point\", None):\n        statement = re.sub(get_params()[\"mount_point\"], \"arv=\", statement)\n\n    return statement\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.is_test","title":"<code>is_test()</code>","text":"<p>return True if the pipeline is run in a \"testing\" mode.</p> <p>This method checks if <code>-is-test</code> has been given as a command line option.</p> Source code in <code>cgatcore/pipeline/utils.py</code> <pre><code>def is_test():\n    \"\"\"return True if the pipeline is run in a \"testing\" mode.\n\n    This method checks if ``-is-test`` has been given as a\n    command line option.\n    \"\"\"\n    return \"--is-test\" in sys.argv\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.is_true","title":"<code>is_true(param, **kwargs)</code>","text":"<p>return True if param has a True value.</p> <p>A parameter is False if it is:</p> <ul> <li>not set</li> <li>0</li> <li>the empty string</li> <li>false or False</li> </ul> <p>Otherwise the value is True.</p>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.is_true--arguments","title":"Arguments","text":"<p>param : string     Parameter to be tested kwargs : dict     Dictionary of local configuration values. These will be passed     to :func:<code>substitute_parameters</code> before evaluating <code>param</code></p>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.is_true--returns","title":"Returns","text":"<p>bool</p> Source code in <code>cgatcore/pipeline/parameters.py</code> <pre><code>def is_true(param, **kwargs):\n    '''return True if param has a True value.\n\n    A parameter is False if it is:\n\n    * not set\n    * 0\n    * the empty string\n    * false or False\n\n    Otherwise the value is True.\n\n    Arguments\n    ---------\n    param : string\n        Parameter to be tested\n    kwargs : dict\n        Dictionary of local configuration values. These will be passed\n        to :func:`substitute_parameters` before evaluating `param`\n\n    Returns\n    -------\n    bool\n\n    '''\n    if kwargs:\n        p = substitute_parameters(**kwargs)\n    else:\n        p = PARAMS\n    value = p.get(param, 0)\n    return value not in (0, '', 'false', 'False')\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.join_statements","title":"<code>join_statements(statements, infile, outfile=None)</code>","text":"<p>join a chain of statements into a single statement.</p> <p>Each statement contains an @IN@ or a @OUT@ placeholder or both. These will be replaced by the names of successive temporary files.</p> <p>In the first statement, @IN@ is replaced with <code>infile</code> and, if given, the @OUT@ is replaced by outfile in the last statement.</p>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.join_statements--arguments","title":"Arguments","text":"<p>statements : list     A list of command line statements. infile : string     Filename of the first data set. outfile : string     Filename of the target data set.</p>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.join_statements--returns","title":"Returns","text":"<p>last_file : string     Filename of last file created, outfile, if given. statement : string     A command line statement built from merging the statements cleanup : string     A command line statement for cleaning up.</p> Source code in <code>cgatcore/pipeline/execution.py</code> <pre><code>def join_statements(statements, infile, outfile=None):\n    '''join a chain of statements into a single statement.\n\n    Each statement contains an @IN@ or a @OUT@ placeholder or both.\n    These will be replaced by the names of successive temporary files.\n\n    In the first statement, @IN@ is replaced with `infile` and, if given,\n    the @OUT@ is replaced by outfile in the last statement.\n\n    Arguments\n    ---------\n    statements : list\n        A list of command line statements.\n    infile : string\n        Filename of the first data set.\n    outfile : string\n        Filename of the target data set.\n\n    Returns\n    -------\n    last_file : string\n        Filename of last file created, outfile, if given.\n    statement : string\n        A command line statement built from merging the statements\n    cleanup : string\n        A command line statement for cleaning up.\n\n    '''\n\n    prefix = get_temp_filename()\n\n    pattern = \"%s_%%i\" % prefix\n\n    result = []\n    for x, statement in enumerate(statements):\n        s = statement\n        if x == 0:\n            if infile is not None:\n                s = re.sub(\"@IN@\", infile, s)\n        else:\n            s = re.sub(\"@IN@\", pattern % x, s)\n            if x &gt; 2:\n                s = re.sub(\"@IN-2@\", pattern % (x - 2), s)\n            if x &gt; 1:\n                s = re.sub(\"@IN-1@\", pattern % (x - 1), s)\n\n        s = re.sub(\"@OUT@\", pattern % (x + 1), s).strip()\n\n        if s.endswith(\";\"):\n            s = s[:-1]\n        result.append(s)\n\n    result = \"; \".join(result)\n    last_file = pattern % (x + 1)\n    if outfile:\n        result = re.sub(last_file, outfile, result)\n        last_file = outfile\n\n    assert prefix != \"\"\n    return last_file, result, \"rm -f %s*\" % prefix\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.load","title":"<code>load(infile, outfile=None, options='', collapse=False, transpose=False, tablename=None, retry=True, limit=0, shuffle=False, job_memory=None, to_cluster=False)</code>","text":"<p>import data from a tab-separated file into database.</p> <p>The table name is given by outfile without the \".load\" suffix.</p> <p>A typical load task in ruffus would look like this::</p> <pre><code>@transform(\"*.tsv.gz\", suffix(\".tsv.gz\"), \".load\")\ndef loadData(infile, outfile):\n    P.load(infile, outfile)\n</code></pre> <p>Upload is performed via the :doc:<code>csv2db</code> script.</p>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.load--arguments","title":"Arguments","text":"<p>infile : string     Filename of the input data outfile : string     Output filename. This will contain the logging information. The     table name is derived from <code>outfile</code> if <code>tablename</code> is not set. options : string     Command line options for the <code>csv2db.py</code> script. collapse : string     If set, the table will be collapsed before loading. This     transforms a data set with two columns where the first column     is the row name into a multi-column table.  The value of     collapse is the value used for missing values. transpose : string     If set, the table will be transposed before loading. The first     column in the first row will be set to the string within     transpose. retry : bool     If True, multiple attempts will be made if the data can     not be loaded at the first try, for example if a table is locked. limit : int     If set, only load the first n lines. shuffle : bool     If set, randomize lines before loading. Together with <code>limit</code>     this permits loading a sample of rows. job_memory : string     Amount of memory to allocate for job. If unset, uses the global     default. Implies to_cluster=True. to_cluster : bool     By default load jobs are not submitted to the cluster as they sometimes     become blocked. Setting this true will override this behavoir.</p> Source code in <code>cgatcore/pipeline/database.py</code> <pre><code>def load(infile,\n         outfile=None,\n         options=\"\",\n         collapse=False,\n         transpose=False,\n         tablename=None,\n         retry=True,\n         limit=0,\n         shuffle=False,\n         job_memory=None,\n         to_cluster=False):\n    \"\"\"import data from a tab-separated file into database.\n\n    The table name is given by outfile without the\n    \".load\" suffix.\n\n    A typical load task in ruffus would look like this::\n\n        @transform(\"*.tsv.gz\", suffix(\".tsv.gz\"), \".load\")\n        def loadData(infile, outfile):\n            P.load(infile, outfile)\n\n    Upload is performed via the :doc:`csv2db` script.\n\n    Arguments\n    ---------\n    infile : string\n        Filename of the input data\n    outfile : string\n        Output filename. This will contain the logging information. The\n        table name is derived from `outfile` if `tablename` is not set.\n    options : string\n        Command line options for the `csv2db.py` script.\n    collapse : string\n        If set, the table will be collapsed before loading. This\n        transforms a data set with two columns where the first column\n        is the row name into a multi-column table.  The value of\n        collapse is the value used for missing values.\n    transpose : string\n        If set, the table will be transposed before loading. The first\n        column in the first row will be set to the string within\n        transpose.\n    retry : bool\n        If True, multiple attempts will be made if the data can\n        not be loaded at the first try, for example if a table is locked.\n    limit : int\n        If set, only load the first n lines.\n    shuffle : bool\n        If set, randomize lines before loading. Together with `limit`\n        this permits loading a sample of rows.\n    job_memory : string\n        Amount of memory to allocate for job. If unset, uses the global\n        default. Implies to_cluster=True.\n    to_cluster : bool\n        By default load jobs are not submitted to the cluster as they sometimes\n        become blocked. Setting this true will override this behavoir.\n    \"\"\"\n\n    if job_memory is None:\n        job_memory = get_params()[\"cluster_memory_default\"]\n\n    if not tablename:\n        tablename = to_table(outfile)\n\n    statement = []\n\n    if infile.endswith(\".gz\"):\n        statement.append(\"zcat %(infile)s\")\n    else:\n        statement.append(\"cat %(infile)s\")\n\n    if collapse:\n        statement.append(\n            \"python -m cgatcore.table \"\n            \"--log=%(outfile)s.collapse.log \"\n            \"--collapse=%(collapse)s\")\n\n    if transpose:\n        statement.append(\n            \"python -m cgatcore.table \"\n            \"--log=%(outfile)s.transpose.log \"\n            \"--transpose \"\n            \"--set-transpose-field=%(transpose)s\")\n\n    if shuffle:\n        statement.append(\n            \"python -m cgatcore.table \"\n            \"--log=%(outfile)s.shuffle.log \"\n            \"--method=randomize-rows\")\n\n    if limit &gt; 0:\n        # use awk to filter in order to avoid a pipeline broken error from head\n        statement.append(\"awk 'NR &gt; %i {exit(0)} {print}'\" % (limit + 1))\n        # ignore errors from cat or zcat due to broken pipe\n        ignore_pipe_errors = True\n\n    statement.append(build_load_statement(tablename,\n                                          options=options,\n                                          retry=retry))\n\n    statement = \" | \".join(statement) + \" &gt; %(outfile)s\"\n\n    run(statement)\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.load_from_iterator","title":"<code>load_from_iterator(outfile, tablename, iterator, columns=None, indices=None)</code>","text":"<p>import data from an iterator into a database.</p>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.load_from_iterator--arguments","title":"Arguments","text":"<p>outfile : string     Output file name tablename : string     Table name iterator : iterator     Iterator to import data from. The iterator should     yield either list/tuples or dictionaries for each     row in the table. columns : list     Column names. If not given, the assumption is that     iterator will dictionaries and column names are derived     from that. indices : list     List of column names to add indices on.</p> Source code in <code>cgatcore/pipeline/database.py</code> <pre><code>def load_from_iterator(\n        outfile,\n        tablename,\n        iterator,\n        columns=None,\n        indices=None):\n    '''import data from an iterator into a database.\n\n    Arguments\n    ---------\n    outfile : string\n        Output file name\n    tablename : string\n        Table name\n    iterator : iterator\n        Iterator to import data from. The iterator should\n        yield either list/tuples or dictionaries for each\n        row in the table.\n    columns : list\n        Column names. If not given, the assumption is that\n        iterator will dictionaries and column names are derived\n        from that.\n    indices : list\n        List of column names to add indices on.\n    '''\n\n    tmpfile = get_temp_file(\".\")\n\n    if columns:\n        keys, values = list(zip(*list(columns.items())))\n        tmpfile.write(\"\\t\".join(values) + \"\\n\")\n\n    for row in iterator:\n        if not columns:\n            keys = list(row[0].keys())\n            values = keys\n            columns = keys\n            tmpfile.write(\"\\t\".join(values) + \"\\n\")\n\n        tmpfile.write(\"\\t\".join(str(row[x]) for x in keys) + \"\\n\")\n\n    tmpfile.close()\n\n    if indices:\n        indices = \" \".join(\"--add-index=%s\" % x for x in indices)\n    else:\n        indices = \"\"\n\n    load(tmpfile.name,\n         outfile,\n         tablename=tablename,\n         options=indices)\n\n    os.unlink(tmpfile.name)\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.main","title":"<code>main(argv=None)</code>","text":"<p>command line control function for a pipeline.</p> <p>This method defines command line options for the pipeline and updates the global configuration dictionary correspondingly.</p> <p>It then provides a command parser to execute particular tasks using the ruffus pipeline control functions. See the generated command line help for usage.</p> <p>To use it, add::</p> <pre><code>import CGAT.pipeline as P\n\nif __name__ == \"__main__\":\n    sys.exit(P.main(sys.argv))\n</code></pre> <p>to your pipeline script.</p>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.main--arguments","title":"Arguments","text":"<p>args : list     List of command line arguments.</p> Source code in <code>cgatcore/pipeline/control.py</code> <pre><code>def main(argv=None):\n    \"\"\"command line control function for a pipeline.\n\n    This method defines command line options for the pipeline and\n    updates the global configuration dictionary correspondingly.\n\n    It then provides a command parser to execute particular tasks\n    using the ruffus pipeline control functions. See the generated\n    command line help for usage.\n\n    To use it, add::\n\n        import CGAT.pipeline as P\n\n        if __name__ == \"__main__\":\n            sys.exit(P.main(sys.argv))\n\n    to your pipeline script.\n\n    Arguments\n    ---------\n    args : list\n        List of command line arguments.\n\n    \"\"\"\n\n    if argv is None:\n        argv = sys.argv\n\n    if E.get_args() is None:\n        initialize(caller=get_caller().__file__)\n\n    args = E.get_args()\n\n    run_workflow(args)\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.match_parameter","title":"<code>match_parameter(param)</code>","text":"<p>find an exact match or prefix-match in the global configuration dictionary param.</p>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.match_parameter--arguments","title":"Arguments","text":"<p>param : string     Parameter to search for.</p>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.match_parameter--returns","title":"Returns","text":"<p>name : string     The full parameter name.</p>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.match_parameter--raises","title":"Raises","text":"<p>KeyError if param can't be matched.</p> Source code in <code>cgatcore/pipeline/parameters.py</code> <pre><code>def match_parameter(param):\n    '''find an exact match or prefix-match in the global\n    configuration dictionary param.\n\n    Arguments\n    ---------\n    param : string\n        Parameter to search for.\n\n    Returns\n    -------\n    name : string\n        The full parameter name.\n\n    Raises\n    ------\n    KeyError if param can't be matched.\n\n    '''\n    if param in PARAMS:\n        return param\n\n    for key in list(PARAMS.keys()):\n        if \"%\" in key:\n            rx = re.compile(re.sub(\"%\", \".*\", key))\n            if rx.search(param):\n                return key\n\n    raise KeyError(\"parameter '%s' can not be matched in dictionary\" %\n                   param)\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.merge_and_load","title":"<code>merge_and_load(infiles, outfile, suffix=None, columns=(0, 1), regex=None, row_wise=True, retry=True, options='', prefixes=None)</code>","text":"<p>merge multiple categorical tables and load into a database.</p> <p>The tables are merged and entered row-wise, i.e, the contents of each file are a row.</p> <p>For example, the statement::</p> <pre><code>mergeAndLoad(['file1.txt', 'file2.txt'],\n             \"test_table.load\")\n</code></pre> <p>with the two files::     &gt; cat file1.txt     Category    Result     length      12     width       100</p> <pre><code>&gt; cat file2.txt\nCategory    Result\nlength      20\nwidth       50\n</code></pre> <p>will be added into table <code>test_table</code> as::     track   length   width     file1   12       100     file2   20       50</p> <p>If row-wise is set::     mergeAndLoad(['file1.txt', 'file2.txt'],                  \"test_table.load\", row_wise=True)</p> <p><code>test_table</code> will be transposed and look like this::     track    file1 file2     length   12    20     width    20    50</p>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.merge_and_load--arguments","title":"Arguments","text":"<p>infiles : list     Filenames of the input data outfile : string     Output filename. This will contain the logging information. The     table name is derived from <code>outfile</code>. suffix : string     If <code>suffix</code> is given, the suffix will be removed from the filenames. columns : list     The columns to be taken. By default, the first two columns are     taken with the first being the key. Filenames are stored in a     <code>track</code> column. Directory names are chopped off.  If     <code>columns</code> is set to None, all columns will be taken. Here,     column names will receive a prefix given by <code>prefixes</code>. If     <code>prefixes</code> is None, the filename will be added as a prefix. regex : string     If set, the full filename will be used to extract a     track name via the supplied regular expression. row_wise : bool     If set to False, each table will be a column in the resulting     table.  This is useful if histograms are being merged. retry : bool     If True, multiple attempts will be made if the data can     not be loaded at the first try, for example if a table is locked. options : string     Command line options for the <code>csv2db.py</code> script. prefixes : list     If given, the respective prefix will be added to each     column. The number of <code>prefixes</code> and <code>infiles</code> needs to be the     same.</p> Source code in <code>cgatcore/pipeline/database.py</code> <pre><code>def merge_and_load(infiles,\n                   outfile,\n                   suffix=None,\n                   columns=(0, 1),\n                   regex=None,\n                   row_wise=True,\n                   retry=True,\n                   options=\"\",\n                   prefixes=None):\n    '''merge multiple categorical tables and load into a database.\n\n    The tables are merged and entered row-wise, i.e, the contents of\n    each file are a row.\n\n    For example, the statement::\n\n        mergeAndLoad(['file1.txt', 'file2.txt'],\n                     \"test_table.load\")\n\n    with the two files::\n        &gt; cat file1.txt\n        Category    Result\n        length      12\n        width       100\n\n        &gt; cat file2.txt\n        Category    Result\n        length      20\n        width       50\n\n    will be added into table ``test_table`` as::\n        track   length   width\n        file1   12       100\n        file2   20       50\n\n    If row-wise is set::\n        mergeAndLoad(['file1.txt', 'file2.txt'],\n                     \"test_table.load\", row_wise=True)\n\n    ``test_table`` will be transposed and look like this::\n        track    file1 file2\n        length   12    20\n        width    20    50\n\n    Arguments\n    ---------\n    infiles : list\n        Filenames of the input data\n    outfile : string\n        Output filename. This will contain the logging information. The\n        table name is derived from `outfile`.\n    suffix : string\n        If `suffix` is given, the suffix will be removed from the filenames.\n    columns : list\n        The columns to be taken. By default, the first two columns are\n        taken with the first being the key. Filenames are stored in a\n        ``track`` column. Directory names are chopped off.  If\n        `columns` is set to None, all columns will be taken. Here,\n        column names will receive a prefix given by `prefixes`. If\n        `prefixes` is None, the filename will be added as a prefix.\n    regex : string\n        If set, the full filename will be used to extract a\n        track name via the supplied regular expression.\n    row_wise : bool\n        If set to False, each table will be a column in the resulting\n        table.  This is useful if histograms are being merged.\n    retry : bool\n        If True, multiple attempts will be made if the data can\n        not be loaded at the first try, for example if a table is locked.\n    options : string\n        Command line options for the `csv2db.py` script.\n    prefixes : list\n        If given, the respective prefix will be added to each\n        column. The number of `prefixes` and `infiles` needs to be the\n        same.\n    '''\n    if len(infiles) == 0:\n        raise ValueError(\"no files for merging\")\n\n    if suffix:\n        header = \",\".join([os.path.basename(snip(x, suffix)) for x in infiles])\n    elif regex:\n        header = \",\".join([\"-\".join(re.search(regex, x).groups())\n                           for x in infiles])\n    else:\n        header = \",\".join([os.path.basename(x) for x in infiles])\n\n    header_stmt = \"--header-names=%s\" % header\n\n    if columns:\n        column_filter = \"| cut -f %s\" % \",\".join(map(str,\n                                                     [x + 1 for x in columns]))\n    else:\n        column_filter = \"\"\n        if prefixes:\n            assert len(prefixes) == len(infiles)\n            header_stmt = \"--prefixes=%s\" % \",\".join(prefixes)\n        else:\n            header_stmt = \"--add-file-prefix\"\n\n    if infiles[0].endswith(\".gz\"):\n        filenames = \" \".join(\n            [\"&lt;( zcat %s %s )\" % (x, column_filter) for x in infiles])\n    else:\n        filenames = \" \".join(\n            [\"&lt;( cat %s %s )\" % (x, column_filter) for x in infiles])\n\n    if row_wise:\n        transform = \"\"\"| perl -p -e \"s/bin/track/\"\n        | python -m cgatcore.table --transpose\"\"\"\n    else:\n        transform = \"\"\n\n    load_statement = build_load_statement(\n        to_table(outfile),\n        options=\"--add-index=track \" + options,\n        retry=retry)\n\n    statement = \"\"\"python -m cgatcore.tables\n    %(header_stmt)s\n    --skip-titles\n    --missing-value=0\n    --ignore-empty\n    %(filenames)s\n    %(transform)s\n    | %(load_statement)s\n    &gt; %(outfile)s\n    \"\"\"\n    run(statement)\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.nested_update","title":"<code>nested_update(old, new)</code>","text":"<p>Update potentially nested dictionaries. If both old[x] and new[x] inherit from collections.abc.Mapping, then update old[x] with entries from new[x], otherwise set old[x] to new[x]</p> Source code in <code>cgatcore/pipeline/parameters.py</code> <pre><code>def nested_update(old, new):\n    '''Update potentially nested dictionaries. If both old[x] and new[x]\n    inherit from collections.abc.Mapping, then update old[x] with entries from\n    new[x], otherwise set old[x] to new[x]'''\n\n    for key, value in new.items():\n        if isinstance(value, Mapping) and \\\n           isinstance(old.get(key, str()), Mapping):\n            old[key].update(new[key])\n        else:\n            old[key] = new[key]\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.parse_commandline","title":"<code>parse_commandline(argv=None, optparse=True, **kwargs)</code>","text":"<p>parse command line.</p> <p>Create option parser and parse command line.</p>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.parse_commandline--arguments","title":"Arguments","text":"<p>argv : list     List of command line options to parse. If None, use sys.argv.</p> <p>**kwargs: dict     Additional arguments overwrite default option settings.</p> Source code in <code>cgatcore/pipeline/control.py</code> <pre><code>def parse_commandline(argv=None, optparse=True, **kwargs):\n    \"\"\"parse command line.\n\n    Create option parser and parse command line.\n\n    Arguments\n    ---------\n    argv : list\n        List of command line options to parse. If None, use sys.argv.\n\n    **kwargs: dict\n        Additional arguments overwrite default option settings.\n\n    \"\"\"\n    if argv is None:\n        argv = sys.argv\n\n    if optparse is True:\n\n        parser = E.OptionParser(version=\"%prog version: $Id$\",\n                                usage=USAGE)\n\n        parser.add_option(\"--pipeline-action\", dest=\"pipeline_action\",\n                          type=\"choice\",\n                          choices=(\n                              \"make\", \"show\", \"plot\", \"dump\", \"config\",\n                              \"clone\", \"check\", \"regenerate\", \"state\",\n                              \"printconfig\", \"svg\"),\n                          help=\"action to take [default=%default].\")\n\n        parser.add_option(\"--pipeline-format\", dest=\"pipeline_format\",\n                          type=\"choice\",\n                          choices=(\"dot\", \"jpg\", \"svg\", \"ps\", \"png\"),\n                          help=\"pipeline format [default=%default].\")\n\n        parser.add_option(\"-n\", \"--dry-run\", dest=\"dry_run\",\n                          action=\"store_true\",\n                          help=\"perform a dry run (do not execute any shell \"\n                          \"commands) [default=%default].\")\n\n        parser.add_option(\"-c\", \"--config-file\", dest=\"config_file\",\n                          help=\"benchmark configuration file \"\n                          \"[default=%default].\")\n\n        parser.add_option(\"-f\", \"--force-run\", dest=\"force_run\",\n                          type=\"string\",\n                          help=\"force running the pipeline even if there are \"\n                          \"up-to-date tasks. If option is 'all', all tasks \"\n                          \"will be rerun. Otherwise, only the tasks given as \"\n                          \"arguments will be rerun. \"\n                          \"[default=%default].\")\n\n        parser.add_option(\"-p\", \"--multiprocess\", dest=\"multiprocess\", type=\"int\",\n                          help=\"number of parallel processes to use on \"\n                          \"submit host \"\n                          \"(different from number of jobs to use for \"\n                          \"cluster jobs) \"\n                          \"[default=%default].\")\n\n        parser.add_option(\"-e\", \"--exceptions\", dest=\"log_exceptions\",\n                          action=\"store_true\",\n                          help=\"echo exceptions immediately as they occur \"\n                          \"[default=%default].\")\n\n        parser.add_option(\"-i\", \"--terminate\", dest=\"terminate\",\n                          action=\"store_true\",\n                          help=\"terminate immediately at the first exception \"\n                          \"[default=%default].\")\n\n        parser.add_option(\"-d\", \"--debug\", dest=\"debug\",\n                          action=\"store_true\",\n                          help=\"output debugging information on console, \"\n                          \"and not the logfile \"\n                          \"[default=%default].\")\n\n        parser.add_option(\"-s\", \"--set\", dest=\"variables_to_set\",\n                          type=\"string\", action=\"append\",\n                          help=\"explicitely set paramater values \"\n                          \"[default=%default].\")\n\n        parser.add_option(\"--input-glob\", \"--input-glob\", dest=\"input_globs\",\n                          type=\"string\", action=\"append\",\n                          help=\"glob expression for input filenames. The exact format \"\n                          \"is pipeline specific. If the pipeline expects only a single input, \"\n                          \"`--input-glob=*.bam` will be sufficient. If the pipeline expects \"\n                          \"multiple types of input, a qualifier might need to be added, for example \"\n                          \"`--input-glob=bam=*.bam` --input-glob=bed=*.bed.gz`. Giving this option \"\n                          \"overrides the default of a pipeline looking for input in the current directory \"\n                          \"or specified the config file. \"\n                          \"[default=%default].\")\n\n        parser.add_option(\"--checksums\", dest=\"ruffus_checksums_level\",\n                          type=\"int\",\n                          help=\"set the level of ruffus checksums\"\n                          \"[default=%default].\")\n\n        parser.add_option(\"-t\", \"--is-test\", dest=\"is_test\",\n                          action=\"store_true\",\n                          help=\"this is a test run\"\n                          \"[default=%default].\")\n\n        parser.add_option(\"--engine\", dest=\"engine\",\n                          choices=(\"local\", \"arvados\"),\n                          help=\"engine to use.\"\n                          \"[default=%default].\")\n\n        parser.add_option(\n            \"--always-mount\", dest=\"always_mount\",\n            action=\"store_true\",\n            help=\"force mounting of arvados keep [%default]\")\n\n        parser.add_option(\"--only-info\", dest=\"only_info\",\n                          action=\"store_true\",\n                          help=\"only update meta information, do not run \"\n                          \"[default=%default].\")\n\n        parser.add_option(\"--work-dir\", dest=\"work_dir\",\n                          type=\"string\",\n                          help=\"working directory. Will be created if it does not exist \"\n                          \"[default=%default].\")\n\n        group = E.OptionGroup(parser, \"pipeline logging configuration\")\n\n        group.add_option(\"--pipeline-logfile\", dest=\"pipeline_logfile\",\n                         type=\"string\",\n                         help=\"primary logging destination.\"\n                         \"[default=%default].\")\n\n        group.add_option(\"--shell-logfile\", dest=\"shell_logfile\",\n                         type=\"string\",\n                         help=\"filename for shell debugging information. \"\n                         \"If it is not an absolute path, \"\n                         \"the output will be written into the current working \"\n                         \"directory. If unset, no logging will be output. \"\n                         \"[default=%default].\")\n\n        parser.add_option(\"--input-validation\", dest=\"input_validation\",\n                          action=\"store_true\",\n                          help=\"perform input validation before starting \"\n                          \"[default=%default].\")\n\n        parser.add_option_group(group)\n\n        parser.set_defaults(\n            pipeline_action=None,\n            pipeline_format=\"svg\",\n            pipeline_targets=[],\n            force_run=False,\n            multiprocess=None,\n            pipeline_logfile=\"pipeline.log\",\n            shell_logfile=None,\n            dry_run=False,\n            log_exceptions=True,\n            engine=\"local\",\n            exceptions_terminate_immediately=None,\n            debug=False,\n            variables_to_set=[],\n            is_test=False,\n            ruffus_checksums_level=0,\n            config_file=\"pipeline.yml\",\n            work_dir=None,\n            always_mount=False,\n            only_info=False,\n            input_globs=[],\n            input_validation=False)\n\n        parser.set_defaults(**kwargs)\n\n        if \"callback\" in kwargs:\n            kwargs[\"callback\"](parser)\n\n        logger_callback = setup_logging\n        (options, args) = E.start(\n            parser,\n            add_cluster_options=True,\n            argv=argv,\n            logger_callback=logger_callback)\n        options.pipeline_name = argv[0]\n        if args:\n            options.pipeline_action = args[0]\n            options.pipeline_targets = args[1:]\n\n    else:\n        parser = E.ArgumentParser(description=USAGE)\n\n        parser.add_argument(\"--pipeline-action\", dest=\"pipeline_action\",\n                            type=str,\n                            choices=(\n                                \"make\", \"show\", \"plot\", \"dump\", \"config\",\n                                \"clone\", \"check\", \"regenerate\", \"state\",\n                                \"printconfig\", \"svg\"),\n                            help=\"action to take.\")\n\n        parser.add_argument(\"--pipeline-format\", dest=\"pipeline_format\",\n                            type=str,\n                            choices=(\"dot\", \"jpg\", \"svg\", \"ps\", \"png\"),\n                            help=\"pipeline format.\")\n\n        parser.add_argument(\"-n\", \"--dry-run\", dest=\"dry_run\",\n                            action=\"store_true\",\n                            help=\"perform a dry run (do not execute any shell \"\n                            \"commands).\")\n\n        parser.add_argument(\"-c\", \"--config-file\", dest=\"config_file\",\n                            help=\"benchmark configuration file \")\n\n        parser.add_argument(\"-f\", \"--force-run\", dest=\"force_run\",\n                            type=str,\n                            help=\"force running the pipeline even if there are \"\n                            \"up-to-date tasks. If option is 'all', all tasks \"\n                            \"will be rerun. Otherwise, only the tasks given as \"\n                            \"arguments will be rerun. \")\n\n        parser.add_argument(\"-p\", \"--multiprocess\", dest=\"multiprocess\", type=int,\n                            help=\"number of parallel processes to use on \"\n                            \"submit host \"\n                            \"(different from number of jobs to use for \"\n                            \"cluster jobs) \")\n\n        parser.add_argument(\"-e\", \"--exceptions\", dest=\"log_exceptions\",\n                            action=\"store_true\",\n                            help=\"echo exceptions immediately as they occur \")\n\n        parser.add_argument(\"-i\", \"--terminate\", dest=\"terminate\",\n                            action=\"store_true\",\n                            help=\"terminate immediately at the first exception\")\n\n        parser.add_argument(\"-d\", \"--debug\", dest=\"debug\",\n                            action=\"store_true\",\n                            help=\"output debugging information on console, \"\n                            \"and not the logfile \")\n\n        parser.add_argument(\"-s\", \"--set\", dest=\"variables_to_set\",\n                            type=str, action=\"append\",\n                            help=\"explicitely set paramater values \")\n\n        parser.add_argument(\"--input-glob\", \"--input-glob\", dest=\"input_globs\",\n                            type=str, action=\"append\",\n                            help=\"glob expression for input filenames. The exact format \"\n                            \"is pipeline specific. If the pipeline expects only a single input, \"\n                            \"`--input-glob=*.bam` will be sufficient. If the pipeline expects \"\n                            \"multiple types of input, a qualifier might need to be added, for example \"\n                            \"`--input-glob=bam=*.bam` --input-glob=bed=*.bed.gz`. Giving this option \"\n                            \"overrides the default of a pipeline looking for input in the current directory \"\n                            \"or specified the config file.\")\n\n        parser.add_argument(\"--checksums\", dest=\"ruffus_checksums_level\",\n                            type=int,\n                            help=\"set the level of ruffus checksums\")\n\n        parser.add_argument(\"-t\", \"--is-test\", dest=\"is_test\",\n                            action=\"store_true\",\n                            help=\"this is a test run\")\n\n        parser.add_argument(\"--engine\", dest=\"engine\",\n                            type=str,\n                            choices=(\"local\", \"arvados\"),\n                            help=\"engine to use.\")\n\n        parser.add_argument(\n            \"--always-mount\", dest=\"always_mount\",\n            action=\"store_true\",\n            help=\"force mounting of arvados keep\")\n\n        parser.add_argument(\"--only-info\", dest=\"only_info\",\n                            action=\"store_true\",\n                            help=\"only update meta information, do not run\")\n\n        parser.add_argument(\"--work-dir\", dest=\"work_dir\",\n                            type=str,\n                            help=\"working directory. Will be created if it does not exist\")\n\n        parser.add_argument(\"--cleanup-on-fail\", action=\"store_true\", default=True,\n                            help=\"Enable cleanup of jobs on pipeline failure.\")\n\n        group = parser.add_argument_group(\"pipeline logging configuration\")\n\n        group.add_argument(\"--pipeline-logfile\", dest=\"pipeline_logfile\",\n                           type=str,\n                           help=\"primary logging destination.\")\n\n        group.add_argument(\"--shell-logfile\", dest=\"shell_logfile\",\n                           type=str,\n                           help=\"filename for shell debugging information. \"\n                           \"If it is not an absolute path, \"\n                           \"the output will be written into the current working \"\n                           \"directory. If unset, no logging will be output.\")\n\n        group.add_argument(\"--input-validation\", dest=\"input_validation\",\n                           action=\"store_true\",\n                           help=\"perform input validation before starting\")\n\n        parser.set_defaults(\n            pipeline_action=None,\n            pipeline_format=\"svg\",\n            pipeline_targets=[],\n            force_run=False,\n            multiprocess=None,\n            pipeline_logfile=\"pipeline.log\",\n            shell_logfile=None,\n            dry_run=False,\n            log_exceptions=True,\n            engine=\"local\",\n            exceptions_terminate_immediately=None,\n            debug=False,\n            variables_to_set=[],\n            is_test=False,\n            ruffus_checksums_level=0,\n            config_file=\"pipeline.yml\",\n            work_dir=None,\n            always_mount=False,\n            only_info=False,\n            input_globs=[],\n            input_validation=False)\n\n        parser.set_defaults(**kwargs)\n\n        if \"callback\" in kwargs:\n            kwargs[\"callback\"](parser)\n\n        logger_callback = setup_logging\n        args, unknown = E.start(\n            parser,\n            add_cluster_options=True,\n            argv=argv,\n            logger_callback=logger_callback,\n            unknowns=True)\n\n        args.pipeline_name = argv[0]\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.peek_parameters","title":"<code>peek_parameters(workingdir, pipeline, on_error_raise=None, prefix=None, update_interface=False, restrict_interface=False)</code>","text":"<p>peek configuration parameters from external pipeline.</p> <p>As the paramater dictionary is built at runtime, this method executes the pipeline in workingdir, dumping its configuration values and reading them into a dictionary.</p> <p>If either <code>pipeline</code> or <code>workingdir</code> are not found, an error is raised. This behaviour can be changed by setting <code>on_error_raise</code> to False. In that case, an empty dictionary is returned.</p>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.peek_parameters--arguments","title":"Arguments","text":"<p>workingdir : string    Working directory. This is the directory that the pipeline    was executed in. pipeline : string    Name of the pipeline script. The pipeline is assumed to live    in the same directory as the current pipeline. on_error_raise : Bool    If set to a boolean, an error will be raised (or not) if there    is an error during parameter peeking, for example if    <code>workingdir</code> can not be found. If <code>on_error_raise</code> is None, it    will be set to the default, which is to raise an exception    unless the calling script is imported or the option    <code>--is-test</code> has been passed at the command line. prefix : string    Add a prefix to all parameters. This is useful if the paramaters    are added to the configuration dictionary of the calling pipeline. update_interface : bool    If True, this method will prefix any options in the    <code>[interface]</code> section with <code>workingdir</code>. This allows    transparent access to files in the external pipeline. restrict_interface : bool    If  True, only interface parameters will be imported.</p>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.peek_parameters--returns","title":"Returns","text":"<p>config : dict     Dictionary of configuration values.</p> Source code in <code>cgatcore/pipeline/control.py</code> <pre><code>def peek_parameters(workingdir,\n                    pipeline,\n                    on_error_raise=None,\n                    prefix=None,\n                    update_interface=False,\n                    restrict_interface=False):\n    '''peek configuration parameters from external pipeline.\n\n    As the paramater dictionary is built at runtime, this method\n    executes the pipeline in workingdir, dumping its configuration\n    values and reading them into a dictionary.\n\n    If either `pipeline` or `workingdir` are not found, an error is\n    raised. This behaviour can be changed by setting `on_error_raise`\n    to False. In that case, an empty dictionary is returned.\n\n    Arguments\n    ---------\n    workingdir : string\n       Working directory. This is the directory that the pipeline\n       was executed in.\n    pipeline : string\n       Name of the pipeline script. The pipeline is assumed to live\n       in the same directory as the current pipeline.\n    on_error_raise : Bool\n       If set to a boolean, an error will be raised (or not) if there\n       is an error during parameter peeking, for example if\n       `workingdir` can not be found. If `on_error_raise` is None, it\n       will be set to the default, which is to raise an exception\n       unless the calling script is imported or the option\n       ``--is-test`` has been passed at the command line.\n    prefix : string\n       Add a prefix to all parameters. This is useful if the paramaters\n       are added to the configuration dictionary of the calling pipeline.\n    update_interface : bool\n       If True, this method will prefix any options in the\n       ``[interface]`` section with `workingdir`. This allows\n       transparent access to files in the external pipeline.\n    restrict_interface : bool\n       If  True, only interface parameters will be imported.\n\n    Returns\n    -------\n    config : dict\n        Dictionary of configuration values.\n\n    '''\n    caller_locals = get_caller_locals()\n\n    # check if we should raise errors\n    if on_error_raise is None:\n        on_error_raise = not is_test() and \\\n            \"__name__\" in caller_locals and \\\n            caller_locals[\"__name__\"] == \"__main__\"\n\n    # patch - if --help or -h in command line arguments,\n    # do not peek as there might be no config file.\n    if \"--help\" in sys.argv or \"-h\" in sys.argv:\n        return {}\n\n    if workingdir == \"\":\n        workingdir = os.path.abspath(\".\")\n\n    # patch for the \"config\" target - use default\n    # pipeline directory if directory is not specified\n    # working dir is set to \"?!\"\n    if (\"config\" in sys.argv or \"check\" in sys.argv or \"clone\" in sys.argv and workingdir == \"?!\"):\n        workingdir = os.path.join(get_params()[\"pipelinedir\"],\n                                  \"pipeline_\" + pipeline)\n\n    if not os.path.exists(workingdir):\n        if on_error_raise:\n            raise ValueError(\n                \"can't find working dir %s\" % workingdir)\n        else:\n            return {}\n\n    statement = \"cgatflow {} dump -v 0\".format(pipeline)\n\n    os.environ.update(\n        {'BASH_ENV': os.path.join(os.environ['HOME'], '.bashrc')})\n    process = subprocess.Popen(statement,\n                               cwd=workingdir,\n                               shell=True,\n                               stdin=subprocess.PIPE,\n                               stdout=subprocess.PIPE,\n                               stderr=subprocess.PIPE,\n                               env=os.environ.copy())\n\n    # process.stdin.close()\n    stdout, stderr = process.communicate()\n    if process.returncode != 0:\n        raise OSError(\n            (\"Child was terminated by signal %i: \\n\"\n             \"Statement: %s\\n\"\n             \"The stderr was: \\n%s\\n\"\n             \"Stdout: %s\") %\n            (-process.returncode, statement, stderr, stdout))\n\n    # subprocess only accepts encoding argument in py &gt;= 3.6 so\n    # decode here.\n    stdout = stdout.decode(\"utf-8\").splitlines()\n    # remove any log messages\n    stdout = [x for x in stdout if x.startswith(\"{\")]\n    if len(stdout) &gt; 1:\n        raise ValueError(\"received multiple configurations\")\n    dump = json.loads(stdout[0])\n\n    # update interface\n    if update_interface:\n        for key, value in list(dump.items()):\n            if key.startswith(\"interface\"):\n                if isinstance(value, str):\n                    dump[key] = os.path.join(workingdir, value)\n                elif isinstance(value, Mapping):\n                    for kkey, vvalue in list(value.items()):\n                        value[key] = os.path.join(workingdir, vvalue)\n\n    # keep only interface if so required\n    if restrict_interface:\n        dump = dict([(k, v) for k, v in dump.items()\n                     if k.startswith(\"interface\")])\n\n    # prefix all parameters\n    if prefix is not None:\n        dump = dict([(\"%s%s\" % (prefix, x), y) for x, y in list(dump.items())])\n\n    return dump\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.print_config_files","title":"<code>print_config_files()</code>","text":"<p>Print the list of .ini files used to configure the pipeline along with their associated priorities. Priority 1 is the highest.</p> Source code in <code>cgatcore/pipeline/control.py</code> <pre><code>def print_config_files():\n    '''\n        Print the list of .ini files used to configure the pipeline\n        along with their associated priorities.\n        Priority 1 is the highest.\n    '''\n\n    filenames = get_params()['pipeline_yml']\n    print(\"\\n List of .yml files used to configure the pipeline\")\n    s = len(filenames)\n    if s == 0:\n        print(\" No yml files passed!\")\n    elif s &gt;= 1:\n        print(\" %-11s: %s \" % (\"Priority\", \"File\"))\n        for f in filenames:\n            if s == 1:\n                print(\" (highest) %s: %s\\n\" % (s, f))\n            else:\n                print(\" %-11s: %s \" % (s, f))\n            s -= 1\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.run","title":"<code>run(statement, **kwargs)</code>","text":"<p>run a command line statement.</p> <p>This function runs a single or multiple statements either locally or on the cluster using drmaa. How a statement is executed or how it is modified depends on the context.</p> <p>The context is provided by keyword arguments provided as named function arguments ('kwargs') but also from defaults (see below). The following keyword arguments are recognized:</p> <p>job_memory     memory to use for the job per thread. Memory specification should be in a     format that is accepted by the job scheduler. Note that memory     is per thread. If you have 6 threads and the total memory is     6Gb, use 1G as job_memory. job_total_memory     total memory to use for a job. This will be divided by the number of     threads. job_threads     number of threads to request for the job. job_options     options to the job scheduler. job_condaenv     conda environment to use for the job. job_array     if set, run statement as an array job. Job_array should be     tuple with start, end, and increment.</p> <p>In addition, any additional variables will be used to interpolate the command line string using python's '%' string interpolation operator.</p> <p>The context is build in a hierarchical manner with successive operations overwriting previous values.</p> <ol> <li>Global variables    The context is initialized    with system-wide defaults stored in the global PARAMS    singleton.</li> <li>Context of caller    The context of the calling function is examined    and any local variables defined in this context are added.</li> <li>kwargs    Any options given explicitely as options to the run() method    are added.</li> <li>params    If the context of the calling function contains a params    variable, its contents are added to the context. This permits    setting variables in configuration files in TaskLibrary    functions.</li> </ol> <p>By default, a job is sent to the cluster, unless:</p> <pre><code>* ``to_cluster`` is present and set to None.\n\n* ``without_cluster`` is True.\n\n* ``--local`` has been specified on the command line\n  and the option ``without_cluster`` has been set as\n  a result.\n\n* no libdrmaa is present\n\n* the global session is not initialized (GLOBAL_SESSION is\n  None)\n</code></pre> <p>Troubleshooting:</p> <ol> <li> <p>DRMAA creates sessions and their is a limited number       of sessions available. If there are two many or sessions       become not available after failed jobs, use <code>qconf -secl</code>       to list sessions and <code>qconf -kec #</code> to delete sessions.</p> </li> <li> <p>Memory: 1G of free memory can be requested using the job_memory       variable: <code>job_memory = \"1G\"</code>       If there are error messages like \"no available queue\", then the       problem could be that a particular complex attribute has       not been defined (the code should be <code>hc</code> for <code>host:complex</code>       and not <code>hl</code> for <code>host:local</code>. Note that qrsh/qsub directly       still works.</p> </li> </ol> <p>The job will be executed within PARAMS[\"work_dir\"], unless PARAMS[\"work_dir\"] is not local. In that case, the job will be executed in a shared temporary directory.</p>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.run--arguments","title":"Arguments","text":"<p>statement : string or list of strings     A command line statement or a list of command line statements     to be executed. kwargs : dictionary     Context for job. The context is used to interpolate the command     line statement.</p> Source code in <code>cgatcore/pipeline/execution.py</code> <pre><code>def run(statement, **kwargs):\n    \"\"\"run a command line statement.\n\n    This function runs a single or multiple statements either locally\n    or on the cluster using drmaa. How a statement is executed or how\n    it is modified depends on the context.\n\n    The context is provided by keyword arguments provided as named\n    function arguments ('kwargs') but also from defaults (see\n    below). The following keyword arguments are recognized:\n\n    job_memory\n        memory to use for the job per thread. Memory specification should be in a\n        format that is accepted by the job scheduler. Note that memory\n        is per thread. If you have 6 threads and the total memory is\n        6Gb, use 1G as job_memory.\n    job_total_memory\n        total memory to use for a job. This will be divided by the number of\n        threads.\n    job_threads\n        number of threads to request for the job.\n    job_options\n        options to the job scheduler.\n    job_condaenv\n        conda environment to use for the job.\n    job_array\n        if set, run statement as an array job. Job_array should be\n        tuple with start, end, and increment.\n\n    In addition, any additional variables will be used to interpolate\n    the command line string using python's '%' string interpolation\n    operator.\n\n    The context is build in a hierarchical manner with successive\n    operations overwriting previous values.\n\n    1. Global variables\n       The context is initialized\n       with system-wide defaults stored in the global PARAMS\n       singleton.\n    2. Context of caller\n       The context of the calling function is examined\n       and any local variables defined in this context are added.\n    3. kwargs\n       Any options given explicitely as options to the run() method\n       are added.\n    4. params\n       If the context of the calling function contains a params\n       variable, its contents are added to the context. This permits\n       setting variables in configuration files in TaskLibrary\n       functions.\n\n    By default, a job is sent to the cluster, unless:\n\n        * ``to_cluster`` is present and set to None.\n\n        * ``without_cluster`` is True.\n\n        * ``--local`` has been specified on the command line\n          and the option ``without_cluster`` has been set as\n          a result.\n\n        * no libdrmaa is present\n\n        * the global session is not initialized (GLOBAL_SESSION is\n          None)\n\n    Troubleshooting:\n\n       1. DRMAA creates sessions and their is a limited number\n          of sessions available. If there are two many or sessions\n          become not available after failed jobs, use ``qconf -secl``\n          to list sessions and ``qconf -kec #`` to delete sessions.\n\n       2. Memory: 1G of free memory can be requested using the job_memory\n          variable: ``job_memory = \"1G\"``\n          If there are error messages like \"no available queue\", then the\n          problem could be that a particular complex attribute has\n          not been defined (the code should be ``hc`` for ``host:complex``\n          and not ``hl`` for ``host:local``. Note that qrsh/qsub directly\n          still works.\n\n    The job will be executed within PARAMS[\"work_dir\"], unless\n    PARAMS[\"work_dir\"] is not local. In that case, the job will\n    be executed in a shared temporary directory.\n\n    Arguments\n    ---------\n    statement : string or list of strings\n        A command line statement or a list of command line statements\n        to be executed.\n    kwargs : dictionary\n        Context for job. The context is used to interpolate the command\n        line statement.\n\n    \"\"\"\n    logger = get_logger()\n\n    # Combine options using priority\n    options = dict(list(get_params().items()))\n    caller_options = get_caller_locals()\n    options.update(list(caller_options.items()))\n\n    if \"self\" in options:\n        del options[\"self\"]\n    options.update(list(kwargs.items()))\n\n    # Inject params named tuple from TaskLibrary functions into option\n    # dict. This allows overriding options set in the code with options set\n    # in a .yml file\n    if \"params\" in options:\n        try:\n            options.update(options[\"params\"]._asdict())\n        except AttributeError:\n            pass\n\n    # Insert parameters supplied through simplified interface such\n    # as job_memory, job_options, job_queue\n    options['cluster']['options'] = options.get(\n        'job_options', options['cluster']['options'])\n    options['cluster']['queue'] = options.get(\n        'job_queue', options['cluster']['queue'])\n    options['without_cluster'] = options.get('without_cluster')\n\n    # SGE compatible job_name\n    name_substrate = str(options.get(\"outfile\", \"cgatcore\"))\n    if os.path.basename(name_substrate).startswith(\"result\"):\n        name_substrate = os.path.basename(os.path.dirname(name_substrate))\n    else:\n        name_substrate = os.path.basename(name_substrate)\n\n    options[\"job_name\"] = re.sub(\"[:]\", \"_\", name_substrate)\n    try:\n        calling_module = get_caller().__name__\n    except AttributeError:\n        calling_module = \"unknown\"\n\n    options[\"task_name\"] = calling_module + \".\" + get_calling_function()\n\n    # Build statements using parameter interpolation\n    if isinstance(statement, list):\n        statement_list = [interpolate_statement(stmt, options) for stmt in statement]\n    else:\n        statement_list = [interpolate_statement(statement, options)]\n\n    if len(statement_list) == 0:\n        logger.warn(\"No statements found - no execution\")\n        return []\n\n    if options.get(\"dryrun\", False):\n        for statement in statement_list:\n            logger.info(\"Dry-run: {}\".format(statement))\n        return []\n\n    # Use get_executor to get the appropriate executor\n    executor = get_executor(options)  # Updated to use get_executor\n\n    # Execute statement list within the context of the executor\n    with executor as e:\n        benchmark_data = e.run(statement_list)\n\n    # Log benchmark data\n    for data in benchmark_data:\n        logger.info(json.dumps(data))\n\n    BenchmarkData = collections.namedtuple('BenchmarkData', sorted(benchmark_data[0]))\n    return [BenchmarkData(**d) for d in benchmark_data]\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.run_pickled","title":"<code>run_pickled(params)</code>","text":"<p>run a function whose arguments have been pickled.</p> <p>expects that params is [module_name, function_name, arguments_file]</p> Source code in <code>cgatcore/pipeline/execution.py</code> <pre><code>def run_pickled(params):\n    ''' run a function whose arguments have been pickled.\n\n    expects that params is [module_name, function_name, arguments_file] '''\n\n    module_name, func_name, args_file = params\n    location = os.path.dirname(module_name)\n    if location != \"\":\n        sys.path.append(location)\n\n    module_base_name = os.path.basename(module_name)\n    logger = get_logger()\n    logger.info(\"importing module '%s' \" % module_base_name)\n    logger.debug(\"sys.path is: %s\" % sys.path)\n\n    module = importlib.import_module(module_base_name)\n    try:\n        function = getattr(module, func_name)\n    except AttributeError as msg:\n        raise AttributeError(msg.message\n                             + \"unknown function, available functions are: %s\" %\n                             \",\".join([x for x in dir(module)\n                                       if not x.startswith(\"_\")]))\n\n    args, kwargs = pickle.load(open(args_file, \"rb\"))\n    logger.info(\"arguments = %s\" % str(args))\n    logger.info(\"keyword arguments = %s\" % str(kwargs))\n\n    function(*args, **kwargs)\n\n    os.unlink(args_file)\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.shellquote","title":"<code>shellquote(statement)</code>","text":"<p>shell quote a string to be used as a function argument.</p> <p>from http://stackoverflow.com/questions/967443/ python-module-to-shellquote-unshellquote</p> Source code in <code>cgatcore/pipeline/execution.py</code> <pre><code>def shellquote(statement):\n    '''shell quote a string to be used as a function argument.\n\n    from http://stackoverflow.com/questions/967443/\n    python-module-to-shellquote-unshellquote\n    '''\n    _quote_pos = re.compile('(?=[^-0-9a-zA-Z_./\\n])')\n\n    if statement:\n        return _quote_pos.sub('\\\\\\\\', statement).replace('\\n', \"'\\n'\")\n    else:\n        return \"''\"\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.snip","title":"<code>snip(filename, extension=None, alt_extension=None, strip_path=False)</code>","text":"<p>return prefix of <code>filename</code>, that is the part without the extension.</p> <p>If <code>extension</code> is given, make sure that filename has the extension (or <code>alt_extension</code>). Both extension or alt_extension can be list of extensions.</p> <p>If <code>strip_path</code> is set to true, the path is stripped from the file name.</p> Source code in <code>cgatcore/iotools.py</code> <pre><code>def snip(filename, extension=None, alt_extension=None,\n         strip_path=False):\n    '''return prefix of `filename`, that is the part without the\n    extension.\n\n    If `extension` is given, make sure that filename has the\n    extension (or `alt_extension`). Both extension or alt_extension\n    can be list of extensions.\n\n    If `strip_path` is set to true, the path is stripped from the file\n    name.\n\n    '''\n    if extension is None:\n        extension = []\n    elif isinstance(extension, str):\n        extension = [extension]\n\n    if alt_extension is None:\n        alt_extension = []\n    elif isinstance(alt_extension, str):\n        alt_extension = [alt_extension]\n\n    if extension:\n        for ext in extension + alt_extension:\n            if filename.endswith(ext):\n                root = filename[:-len(ext)]\n                break\n        else:\n            raise ValueError(\"'%s' expected to end in '%s'\" %\n                             (filename, \",\".join(\n                                 extension + alt_extension)))\n    else:\n        root, ext = os.path.splitext(filename)\n\n    if strip_path:\n        snipped = os.path.basename(root)\n    else:\n        snipped = root\n\n    return snipped\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.start_session","title":"<code>start_session()</code>","text":"<p>start and initialize the global DRMAA session.</p> Source code in <code>cgatcore/pipeline/execution.py</code> <pre><code>def start_session():\n    \"\"\"start and initialize the global DRMAA session.\"\"\"\n    global GLOBAL_SESSION\n\n    if HAS_DRMAA and GLOBAL_SESSION is None:\n        GLOBAL_SESSION = drmaa.Session()\n        try:\n            GLOBAL_SESSION.initialize()\n        except drmaa.errors.InternalException as ex:\n            get_logger().warn(\"could not initialize global drmaa session: {}\".format(\n                ex))\n            GLOBAL_SESSION = None\n        return GLOBAL_SESSION\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.submit","title":"<code>submit(module, function, args=None, infiles=None, outfiles=None, to_cluster=True, logfile=None, job_options='', job_threads=1, job_memory=False)</code>","text":"<p>submit a python function as a job to the cluster.</p> <p>This method runs the script :file:<code>run_function</code> using the :func:<code>run</code> method in this module thus providing the same control options as for command line tools.</p>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.submit--arguments","title":"Arguments","text":"<p>module : string     Module name that contains the function. If <code>module</code> is     not part of the PYTHONPATH, an absolute path can be given. function : string     Name of function to execute infiles : string or list     Filenames of input data outfiles : string or list     Filenames of output data logfile : filename     Logfile to provide to the <code>--log</code> option job_options : string     String for generic job options for the queuing system job_threads : int     Number of slots (threads/cores/CPU) to use for the task job_memory : string     Amount of memory to reserve for the job.</p> Source code in <code>cgatcore/pipeline/execution.py</code> <pre><code>def submit(module,\n           function,\n           args=None,\n           infiles=None,\n           outfiles=None,\n           to_cluster=True,\n           logfile=None,\n           job_options=\"\",\n           job_threads=1,\n           job_memory=False):\n    '''submit a python *function* as a job to the cluster.\n\n    This method runs the script :file:`run_function` using the\n    :func:`run` method in this module thus providing the same\n    control options as for command line tools.\n\n    Arguments\n    ---------\n    module : string\n        Module name that contains the function. If `module` is\n        not part of the PYTHONPATH, an absolute path can be given.\n    function : string\n        Name of function to execute\n    infiles : string or list\n        Filenames of input data\n    outfiles : string or list\n        Filenames of output data\n    logfile : filename\n        Logfile to provide to the ``--log`` option\n    job_options : string\n        String for generic job options for the queuing system\n    job_threads : int\n        Number of slots (threads/cores/CPU) to use for the task\n    job_memory : string\n        Amount of memory to reserve for the job.\n\n    '''\n\n    if not job_memory:\n        job_memory = get_params().get(\"cluster_memory_default\", \"2G\")\n\n    if type(infiles) in (list, tuple):\n        infiles = \" \".join([\"--input=%s\" % x for x in infiles])\n    else:\n        infiles = \"--input=%s\" % infiles\n\n    if type(outfiles) in (list, tuple):\n        outfiles = \" \".join([\"--output-section=%s\" % x for x in outfiles])\n    else:\n        outfiles = \"--output-section=%s\" % outfiles\n\n    if logfile:\n        logfile = \"--log=%s\" % logfile\n    else:\n        logfile = \"\"\n\n    if args:\n        args = \"--args=%s\" % \",\".join(args)\n    else:\n        args = \"\"\n\n    statement = (\n        \"python -m cgatcore.pipeline.run_function \"\n        \"--module=%(module)s \"\n        \"--function=%(function)s \"\n        \"%(logfile)s \"\n        \"%(infiles)s \"\n        \"%(outfiles)s \"\n        \"%(args)s\")\n    run(statement)\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.substitute_parameters","title":"<code>substitute_parameters(**kwargs)</code>","text":"<p>return a parameter dictionary.</p> <p>This method builds a dictionary of parameter values to apply for a specific task. The dictionary is built in the following order:</p> <ol> <li>take values from the global dictionary (:py:data:<code>PARAMS</code>)</li> <li>substitute values appearing in <code>kwargs</code>.</li> <li>Apply task specific configuration values by looking for the    presence of <code>outfile</code> in kwargs.</li> </ol> <p>The substition of task specific values works by looking for any parameter values starting with the value of <code>outfile</code>.  The suffix of the parameter value will then be substituted.</p> <p>For example::</p> <pre><code>PARAMS = {\"tophat_threads\": 4,\n          \"tophat_cutoff\": 0.5,\n          \"sample1.bam.gz_tophat_threads\" : 6}\noutfile = \"sample1.bam.gz\"\nprint(substitute_parameters(**locals()))\n{\"tophat_cutoff\": 0.5, \"tophat_threads\": 6}\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.substitute_parameters--returns","title":"Returns","text":"<p>params : dict     Dictionary with parameter values.</p> Source code in <code>cgatcore/pipeline/parameters.py</code> <pre><code>def substitute_parameters(**kwargs):\n    '''return a parameter dictionary.\n\n    This method builds a dictionary of parameter values to\n    apply for a specific task. The dictionary is built in\n    the following order:\n\n    1. take values from the global dictionary (:py:data:`PARAMS`)\n    2. substitute values appearing in `kwargs`.\n    3. Apply task specific configuration values by looking for the\n       presence of ``outfile`` in kwargs.\n\n    The substition of task specific values works by looking for any\n    parameter values starting with the value of ``outfile``.  The\n    suffix of the parameter value will then be substituted.\n\n    For example::\n\n        PARAMS = {\"tophat_threads\": 4,\n                  \"tophat_cutoff\": 0.5,\n                  \"sample1.bam.gz_tophat_threads\" : 6}\n        outfile = \"sample1.bam.gz\"\n        print(substitute_parameters(**locals()))\n        {\"tophat_cutoff\": 0.5, \"tophat_threads\": 6}\n\n    Returns\n    -------\n    params : dict\n        Dictionary with parameter values.\n\n    '''\n\n    # build parameter dictionary\n    # note the order of addition to make sure that kwargs takes precedence\n    local_params = dict(list(PARAMS.items()) + list(kwargs.items()))\n\n    if \"outfile\" in local_params:\n        # replace specific parameters with task (outfile) specific parameters\n        outfile = local_params[\"outfile\"]\n        keys = list(local_params.keys())\n        for k in keys:\n            if k.startswith(outfile):\n                p = k[len(outfile) + 1:]\n                if p not in local_params:\n                    # do not raise error, argument might be a prefix\n                    continue\n                get_logger.debug(\"substituting task specific parameter \"\n                                 \"for %s: %s = %s\" %\n                                 (outfile, p, local_params[k]))\n                local_params[p] = local_params[k]\n\n    return local_params\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.tablequote","title":"<code>tablequote(track)</code>","text":"<p>quote a track name such that is suitable as a table name.</p> Source code in <code>cgatcore/pipeline/database.py</code> <pre><code>def tablequote(track):\n    '''quote a track name such that is suitable as a table name.'''\n    return re.sub(r\"[-(),\\[\\].]\", \"_\", track)\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.to_table","title":"<code>to_table(outfile)</code>","text":"<p>convert a filename from a load statement into a table name.</p> <p>This method checks if the filename ends with \".load\". The suffix is then removed and the filename quoted so that it is suitable as a table name.</p>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.to_table--arguments","title":"Arguments","text":"<p>outfile : string     A filename ending in \".load\".</p>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.to_table--returns","title":"Returns","text":"<p>tablename : string</p> Source code in <code>cgatcore/pipeline/database.py</code> <pre><code>def to_table(outfile):\n    '''convert a filename from a load statement into a table name.\n\n    This method checks if the filename ends with \".load\". The suffix\n    is then removed and the filename quoted so that it is suitable\n    as a table name.\n\n    Arguments\n    ---------\n    outfile : string\n        A filename ending in \".load\".\n\n    Returns\n    -------\n    tablename : string\n\n    '''\n    assert outfile.endswith(\".load\")\n    name = os.path.basename(outfile[:-len(\".load\")])\n    return tablequote(name)\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.touch_file","title":"<code>touch_file(filename, mode=438, times=None, dir_fd=None, ref=None, **kwargs)</code>","text":"<p>update/create a sentinel file.</p> <p>modified from: https://stackoverflow.com/questions/1158076/implement-touch-using-python</p> <p>Compressed files (ending in .gz) are created as empty 'gzip' files, i.e., with a header.</p> Source code in <code>cgatcore/iotools.py</code> <pre><code>def touch_file(filename, mode=0o666, times=None, dir_fd=None, ref=None, **kwargs):\n    '''update/create a sentinel file.\n\n    modified from: https://stackoverflow.com/questions/1158076/implement-touch-using-python\n\n    Compressed files (ending in .gz) are created as empty 'gzip'\n    files, i.e., with a header.\n\n    '''\n    flags = os.O_CREAT | os.O_APPEND\n    existed = os.path.exists(filename)\n\n    if filename.endswith(\".gz\") and not existed:\n        # this will automatically add a gzip header\n        with gzip.GzipFile(filename, \"w\") as fhandle:\n            pass\n\n    if ref:\n        stattime = os.stat(ref)\n        times = (stattime.st_atime, stattime.st_mtime)\n\n    with os.fdopen(os.open(\n            filename, flags=flags, mode=mode, dir_fd=dir_fd)) as fhandle:\n        os.utime(\n            fhandle.fileno() if os.utime in os.supports_fd else filename,\n            dir_fd=None if os.supports_fd else dir_fd,\n            **kwargs)\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.update_params_with_commandline_options","title":"<code>update_params_with_commandline_options(params, args)</code>","text":"<p>add and update selected parameters in the parameter dictionary with command line args.</p> Source code in <code>cgatcore/pipeline/control.py</code> <pre><code>def update_params_with_commandline_options(params, args):\n    \"\"\"add and update selected parameters in the parameter\n    dictionary with command line args.\n    \"\"\"\n\n    params[\"pipeline_name\"] = args.pipeline_name\n    params[\"dryrun\"] = args.dry_run\n\n    # translate cluster options into dict\n    for key in params[\"cluster\"].keys():\n        arg_key = \"cluster_{}\".format(key)\n        if hasattr(args, arg_key):\n            val = getattr(args, arg_key)\n            if val is not None:\n                params[\"cluster\"][key] = val\n\n    if args.without_cluster:\n        params[\"without_cluster\"] = True\n\n    params[\"shell_logfile\"] = args.shell_logfile\n\n    params[\"ruffus_checksums_level\"] = args.ruffus_checksums_level\n    # always create an \"input\" section\n    params[\"input_globs\"] = {}\n    for variable in args.input_globs:\n        if \"=\" in variable:\n            variable, value = variable.split(\"=\")\n            params[\"input_globs\"][variable.strip()] = value.strip()\n        else:\n            params[\"input_globs\"][\"default\"] = variable.strip()\n\n    for variables in args.variables_to_set:\n        variable, value = variables.split(\"=\")\n        value = iotools.str2val(value.strip())\n        # enter old style\n        params[variable.strip()] = value\n        # enter new style\n        parts = variable.split(\"_\")\n        for x in range(1, len(parts)):\n            prefix = \"_\".join(parts[:x])\n            if prefix in params:\n                suffix = \"_\".join(parts[x:])\n                params[prefix][suffix] = value\n\n    if args.work_dir:\n        params[\"work_dir\"] = os.path.abspath(args.work_dir)\n    else:\n        params[\"work_dir\"] = params[\"start_dir\"]\n</code></pre>"},{"location":"s3_integration/s3_decorators/#cgatcore.pipeline.write_config_files","title":"<code>write_config_files(pipeline_path, general_path)</code>","text":"<p>create default configuration files in <code>path</code>.</p> Source code in <code>cgatcore/pipeline/control.py</code> <pre><code>def write_config_files(pipeline_path, general_path):\n    '''create default configuration files in `path`.\n    '''\n\n    paths = [pipeline_path, general_path]\n    config_files = ['pipeline.yml']\n\n    for dest in config_files:\n        if os.path.exists(dest):\n            E.warn(\"file `%s` already exists - skipped\" % dest)\n            continue\n\n        for path in paths:\n            src = os.path.join(path, dest)\n            if os.path.exists(src):\n                shutil.copyfile(src, dest)\n                E.info(\"created new configuration file `%s` \" % dest)\n                break\n        else:\n            raise ValueError(\n                \"default config file `%s` not found in %s\" %\n                (config_files, paths))\n</code></pre>"},{"location":"s3_integration/s3_pipeline/","title":"S3 Pipeline","text":"<p>The <code>S3Pipeline</code> class is part of the integration with AWS S3, enabling seamless data handling in CGAT pipelines that use both local files and S3 storage. This is particularly useful when working with large datasets that are better managed in cloud storage or when collaborating across multiple locations.</p>"},{"location":"s3_integration/s3_pipeline/#overview","title":"Overview","text":"<p><code>S3Pipeline</code> provides the following functionalities:</p> <ul> <li>Integration of AWS S3 into CGAT pipeline workflows</li> <li>Lazy-loading of S3-specific classes and functions to avoid circular dependencies</li> <li>Facilitating operations on files that reside on S3, making it possible to apply transformations and merges without copying everything locally</li> </ul>"},{"location":"s3_integration/s3_pipeline/#example-usage","title":"Example Usage","text":"<p>The <code>S3Pipeline</code> class can be accessed through the <code>get_s3_pipeline()</code> function, which returns an instance that is lazy-loaded to prevent issues related to circular imports. Below is an example of how to use it:</p> <pre><code>from cgatcore.pipeline import get_s3_pipeline\n\n# Instantiate the S3 pipeline\ns3_pipeline = get_s3_pipeline()\n\n# Use methods from s3_pipeline as needed\ns3_pipeline.s3_transform(...)\n</code></pre>"},{"location":"s3_integration/s3_pipeline/#building-a-function-using-s3pipeline","title":"Building a Function Using <code>S3Pipeline</code>","text":"<p>To build a function that utilises <code>S3Pipeline</code>, you can follow a few simple steps. Below is a guide on how to create a function that uses the <code>s3_transform</code> method to process data from S3:</p> <ol> <li>Import the required modules: First, import <code>get_s3_pipeline</code> from <code>cgatcore.pipeline</code>.</li> <li>Instantiate the pipeline: Use <code>get_s3_pipeline()</code> to create an instance of <code>S3Pipeline</code>.</li> <li>Define your function: Use the S3-aware methods like <code>s3_transform()</code> to perform the desired operations on your S3 files.</li> </ol>"},{"location":"s3_integration/s3_pipeline/#example-function","title":"Example Function","text":"<pre><code>from cgatcore.pipeline import get_s3_pipeline\n\n# Instantiate the S3 pipeline\ns3_pipeline = get_s3_pipeline()\n\n# Define a function that uses s3_transform\ndef process_s3_data(input_s3_path, output_s3_path):\n    @s3_pipeline.s3_transform(input_s3_path, suffix(\".txt\"), output_s3_path)\n    def transform_data(infile, outfile):\n        # Add your processing logic here\n        with open(infile, 'r') as fin:\n            data = fin.read()\n            # Example transformation\n            processed_data = data.upper()\n        with open(outfile, 'w') as fout:\n            fout.write(processed_data)\n\n    # Run the transformation\n    transform_data()\n</code></pre>"},{"location":"s3_integration/s3_pipeline/#methods-in-s3pipeline","title":"Methods in <code>S3Pipeline</code>","text":"<ul> <li><code>s3_transform(*args, **kwargs)</code>: Perform a transformation on data stored in S3, similar to Ruffus <code>transform()</code> but adapted for S3 files.</li> <li><code>s3_merge(*args, **kwargs)</code>: Merge multiple input files into one, allowing the files to be located on S3.</li> <li><code>s3_split(*args, **kwargs)</code>: Split input data into smaller chunks, enabling parallel processing, even when the input resides on S3.</li> <li><code>s3_originate(*args, **kwargs)</code>: Create new files directly in S3.</li> <li><code>s3_follows(*args, **kwargs)</code>: Indicate a dependency on another task, ensuring correct task ordering even for S3 files.</li> </ul> <p>These methods are intended to be directly equivalent to standard Ruffus methods, allowing pipelines to easily mix and match S3-based and local operations.</p>"},{"location":"s3_integration/s3_pipeline/#why-use-s3pipeline","title":"Why Use <code>S3Pipeline</code>?","text":"<ul> <li>Scalable Data Management: Keeps large datasets in the cloud, reducing local storage requirements.</li> <li>Seamless Integration: Provides a drop-in replacement for standard decorators, enabling hybrid workflows involving both local and cloud files.</li> <li>Lazy Loading: Optimised to initialise S3 components only when they are needed, minimising overhead and avoiding unnecessary dependencies.</li> </ul>"}]}